---
date: '2025-08-15T01:41:53+08:00'
title: 'ã€ŠWhat are Diffusion Models?ã€‹æ³¨é‡Š'
author:
  - Shichao Song
summary: ''
tags: ["diffusion"]
math: true
---

<!-- TODOï¼šè®°å¾—emailç»™lilian about this article -->

æœ¬æ–‡å¯¹Lilian Wengçš„ã€ŠWhat are Diffusion Models?ã€‹ [^lilian_diffusion] è¿›è¡Œå®Œå–„çš„æ³¨é‡Šå¯¼è¯»ã€‚

ç¬”è€…åœ¨å†™æ­¤æ–‡æ—¶ï¼Œå¯¹å›¾åƒç”Ÿæˆæ¨¡å‹äº†è§£å’Œç›¸å…³çš„æ•°å­¦èƒŒæ™¯çŸ¥è¯†äº†è§£å‡è¾ƒå°‘ã€‚å¦‚æœä½ ä¹Ÿæœ‰ç›¸ä¼¼çš„èƒŒæ™¯ï¼Œé‚£ä¹ˆæ­¤æ–‡åº”è¯¥ä¼šé€‚åˆä½ ã€‚å½“ç„¶ï¼Œæˆ‘å¯èƒ½ä¹Ÿå› æ­¤çŠ¯ä¸€äº›ä½çº§é”™è¯¯ï¼Œæ•¬è¯·æŒ‡æ­£ã€‚

æœ¬æ–‡çš„ç»“æ„å’ŒåŸæ–‡åŸºæœ¬ä¿æŒä¸€è‡´ã€‚æ¯ä¸ªå°èŠ‚ä¸­é‡ç‚¹çš„å…¬å¼ï¼Œæ¦‚å¿µéƒ½ä¼šè¿›è¡Œæ‰©å±•çš„æ¨å¯¼æˆ–è§£é‡Šã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ–‡ç« å¼€å§‹é™„ä¸Šäº†å¸¸è§çš„ç¬¦å·è§£é‡Šï¼›æ–‡æœ«è¿˜é™„åŠ ä¸Šäº†ä¸ºäº†çœ‹æ‡‚åŸæ–‡æ‰€éœ€çš„ä¸€äº›èƒŒæ™¯çŸ¥è¯†ã€‚

### Notations

| Category / Symbol                                                                                                            | Meaning                                                                                                                                                                                                                                                  |
| ---------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **æ ·æœ¬ä¸åˆ†å¸ƒ**                                                                                                               |                                                                                                                                                                                                                                                          |
| $\mathbf{x}_0$                                                                                                               | ä¸€ä¸ªçœŸå®æ•°æ®æ ·æœ¬ï¼Œæ¯”å¦‚å›¾åƒã€è¯­éŸ³æˆ–æ–‡æœ¬ã€‚æ˜¯ä¸€ä¸ªå‘é‡ï¼ˆä¾‹å¦‚å›¾åƒçš„åƒç´ å‘é‡ã€æ–‡æœ¬çš„åµŒå…¥å‘é‡ç­‰ï¼‰ï¼Œç»´åº¦å¯èƒ½æ˜¯å‡ ç™¾ç”šè‡³å‡ åƒ.                                                                                                                                      |
| $q(\mathbf{x})$                                                                                                              | åœ¨Diffusionç›¸å…³è®ºæ–‡ä¸­ï¼Œä¸ºäº†è¡¨ç¤ºæ–¹ä¾¿ï¼Œè¿™ä¸ªå¯ä»¥è¡¨ç¤ºæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯è¯¥PDFå¯¹åº”çš„åˆ†å¸ƒã€‚è¿™é‡Œï¼Œ$q(\mathbf{x})$ æ˜¯çœŸå®æ•°æ®çš„åˆ†å¸ƒï¼Œä¹Ÿå«ç»éªŒåˆ†å¸ƒï¼Œæ¯”å¦‚è®­ç»ƒé›†ä¸­çš„å›¾åƒåˆ†å¸ƒã€‚                                                                            |
| $\mathbf{x}_0 \sim q(\mathbf{x})$                                                                                            | ä»çœŸå®æ•°æ®åˆ†å¸ƒ $q(\mathbf{x})$ ä¸­é‡‡æ ·å¾—åˆ°çš„æ ·æœ¬ $\mathbf{x}_0$                                                                                                                                                                                           |
| $q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})$ | è¿™é‡Œçš„$q(\mathbf{x}_t \vert \mathbf{x}_{t-1})$ æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°. $\mathbf{x}_t \sim \mathcal{N}(\sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})$  çš„æ­£æ€åˆ†å¸ƒï¼Œ $q(\mathbf{x}_t \vert \mathbf{x}_{t-1})=f(\mathbf{x}_t)$ï¼Œ$f(\cdot)$ æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•° |
| **å™ªå£°è¶…å‚æ•°**                                                                                                               |                                                                                                                                                                                                                                                          |
| $\beta_t$                                                                                                                    | Noise variance schedule parameterã€‚è¶…å‚æ•°ï¼Œä»–å¯¹åº”ä¸€ä¸ªvariance scheduleï¼Œ$\{\beta_t \in (0, 1)\}_{t=1}^T$ï¼Œå’Œå­¦ä¹ ç‡è°ƒåº¦æ˜¯ç±»ä¼¼çš„.                                                                                                                          |
| $\alpha_t$                                                                                                                   | $\alpha_t = 1 - \beta_t$,æ˜¯ä¸ºäº†å…¬å¼ä¹¦å†™æ–¹ä¾¿è€Œåšçš„ç¬¦å·ã€‚                                                                                                                                                                                                  |
| $\bar{\alpha}_t$                                                                                                             | $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$ï¼Œæ˜¯ä¸ºäº†å…¬å¼ä¹¦å†™æ–¹ä¾¿è€Œåšçš„ç¬¦å·ã€‚                                                                                                                                                                                |
| **Diffusion è¿‡ç¨‹**                                                                                                           |                                                                                                                                                                                                                                                          |
| $q(\mathbf{x}_t \vert \mathbf{x}_{t-1})$                                                                                     | Forward diffusion processã€‚æ„é€ é«˜æ–¯é©¬å°”å¯å¤«é“¾ï¼Œé€æ­¥åŠ å™ªï¼Œç ´åæ•°æ®ã€‚                                                                                                                                                                                      |
| $p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$                                                                              | Reverse diffusion processã€‚é€šè¿‡è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹$\theta$æ¢å¤æ•°æ®ï¼Œä»å™ªå£°ä¸­ç”Ÿæˆæ ·æœ¬ã€‚å³è¿‘ä¼¼åéªŒã€‚                                                                                                                                                                    |

## What are Diffusion Models?

### Forward diffusion process

{{< admonition type=quote title="å‰å‘æ‰©æ•£è¡¨è¾¾å¼" >}}
$$
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I}) \quad
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1})
$$
{{< /admonition >}}

æ˜¯å‰å‘æ‰©æ•£è¿‡ç¨‹çš„ä¸¤ç§è¡¨è¾¾å½¢å¼ï¼Œå•æ­¥æ‰©æ•£è¿‡ç¨‹å’Œæ•´ä½“æ‰©æ•£è¿‡ç¨‹ã€‚

å•æ­¥æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œ

- $\mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})$ è¡¨ç¤º $\mathbf{x}_t$ æœä» å‡å€¼ä¸º $\sqrt{1 - \beta_t} \mathbf{x}_{t-1}$ï¼Œæ–¹å·®ä¸º $\beta_t\mathbf{I}$ çš„é«˜æ–¯åˆ†å¸ƒã€‚
- $\beta_t$ æ˜¯Noise variance schedule parameterï¼Œä»–å¯¹åº”ä¸€ä¸ªvariance scheduleï¼Œ$\{\beta_t \in (0, 1)\}_{t=1}^T$ï¼Œå’Œå­¦ä¹ ç‡è°ƒåº¦æ˜¯ç±»ä¼¼çš„.
- $\beta_t$ å®šä¹‰äº†åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æ¯ä¸ªæ—¶é—´æ­¥çš„æ–¹å·®å¤§å°ï¼Œä¸€èˆ¬æ¥è¯´$\beta_t$é€æ¸å¢å¤§ï¼Œå› æ­¤å’ŒåŸå§‹æ•°æ®å·®å¼‚è¶Šæ¥è¶Šå¤§ï¼ˆ$\sqrt{1 - \beta_t}$ â†“ï¼‰ï¼Œæ•°æ®å˜å¼‚æ€§ä¹Ÿé€æ¸å˜å¤§ï¼ˆ$\beta_t\mathbf{I}$ â†‘ï¼‰ï¼Œæ€»ä½“ä¸Šé€æ¸ä½¿å¾—æ¯ä¸€æ­¥çš„å™ªå£°æ›´å¤šã€‚
- $\beta_t\mathbf{I}$ï¼Œæ˜¯åæ–¹å·®çŸ©é˜µï¼Œä¹Ÿæ˜¯ä¸ªå¯¹è§’çŸ©é˜µï¼Œæ‰€æœ‰å¯¹è§’çº¿å…ƒç´ éƒ½æ˜¯ $\beta_t$. æ¯ä¸€ç»´éƒ½åŠ ç›¸åŒå¼ºåº¦çš„å™ªå£°ï¼Œä¸åå‘ä»»ä½•æ–¹å‘ã€‚

æ•´ä½“æ‰©æ•£è¿‡ç¨‹åªæ˜¯ä½¿ç”¨é©¬å°”å¯å¤«è¿‡ç¨‹æ€§è´¨ï¼ˆæ¯ä¸€æ­¥åªä¾èµ–å‰ä¸€æ­¥ï¼‰æ¥è¿ä¹˜è€Œå·²çš„é€’æ¨å¼ã€‚å®è·µä¸­ä¼šä½¿ç”¨æ›´ç®€å•çš„è®¡ç®—æ–¹å¼ï¼Œå°é—­å½¢å¼çš„å…¬å¼ã€‚

{{< admonition type=quote title="å‰å‘æ‰©æ•£è¡¨è¾¾å¼çš„closed formå½¢å¼" >}}
Let $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$.
$$
\begin{aligned}
\mathbf{x}_t
&= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} & \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} & \text{ ;where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians (*).} \\
&= \dots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$
{{< /admonition >}}

Closed-form expression æŒ‡çš„æ˜¯å¯ä»¥ç”¨æœ‰é™çš„ã€æ˜ç¡®çš„æ•°å­¦è¡¨è¾¾å¼ç›´æ¥å†™å‡ºæ¥è§£ï¼Œä¸éœ€è¦è¿­ä»£ã€æ•°å€¼è¿‘ä¼¼æˆ–æ±‚è§£æ–¹ç¨‹çš„å…¬å¼ [^wiki_closed]ã€‚

æ ¹æ®å•æ­¥æ‰©æ•£è¿‡ç¨‹$q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})$ï¼Œä»¥åŠé‡å‚æ•°åŒ–æŠ€å·§ $z = \mu + \sigma \cdot \epsilon$ï¼Œæˆ‘ä»¬å¯ä»¥é‡å†™å•æ­¥æ‰©æ•£è¿‡ç¨‹ä¸ºï¼š

$$\mathbf{x}_t = \sqrt{1 - \beta_t}\mathbf{x}_{t-1} + \sqrt{\beta_t}\boldsymbol{\epsilon}_{t-1}$$

è¿™æ ·å°±å¯ä»¥è®©æˆ‘ä»¬æ¥é‡å†™æ›´è¯¦ç»†çš„closed formçš„æ¨å¯¼ï¼š

$$
\begin{aligned}
\mathbf{x}_t
&= \sqrt{1 - \beta_t}\mathbf{x}_{t-1} + \sqrt{\beta_t}\boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} & \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&= \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2} \right) + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t (1 - \alpha_{t-1})} \boldsymbol{\epsilon}_{t-2} + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} & \text{ ;where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians (*).} \\
&= \dots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$

(*) Recall that when we merge two Gaussians with different variance, $\mathcal{N}(\mathbf{0}, \sigma_1^2\mathbf{I})$
 and $\mathcal{N}(\mathbf{0}, \sigma_2^2\mathbf{I})$, the new distribution is $\mathcal{N}(\mathbf{0}, (\sigma_1^2 + \sigma_2^2)\mathbf{I})$. Here the merged standard deviation is $\sqrt{\alpha_t (1-\alpha_{t-1}) + (1 - \alpha_t)} = \sqrt{1 - \alpha_t\alpha_{t-1}}$.

PSã€‚è¿™é‡Œè¿˜æœ‰ä¸€ç‚¹æŒ‡çš„æ³¨æ„çš„ï¼Œ$\mathbf{X}_t$ æ˜¯ä¸€ä¸ªä¸­é—´çŠ¶æ€ï¼Œæ˜¯ä¸ªè¢«åŠ å™ªå£°çš„ä¸­é—´æ ·æœ¬ï¼Œä»–æœä»ä¸¤ä¸ªäº‹æƒ…ï¼Œ

ä¸€ä¸ªæ˜¯ $\mathbf{x}_t= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}$ï¼Œè¿™ä¸ªæ˜¯ä»æ‰©æ•£çš„è§’åº¦æ¥è¯´çš„ï¼Œæ ·æœ¬ä¼šæ€ä¹ˆå˜åŒ–ã€‚

å¦å¤–ä¸€ä¸ªæ˜¯å®ƒå¯¹åº”çš„æ¦‚ç‡ $q(\mathbf{x}_t \vert \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})$ï¼Œè¿™æ˜¯å¦å¤–ä¸€ä¸ªä¸œè¥¿ï¼Œæ˜¯æŒ‡å¾—åˆ°è¿™ä¸ªæ ·æœ¬çš„å…ˆéªŒæ¦‚ç‡æ˜¯æ€æ ·çš„ï¼Œæ¦‚ç‡å¯†åº¦æ˜¯å¤šå°‘ã€‚

è¿™ä¸¤ä¸ªæ˜¯ç›¸è¾…ç›¸æˆçš„å…³ç³»ã€‚

{{< admonition type=quote title="Connection with stochastic gradient Langevin dynamics" >}}
$$
\mathbf{x}_t = \mathbf{x}_{t-1} + \frac{\delta}{2} \nabla_\mathbf{x} \log p(\mathbf{x}_{t-1}) + \sqrt{\delta} \boldsymbol{\epsilon}_t
,\quad\text{where }
\boldsymbol{\epsilon}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
$$
{{< /admonition >}}

## Appendix

### ç”Ÿæˆæ¨¡å‹èƒŒæ™¯

#### GAN, VAE, and Flow-based models æ˜¯ä»€ä¹ˆ

![Generative Models](/images/Generative_Models.png)

è¿™å‡ ä¸ªéƒ½æ˜¯æœ€å¸¸è§çš„å‡ ç±»å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å¤§è‡´äº†è§£å…¶åŸç†ï¼š

- GAN ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼šè®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆå›¾åƒï¼Œä¸€ä¸ªç”¨äºåˆ¤åˆ«å›¾åƒçš„çœŸä¼ª
- VAE å˜åˆ†è‡ªç¼–ç å™¨æ¨¡å‹ï¼šé€šè¿‡ç¼–ç å™¨å°†è¾“å…¥å›¾åƒå‹ç¼©ä¸ºæ½œåœ¨ç©ºé—´å˜é‡ï¼Œå†é€šè¿‡è§£ç å™¨é‡å»ºå›¾åƒ
- Flow matching modelsï¼šé€šè¿‡æµåŠ¨åŒ¹é…çš„æ–¹æ³•ç”Ÿæˆå›¾åƒï¼Œæ˜¯å‡½æ•°çº§åˆ«çš„è¿‡ç¨‹ç»„åˆèµ·æ¥ç”Ÿæˆå›¾åƒçš„ã€‚

#### ä» AE åˆ° VAE å†åˆ° VQ-VAE

- AE æ˜¯ä¸€ä¸ªå…·æœ‰ç¼–ç å™¨å’Œè§£ç å™¨çš„ç¥ç»ç½‘ç»œï¼Œç›®æ ‡æ˜¯å­¦ä¹ è¾“å…¥æ•°æ®çš„å‹ç¼©è¡¨ç¤ºï¼ˆæ½œåœ¨å˜é‡ï¼‰ï¼Œä¸»è¦ç”¨äºé™ç»´å’Œç‰¹å¾æå–ã€‚  
- VAE åœ¨ AE çš„åŸºç¡€ä¸Šå¼•å…¥äº†æ¦‚ç‡å»ºæ¨¡å’Œå˜åˆ†æ¨æ–­ï¼Œä½¿æ¨¡å‹å…·å¤‡ç”Ÿæˆèƒ½åŠ›ã€‚  
- VQ-VAEï¼ˆå‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼‰è¿›ä¸€æ­¥å¼•å…¥äº†ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œé€šè¿‡å‘é‡é‡åŒ–æ›¿ä»£è¿ç»­æ½œåœ¨åˆ†å¸ƒï¼Œæ›´é€‚åˆç¦»æ•£ç»“æ„å»ºæ¨¡ï¼ˆå¦‚è¯­éŸ³ã€å›¾åƒä¸­çš„ç¬¦å·åŒ–ç‰¹å¾ï¼‰ï¼Œå¹¶ä¸ºåç»­çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ Transformer è§£ç å™¨ï¼‰æä¾›ç¦»æ•£ token è¡¨ç¤ºã€‚

| ç‰¹æ€§                 | è‡ªç¼–ç å™¨ï¼ˆAEï¼‰                  | å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰                                            | å‘é‡é‡åŒ– VAEï¼ˆVQ-VAEï¼‰                               |
| -------------------- | ------------------------------- | -------------------------------------------------------------- | ---------------------------------------------------- |
| ç¼–ç å™¨è¾“å‡º           | ä¸€ä¸ªç¡®å®šæ€§çš„å‘é‡ \( z = f(x) \) | ä¸€ä¸ªåˆ†å¸ƒ \( q(z \vert x) = \mathcal{N}(\mu(x), \sigma^2(x)) \) | æœ€è¿‘é‚»æŸ¥æ‰¾å¾—åˆ°çš„ç¦»æ•£ç æœ¬å‘é‡ \( z \in \mathcal{E} \) |
| è§£ç å™¨è¾“å…¥           | å›ºå®šå‘é‡ \( z \)                | ä»åˆ†å¸ƒä¸­é‡‡æ ·çš„ \( z \sim q(z \vert x) \)                       | ç¦»æ•£ç æœ¬å‘é‡ \( z \)                                 |
| æ½œåœ¨ç©ºé—´ç±»å‹         | è¿ç»­ã€ç¡®å®šæ€§                    | è¿ç»­ã€æ¦‚ç‡åˆ†å¸ƒ                                                 | ç¦»æ•£ã€æœ‰é™çš„ç æœ¬ï¼ˆcodebookï¼‰                         |
| è®­ç»ƒç›®æ ‡             | æœ€å°åŒ–é‡æ„è¯¯å·®ï¼ˆå¦‚ MSEï¼‰        | æœ€å¤§åŒ–å˜åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰                                         | é‡æ„è¯¯å·® + codebook æŸå¤± + commitment æŸå¤±           |
| æ˜¯å¦ä¸ºç”Ÿæˆæ¨¡å‹       | å¦                              | æ˜¯                                                             | æ˜¯                                                   |
| æ˜¯å¦æœ‰æ¦‚ç‡å»ºæ¨¡       | å¦                              | æœ‰ï¼ˆå¯¹æ½œåœ¨å˜é‡åˆ†å¸ƒå»ºæ¨¡ï¼‰                                       | æ— ï¼ˆä½†æä¾›ç¦»æ•£ç¬¦å·åŒ–æ½œåœ¨ç©ºé—´ï¼‰                       |
| æ˜¯å¦å¯é‡‡æ ·ç”Ÿæˆæ–°æ•°æ® | å¦                              | å¯ä»å…ˆéªŒ \( p(z) \) é‡‡æ ·ç”Ÿæˆæ•°æ®                               | å¯é€šè¿‡é‡‡æ ·ç¦»æ•£ token å¹¶è§£ç ç”Ÿæˆæ•°æ®                  |
| æ˜¯å¦ä½¿ç”¨KLæ•£åº¦       | å¦                              | æ˜¯ï¼ˆçº¦æŸæ½œåœ¨åˆ†å¸ƒæ¥è¿‘å…ˆéªŒï¼‰                                     | å¦ï¼ˆæ”¹ç”¨å‘é‡é‡åŒ–å’Œé¢å¤–æŸå¤±å‡½æ•°æ›¿ä»£ï¼‰                 |

#### reparameterization trick

å®šä¹‰ï¼šé‡å‚æ•°åŒ–**å°†éšæœºå˜é‡ä»ä¸å¯å¯¼çš„é‡‡æ ·æ“ä½œä¸­è§£è€¦å‡ºæ¥**çš„æ–¹æ³•ï¼Œè®©é‡‡æ ·æ“ä½œå¯ä»¥å‚ä¸æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚

åŸç†ï¼šä»–æ²¡æœ‰æ¶ˆé™¤éšæœºé‡‡æ ·ï¼Œåªæ˜¯å°†éšæœºé‡‡æ ·å¯¹æ¢¯åº¦ä¼ æ’­çš„å½±å“é™åˆ°äº†æœ€ä½.

ä¸¾ä¾‹ï¼šå¦‚æœä½ æœ‰ä¸€ä¸ªéšæœºå˜é‡ $z \sim \mathcal{N}(\mu, \sigma^2)$ï¼Œç›´æ¥ä»è¿™ä¸ªåˆ†å¸ƒé‡‡æ ·ï¼Œæ¢¯åº¦æ— æ³•é€šè¿‡ $\mu, \sigma$ ä¼ æ’­ã€‚é‚£å°±å¯ä»¥æŒ‰ç…§ä¸‹å¼ä»éšæœºé‡‡æ · $z$ è½¬æ¢ä¸ºéšæœºé‡‡æ · $\epsilon$ã€‚

$$
\mathcal{N}(z; \mu, \sigma^2)
= \mu + \mathcal{N}(\epsilon'; 0, \sigma^2)
= \mu + \sigma \cdot \mathcal{N}(\epsilon; 0, 1)
$$

PyTorch ä»£ç ç¤ºä¾‹ï¼š

```python
import torch

# å‡è®¾ç¼–ç å™¨è¾“å‡ºçš„å‡å€¼å’Œæ ‡å‡†å·®
mu = torch.tensor([0.0, 1.0], requires_grad=True)
log_sigma = torch.tensor([0.0, 0.0], requires_grad=True)  # é€šå¸¸è¾“å‡º log(sigma) é¿å…è´Ÿæ•°
sigma = torch.exp(log_sigma)

# é‡å‚æ•°åŒ–é‡‡æ ·
epsilon = torch.randn_like(mu)  # ä»æ ‡å‡†æ­£æ€é‡‡æ ·
z = mu + sigma * epsilon  # z å¯å¯¼

# å‡è®¾ä¸€ä¸ªç®€å•çš„æŸå¤±å‡½æ•°
loss = (z**2).sum()
loss.backward()

print("grad mu:", mu.grad)
print("grad log_sigma:", log_sigma.grad)
```


#### é‡è¦çš„diffusionç›¸å…³çš„è®ºæ–‡

è¿™äº›è®ºæ–‡å‡ä¸º weng lilianå†™ä½œdiffusionåšæ–‡çš„æ—¶å€™æ‰€å¼•ç”¨çš„æ–‡ç« ï¼ŒåŒæ ·ä¹Ÿæ˜¯diffusioné¢†åŸŸæœ€é‡è¦çš„ä¸€äº›æ–‡ç« ã€‚

| è®ºæ–‡                                                                                                                                                                   | ä»‹ç»                                                                                                     |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| [1] Jascha Sohl-Dickstein et al. â€œDeep Unsupervised Learning using Nonequilibrium Thermodynamics.â€ ICML 2015.                                                          | æœ€æ—©æå‡ºåŸºäºæ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆå»ºæ¨¡æ€æƒ³ï¼Œå°†å‰å‘å™ªå£°æ‰©æ•£ä¸åå‘è¿‡ç¨‹è”ç³»èµ·æ¥ï¼Œå¥ å®šäº† diffusion model çš„åŸºç¡€ã€‚    |
| [2] Max Welling & Yee Whye Teh. â€œBayesian learning via stochastic gradient langevin dynamics.â€ ICML 2011.                                                              | æå‡º SGLD æ–¹æ³•ï¼Œå°†æ¢¯åº¦ä¸‹é™ä¸ Langevin åŠ¨åŠ›å­¦ç»“åˆï¼Œç”¨å™ªå£°é©±åŠ¨é‡‡æ ·ï¼Œå¯å‘äº†åç»­åŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹çš„ç”Ÿæˆæ¨¡å‹ã€‚ |
| [3] Yang Song & Stefano Ermon. â€œGenerative modeling by estimating gradients of the data distribution.â€ NeurIPS 2019.                                                   | æå‡º **score matching + Langevin dynamics** æ¡†æ¶ï¼Œé€šè¿‡ä¼°è®¡æ•°æ®åˆ†å¸ƒæ¢¯åº¦ï¼ˆscore functionï¼‰è¿›è¡Œç”Ÿæˆå»ºæ¨¡ã€‚   |
| [4] Yang Song & Stefano Ermon. â€œImproved techniques for training score-based generative models.â€ NeurIPS 2020.                                                         | æå‡ºå¤šå™ªå£°å°ºåº¦è®­ç»ƒå’Œæ”¹è¿›çš„é‡‡æ ·æŠ€å·§ï¼Œä½¿ score-based models æ€§èƒ½å¤§å¹…æå‡ã€‚                                 |
| [5] Jonathan Ho et al. â€œDenoising diffusion probabilistic models.â€ arXiv 2020.                                                                                         | æå‡º **DDPM**ï¼Œå°†æ‰©æ•£æ¨¡å‹ä¸å˜åˆ†æ¨æ–­ç»“åˆï¼Œé¦–æ¬¡åœ¨å›¾åƒç”Ÿæˆä¸Šå–å¾—æ¥è¿‘ GAN çš„æ•ˆæœã€‚                           |
| [6] Jiaming Song et al. â€œDenoising diffusion implicit models.â€ arXiv 2020.                                                                                             | æå‡º **DDIM**ï¼Œæä¾›ç¡®å®šæ€§é‡‡æ ·æ–¹å¼ï¼Œå¤§å¹…å‡å°‘é‡‡æ ·æ­¥éª¤å¹¶ä¿æŒé«˜è´¨é‡ç”Ÿæˆã€‚                                    |
| [7] Alex Nichol & Prafulla Dhariwal. â€œImproved denoising diffusion probabilistic models.â€ arXiv 2021.                                                                  | æå‡ºæ”¹è¿›è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚ä½™å¼¦å™ªå£°è°ƒåº¦ã€æ•°æ®å¢å¼ºï¼‰ï¼Œæ˜¾è‘—æå‡ DDPM æ€§èƒ½ã€‚                                       |
| [8] Prafula Dhariwal & Alex Nichol. â€œDiffusion Models Beat GANs on Image Synthesis.â€ arXiv 2021.                                                                       | å±•ç¤ºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Š GANï¼Œæ¨åŠ¨æ‰©æ•£æ¨¡å‹æˆä¸ºä¸»æµç”Ÿæˆæ–¹æ³•ã€‚                                     |
| [9] Jonathan Ho & Tim Salimans. â€œClassifier-Free Diffusion Guidance.â€ NeurIPS 2021 Workshop.                                                                           | æå‡º **æ— åˆ†ç±»å™¨å¼•å¯¼**ï¼Œé€šè¿‡æ¡ä»¶/æ— æ¡ä»¶æ¨¡å‹å·®å€¼å®ç° controllable generationï¼Œæˆä¸ºä¸»æµæ§åˆ¶æ–¹æ³•ã€‚           |
| [10] Yang Song, et al. â€œScore-Based Generative Modeling through Stochastic Differential Equations.â€ ICLR 2021.                                                         | å°†æ‰©æ•£ä¸ SDE ç»Ÿä¸€ï¼Œæå‡ºè¿ç»­æ—¶é—´ score-based frameworkï¼Œè¿æ¥ DDPM å’Œ SDEã€‚                                |
| [11] Alex Nichol, Prafulla Dhariwal & Aditya Ramesh, et al. â€œGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.â€ ICML 2022. | æå‡º **GLIDE**ï¼Œç»“åˆæ‰©æ•£ä¸ CLIP æ–‡æœ¬å¼•å¯¼ï¼Œå®ç°é«˜è´¨é‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚                                     |
| [12] Jonathan Ho, et al. â€œCascaded diffusion models.â€ JMLR 2022.                                                                                                       | æå‡ºçº§è”æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡é€çº§æé«˜åˆ†è¾¨ç‡ç”Ÿæˆé«˜ä¿çœŸå›¾åƒã€‚                                                     |
| [13] Aditya Ramesh et al. â€œHierarchical Text-Conditional Image Generation with CLIP Latents.â€ arXiv 2022.                                                              | æå‡º **DALLÂ·E 2** çš„æ ¸å¿ƒï¼šåˆ©ç”¨ CLIP latent ä½œä¸ºæ‰©æ•£æ¡ä»¶ï¼Œå®ç°è¯­ä¹‰ä¸€è‡´çš„æ–‡æœ¬-å›¾åƒç”Ÿæˆã€‚                   |
| [14] Chitwan Saharia & William Chan, et al. â€œPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding.â€ arXiv 2022.                              | æå‡º **Imagen**ï¼Œç»“åˆå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£ï¼Œå®ç°å½“æ—¶æœ€ä¼˜çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚                                |
| [15] Rombach & Blattmann, et al. â€œHigh-Resolution Image Synthesis with Latent Diffusion Models.â€ CVPR 2022.                                                            | æå‡º **Latent Diffusion / Stable Diffusion**ï¼Œåœ¨æ½œåœ¨ç©ºé—´è€Œéåƒç´ ç©ºé—´æ‰©æ•£ï¼Œå¤§å¹…æé«˜æ•ˆç‡ã€‚                 |
| [16] Song et al. â€œConsistency Models.â€ arXiv 2023.                                                                                                                     | æå‡ºä¸€è‡´æ€§æ¨¡å‹ï¼Œæ”¯æŒä¸€æ­¥/å°‘æ­¥é‡‡æ ·ï¼Œæå‡ç”Ÿæˆé€Ÿåº¦ã€‚                                                        |
| [17] Salimans & Ho. â€œProgressive Distillation for Fast Sampling of Diffusion Models.â€ ICLR 2022.                                                                       | æå‡ºè’¸é¦æ–¹æ³•ï¼Œå°†æ‰©æ•£æ¨¡å‹åŠ é€Ÿè‡³å°‘é‡é‡‡æ ·æ­¥æ•°ã€‚                                                             |
| [18] Ronneberger, et al. â€œU-Net: Convolutional Networks for Biomedical Image Segmentation.â€ MICCAI 2015.                                                               | æå‡º U-Net æ¶æ„ï¼Œåæ¥æˆä¸ºæ‰©æ•£æ¨¡å‹å»å™ªç½‘ç»œçš„æ ‡å‡† backboneã€‚                                               |
| [19] Peebles & Xie. â€œScalable diffusion models with transformers.â€ ICCV 2023.                                                                                          | å°†æ‰©æ•£æ¨¡å‹ backbone æ¢ä¸º transformerï¼Œæå‡å¯æ‰©å±•æ€§ä¸ç”Ÿæˆè´¨é‡ã€‚                                           |
| [20] Zhang et al. â€œAdding Conditional Control to Text-to-Image Diffusion Models.â€ arXiv 2023.                                                                          | æå‡º **ControlNet**ï¼Œåœ¨å·²æœ‰æ‰©æ•£æ¨¡å‹ä¸Šæ·»åŠ æ¡ä»¶æ§åˆ¶å±‚ï¼Œå®ç°å¯æ§å›¾åƒç”Ÿæˆã€‚                                  |


### æ¦‚ç‡è®º

#### è”åˆåˆ†å¸ƒï¼Œè¾¹ç¼˜åˆ†å¸ƒå’Œæ¡ä»¶åˆ†å¸ƒ

- è”åˆåˆ†å¸ƒ $P(A, B)$ï¼šå…¨æ™¯åœ°å›¾ï¼ˆåŒ…å«æ‰€æœ‰ç»„åˆçš„æ¦‚ç‡ï¼‰ã€‚
- è¾¹ç¼˜åˆ†å¸ƒ $P(B)$ï¼šå…¨æ™¯åœ°å›¾æŠ•å½±åˆ°æŸä¸€ä¸ªè½´ã€‚
- æ¡ä»¶åˆ†å¸ƒ $P(A \vert B)$ï¼šå…¨æ™¯åœ°å›¾åˆ‡ä¸€æ¡çº¿ï¼ˆå·²çŸ¥å¦ä¸€å˜é‡çš„å€¼ï¼‰ï¼Œçœ‹è¿™æ¡çº¿ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒã€‚å…¬å¼ä¸ºï¼š$P(A \vert B) = \frac{P(A, B)}{P(B)}$

#### Gaussian distribution

é«˜æ–¯åˆ†å¸ƒï¼ˆGaussian distributionï¼‰ä¹Ÿè¢«ç§°ä¸º**æ­£æ€åˆ†å¸ƒ**ï¼Œ$\mathcal{N}(\mu, \sigma)$ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDF, Probability Density Functionï¼‰ä¸ºï¼š

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \; \exp\!\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
$$

- $\mu$ï¼šå‡å€¼ï¼ˆmeanï¼‰ï¼Œå†³å®šåˆ†å¸ƒçš„ä¸­å¿ƒä½ç½®
- $\sigma$ï¼šæ ‡å‡†å·®ï¼ˆstandard deviationï¼‰ï¼Œå†³å®šåˆ†å¸ƒçš„å®½åº¦
- $\sigma^2$ï¼šæ–¹å·®ï¼ˆvarianceï¼‰
- $\exp(\cdot)$ï¼šè‡ªç„¶æŒ‡æ•°å‡½æ•° $e^x$

å…¶ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDF, Cumulative Distribution Functionï¼‰ä¸ºï¼š

$$
F(x) = P(X \le x) = \frac{1}{2} \left[ 1 + \operatorname{erf} \!\left( \frac{x - \mu}{\sigma\sqrt{2}} \right) \right]
$$

- $\operatorname{erf}(\cdot)$ï¼šè¯¯å·®å‡½æ•°ï¼ˆerror functionï¼‰ï¼Œæ˜¯æ— æ³•ç”¨åˆç­‰å‡½æ•°è¡¨ç¤ºçš„ç§¯åˆ†å‡½æ•°ï¼Œå®šä¹‰ä¸º

$$
\operatorname{erf}(z) = \frac{2}{\sqrt{\pi}} \int_{0}^{z} e^{-t^2} \, dt
$$

æ­¤å¤–ï¼Œisotropic Gaussian distributionæ˜¯æŒ‡å„æ–¹å‘éƒ½å‡åŒ€çš„é«˜æ–¯åˆ†å¸ƒï¼Œå³å‘é‡ä¸­çš„æ¯ä¸ªåˆ†é‡éƒ½ç¬¦åˆ $\mathcal{N}(0, \mathbf{I})$ã€‚

#### Bayesâ€™ rule

è´å¶æ–¯å…¬å¼ï¼ˆBayesâ€™ Ruleï¼‰æ˜¯æ¦‚ç‡è®ºä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæ³•åˆ™ï¼Œç”¨äºåœ¨å·²çŸ¥æ¡ä»¶ä¸‹æ›´æ–°äº‹ä»¶çš„æ¦‚ç‡ã€‚å®ƒçš„åŸºæœ¬å½¢å¼æ˜¯ï¼š

\[
P(A \vert B) = \frac{P(B \vert A) \cdot P(A)}{P(B)}
\]

å…¶ä¸­ï¼š

- \(P(A)\)ï¼šäº‹ä»¶ A çš„å…ˆéªŒæ¦‚ç‡ï¼ˆåœ¨è§‚å¯Ÿ B ä¹‹å‰å¯¹ A çš„ä¿¡å¿µï¼‰
- \(P(B \vert A)\)ï¼šåœ¨ A å‘ç”Ÿçš„å‰æä¸‹ï¼Œè§‚å¯Ÿåˆ° B çš„å¯èƒ½æ€§ï¼ˆä¼¼ç„¶ï¼‰
- \(P(B)\)ï¼šäº‹ä»¶ B çš„è¾¹é™…æ¦‚ç‡ï¼ˆæ‰€æœ‰å¯èƒ½æƒ…å†µä¸‹ B å‘ç”Ÿçš„æ¦‚ç‡ï¼‰
- \(P(A \vert B)\)ï¼šåœ¨è§‚å¯Ÿåˆ° B ä¹‹åï¼Œäº‹ä»¶ A çš„åéªŒæ¦‚ç‡ï¼ˆæ›´æ–°åçš„ä¿¡å¿µï¼‰



#### å…ˆéªŒï¼Œä¼¼ç„¶ï¼Œä¸åéªŒ

| æ¦‚å¿µ | ç±»æ¯”è§£é‡Š                 | åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ç±»æ¯”                                     |
| ---- | ------------------------ | ------------------------------------------------------ |
| å…ˆéªŒ | åœ¨è§‚å¯Ÿä»»ä½•æ•°æ®ä¹‹å‰å¯¹å˜é‡çš„å‡è®¾åˆ†å¸ƒ             | $q(\mathbf{x}_t) \sim \mathcal{N}(0, I)$               |
| ä¼¼ç„¶ | æŸå‡è®¾/æ¨¡å‹å‚æ•°ä¸‹ï¼Œè§‚æµ‹æ•°æ®å‡ºç°çš„æ¦‚ç‡ | $p_\theta(\mathbf{x}_{t-1}\mid \mathbf{x}_t)$               |
| åéªŒ | ç»“åˆå…ˆéªŒä¸è§‚æµ‹æ•°æ®ä¹‹åå¯¹æ½œåœ¨å˜é‡çš„æ›´æ–°åˆ†å¸ƒ     | $q(\mathbf{x}_{t-1}\mid \mathbf{x}_t, \mathbf{x}_0)$ |

ä¸‰è€…ä¹‹é—´çš„å…³ç³»æ˜¯ï¼š

\[
\text{åéªŒ} = \frac{\text{ä¼¼ç„¶} \times \text{å…ˆéªŒ}}{\text{è¯æ®}}
\quad\leftrightarrow\quad
p(\text{å‚æ•°} | \text{æ•°æ®}) = \frac{p(\text{æ•°æ®} | \text{å‚æ•°}) \cdot p(\text{å‚æ•°})}{p(\text{æ•°æ®})}
\]


#### Stein Score / Score Function

è¿™é‡Œæˆ‘ä»¬ç”¨çš„æ˜¯æ¦‚ç‡è®ºä¸­çš„æ¦‚å¿µ

â€œscore functionâ€ï¼ˆä¹Ÿå« **Stein score function** æˆ–ç®€ç§° **score**ï¼‰çš„æå‡ºæœ€æ—©å¯ä»¥è¿½æº¯åˆ°ç»Ÿè®¡å­¦é‡Œçš„ **Fisher (1920s)**ã€‚

å…·ä½“è„‰ç»œæ˜¯è¿™æ ·çš„ï¼š

- **Score functionï¼ˆå¾—åˆ†å‡½æ•°ï¼‰**
  æœ€ç»å…¸çš„å®šä¹‰æ˜¯ç»Ÿè®¡å­¦é‡Œå¯¹å¯¹æ•°ä¼¼ç„¶çš„æ¢¯åº¦ï¼š

  $$
  s_\theta(x) = \nabla_\theta \log p_\theta(x)
  $$

  è¿™ä¸ªæ¦‚å¿µæœ€æ—©è§äº **R.A. Fisher** çš„æå¤§ä¼¼ç„¶ä¼°è®¡ç†è®ºä¸­ï¼Œå¤§çº¦ **1922 å¹´ Fisher å‘è¡¨çš„ã€ŠOn the Mathematical Foundations of Theoretical Statisticsã€‹** å°±å·²ç»åœ¨ç”¨ã€‚
  æ‰€ä»¥ä¸¥æ ¼æ¥è¯´ï¼Œ**score function æ˜¯ Fisher æå‡ºæ¥çš„**ã€‚

- **Stein score / Steinâ€™s identity**
  åœ¨æ¦‚ç‡è®ºä¸å‡½æ•°åˆ†æä¸­ï¼Œåæ¥ **Charles Stein** åœ¨ 1970 å¹´ä»£å‘å±•äº† **Steinâ€™s method**ï¼ˆ1972 å¹´è®ºæ–‡ã€ŠA bound for the error in the normal approximation to the distribution of a sum of dependent random variablesã€‹ï¼‰ï¼Œæå‡ºäº†ç°åœ¨å¸¸ç”¨çš„â€œStein identityâ€ï¼š

  $$
  \mathbb{E}_{p(x)}[\nabla_x \log p(x) f(x)] = - \mathbb{E}_{p(x)}[\nabla_x f(x)]
  $$

  å…¶ä¸­çš„ $\nabla_x \log p(x)$ å°±æ˜¯æ‰€è°“çš„ **Stein score function**ã€‚

ğŸ”¹ æ€»ç»“ï¼š

- **Score function**ï¼ˆå¯¹æ•°ä¼¼ç„¶çš„æ¢¯åº¦ï¼‰ â†’ Fisher, 1922ã€‚
- **Stein score / Steinâ€™s identity**ï¼ˆåŸºäºåˆ†å¸ƒçš„æ¢¯åº¦ç‰¹å¾ï¼‰ â†’ Stein, 1972ã€‚

è¦çœ‹ä½ é—®çš„æ˜¯å“ªä¸€ä¸ªè¯­å¢ƒï¼š

- å¦‚æœæ˜¯ç»Ÿè®¡å­¦ MLE é‡Œçš„ **score function**ï¼Œæºå¤´æ˜¯ **Fisher (1922)**ã€‚
- å¦‚æœæ˜¯æ¦‚ç‡è®ºé‡Œç”¨åœ¨ Steinâ€™s method / score matching çš„ **Stein score function**ï¼Œæºå¤´æ˜¯ **Stein (1972)**ã€‚

### ä¿¡æ¯è®º

ä¿¡æ¯è®ºç”¨äº‹ä»¶åŠå…¶å‘ç”Ÿæ¦‚ç‡æ¥è¡¡é‡äº‹ä»¶æœ¬èº«çš„ä¸ç¡®å®šæ€§ï¼Œä¸ç¡®å®šæ€§è¶Šé«˜ï¼Œä¿¡æ¯é‡è¶Šå¤§ã€‚

#### ä¿¡æ¯é‡ï¼Œé¦™å†œç†µï¼Œäº¤å‰ç†µä¸ç›¸å¯¹ç†µ

| ç†µç±»å‹     | å…¬å¼                                                      | è§£é‡Š                                   |
| ---------- | --------------------------------------------------------- | -------------------------------------- |
| **ä¿¡æ¯é‡** | $I(x) = -\log p(x)$                                       | è¡¡é‡å•ä¸ªäº‹ä»¶ $x$ çš„ä¸ç¡®å®šæ€§            |
| **é¦™å†œç†µ** | $H(p) = -\sum_i p(x_i) \log p(x_i)$                       | è¡¡é‡åˆ†å¸ƒ $p$ çš„ä¸ç¡®å®šæ€§                |
| **äº¤å‰ç†µ** | $H(p,q) = -\sum_i p(x_i) \log q(x_i)$                     | è¡¡é‡ç”¨ $q$ è¡¨ç¤º $p$ çš„å¹³å‡ä¿¡æ¯é‡       |
| **ç›¸å¯¹ç†µ** | $D_{KL}(p\|q) = \sum_i p(x_i) \log \frac{p(x_i)}{q(x_i)}$ | è¡¡é‡ $p$ å’Œ $q$ çš„å·®å¼‚ï¼Œå¤šä»˜å‡ºçš„ä¿¡æ¯é‡ |

#### KL æ•£åº¦

KL æ•£åº¦ï¼ˆKullbackâ€“Leibler Divergenceï¼‰ï¼Œä¹Ÿå«ç›¸å¯¹ç†µï¼ˆRelative Entropyï¼‰ï¼Œå®ƒç”¨æ¥è¡¡é‡ **ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚** çš„ä¸€ç§ä¿¡æ¯è®ºåº¦é‡ã€‚

$$
\begin{align}
& D_{\mathrm{KL}}(P \,\|\, Q) = \mathbb{E}_{X \sim P(X)} \left[ \log \frac{P(X)}{Q(X)} \right] \\
\text{ç¦»æ•£åˆ†å¸ƒï¼Œå¯å±•å¼€ä¸º:}\quad & D_{\mathrm{KL}}(P \,\|\, Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} \\
\text{è¿ç»­åˆ†å¸ƒï¼Œå¯å±•å¼€ä¸º:}\quad & D_{\mathrm{KL}}(P \,\|\, Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx
\end{align}
$$

è¿™é‡Œï¼š

- $P$ æ˜¯â€œçœŸå®â€åˆ†å¸ƒï¼ˆæˆ–ç›®æ ‡åˆ†å¸ƒï¼‰ã€‚
- $Q$ æ˜¯ç”¨æ¥è¿‘ä¼¼ $P$ çš„åˆ†å¸ƒã€‚
- $\log$ çš„åº•é€šå¸¸å–è‡ªç„¶å¯¹æ•°ï¼ˆä¿¡æ¯å•ä½ä¸º natsï¼‰ï¼Œå– 2 åˆ™å•ä½ä¸º bitsã€‚

ç›´è§‚ç†è§£:

- å¦‚æœä½ ç”¨ $Q$ ä½œä¸ºç¼–ç ç­–ç•¥å»ç¼–ç å®é™…ä¸Šæ¥è‡ª $P$ çš„æ•°æ®ï¼Œé‚£ä¹ˆå¹³å‡æ¯ä¸ªæ ·æœ¬ä¼šå¤šèŠ± $D_{\mathrm{KL}}(P\|Q)$ ä¸ªä¿¡æ¯å•ä½ï¼ˆnats/bitsï¼‰ã€‚
- KL æ•£åº¦è¡¨ç¤ºä½¿ç”¨åˆ†å¸ƒ $Q$ æ¥æ›¿ä»£ $P$ æ—¶ä¸¢å¤±çš„ä¿¡æ¯é‡ã€‚
- å…¬å¼é‡Œçš„ $\log \frac{P(x)}{Q(x)}$ æ˜¯ **å¯¹æ•°æ¦‚ç‡æ¯”**ï¼Œä¹˜ä¸Š $P(x)$ å¹¶å–æœŸæœ›ï¼Œå°±æ˜¯å¹³å‡çš„æ¦‚ç‡æ¯”å·®å¼‚ã€‚

æ€§è´¨:

- **éè´Ÿæ€§**: $D_{\mathrm{KL}}(P \,\|\, Q) \ge 0$
- **éå¯¹ç§°æ€§**: $D_{\mathrm{KL}}(P \,\|\, Q) \neq D_{\mathrm{KL}}(Q \,\|\, P)$
- **ç›¸å¯¹ç†µ=äº¤å‰ç†µ-é¦™å†œç†µ**: $D_{\mathrm{KL}}(P \,\|\, Q) = H(P, Q) - H(P)$

### éšæœºè¿‡ç¨‹

éšæœºè¿‡ç¨‹ï¼ˆStochastic Processï¼‰æ˜¯éšæ—¶é—´ï¼ˆæˆ–ç©ºé—´ï¼‰æ¼”åŒ–çš„éšæœºå˜é‡æ—ã€‚ä¸€ä¸ªéšæœºè¿‡ç¨‹å¯ä»¥å†™æˆï¼š

$$
\{ X_t \}_{t \in T}
$$

* $t$ï¼šç´¢å¼•é›†ï¼Œå¯ä»¥æ˜¯ **ç¦»æ•£çš„**ï¼ˆå¦‚æ•´æ•°æ—¶é—´ç‚¹ $t=0,1,2,\dots$ï¼‰æˆ– **è¿ç»­çš„**ï¼ˆå¦‚å®æ•°æ—¶é—´ $t \ge 0$ï¼‰ã€‚
* $X_t$ï¼šåœ¨æ¯ä¸ªæ—¶é—´ç‚¹ $t$ ä¸Šçš„ä¸€ä¸ªéšæœºå˜é‡ã€‚
* æ•´ä¸ªè¿‡ç¨‹å°±æ˜¯ä¸€ç»„éšæœºå˜é‡ç»„æˆçš„æ—ï¼Œåæ˜ ç³»ç»Ÿéš $t$ æ¼”åŒ–æ—¶çš„éšæœºæ€§ã€‚

ç›´è§‚ç†è§£ï¼š

* éšæœºå˜é‡æ˜¯â€œæŸä¸ªæ—¶åˆ»çš„éšæœºé‡â€ï¼›
* éšæœºè¿‡ç¨‹æ˜¯â€œéšæ—¶é—´å˜åŒ–çš„ä¸€ä¸²éšæœºé‡â€ã€‚

#### Markov Property

ä¸€ä¸ªéšæœºè¿‡ç¨‹è‹¥æ»¡è¶³

$$
P(X_{t+1} \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} \mid X_t)
$$

å°±è¯´å®ƒå…·æœ‰é©¬å°”å¯å¤«æ€§è´¨ã€‚

æœªæ¥åªä¾èµ–äºç°åœ¨ï¼Œè€Œä¸è¿‡å»æ— å…³ã€‚è¿™æ˜¯å¾ˆå¤šæ¨¡å‹çš„æ ¸å¿ƒå‡è®¾ï¼Œæ¯”å¦‚é©¬å°”å¯å¤«é“¾ã€éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰ã€é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚

## Citation

{{< bibtex >}}

## References

[^ho_ddpm]: **Ho, Jonathan, Ajay Jain, and Pieter Abbeel.** â€œDenoising Diffusion Probabilistic Models.â€ _Advances in Neural Information Processing Systems_, edited by H. Larochelle et al., vol. 33, Curran Associates, Inc., 2020, pp. 6840â€“6851. https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.

[^nichol_improved_ddpm]: **Nichol, Alexander Quinn, and Prafulla Dhariwal.** â€œImproved Denoising Diffusion Probabilistic Models.â€ _Proceedings of the 38th International Conference on Machine Learning_, edited by Marina Meila and Tong Zhang, vol. 139, Proceedings of Machine Learning Research, 18â€“24 July 2021, pp. 8162â€“8171. PMLR. https://proceedings.mlr.press/v139/nichol21a.html.

[^mc_candlish_grad_noise]: **McCandlish, Sam, et al.** _An Empirical Model of Large-Batch Training_. arXiv, 14 Dec. 2018, https://arxiv.org/abs/1812.06162.

[^lilian_diffusion]: **Weng, Lilian.** â€œWhat Are Diffusion Models?â€ _Lil'Log_, 11 July 2021, https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.

[^lilian_ae]: **Weng, Lilian.** â€œFrom Autoencoder to Beta-VAE.â€ _Lil'Log_, 12 Aug. 2018, https://lilianweng.github.io/posts/2018-08-12-vae/.

[^wiki_closed]: â€œClosed-form Expression.â€ _Wikipedia_, Wikimedia Foundation, https://en.wikipedia.org/wiki/Closed-form_expression.
