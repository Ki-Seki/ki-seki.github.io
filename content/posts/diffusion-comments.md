---
date: '2025-08-15T01:41:53+08:00'
title: 'ã€ŠWhat are Diffusion Models?ã€‹æ³¨é‡Š'
author:
  - Shichao Song
summary: ''
tags: []
math: true
---

<!-- TODOï¼šè®°å¾—emailç»™lilian about this article -->
<!-- TODOï¼šçœ‹çœ‹å¯ä»¥è€ƒè™‘é‡æ–°æ•´ç†ä¸‹æ‰€æœ‰å†…å®¹ï¼Œç›®å‰çœ‹èµ·æ¥æœ‰ç‚¹ä¹±ã€‚ -->

æœ¬æ–‡è‡´åŠ›äºåœ¨å‡ ä¹é›¶æ•°å­¦èƒŒæ™¯çŸ¥è¯†å’Œé›¶ç”Ÿæˆæ¨¡å‹çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œå¯¹Lilian Wengçš„ã€ŠWhat are Diffusion Models?ã€‹ [^lilian_diffusion] è¿›è¡Œå®Œå–„çš„æ³¨é‡Šå¯¼è¯»ã€‚

## What are Diffusion Models?

### Forward diffusion process

{{< admonition type=quote title="å‰å‘æ‰©æ•£è¡¨è¾¾å¼" >}}
$$
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I}) \quad
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1})
$$
{{< /admonition >}}

æ˜¯å‰å‘æ‰©æ•£è¿‡ç¨‹çš„ä¸¤ç§è¡¨è¾¾å½¢å¼ï¼Œå•æ­¥æ‰©æ•£è¿‡ç¨‹å’Œæ•´ä½“æ‰©æ•£è¿‡ç¨‹ã€‚

å•æ­¥æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œ

- $\mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})$ è¡¨ç¤º $\mathbf{x}_t$ æœä» å‡å€¼ä¸º $\sqrt{1 - \beta_t} \mathbf{x}_{t-1}$ï¼Œæ–¹å·®ä¸º $\beta_t\mathbf{I}$ çš„é«˜æ–¯åˆ†å¸ƒã€‚
- $\beta_t$ æ˜¯Noise variance schedule parameterï¼Œä»–å¯¹åº”ä¸€ä¸ªvariance scheduleï¼Œ$\{\beta_t \in (0, 1)\}_{t=1}^T$ï¼Œå’Œå­¦ä¹ ç‡è°ƒåº¦æ˜¯ç±»ä¼¼çš„.
- $\beta_t$ å®šä¹‰äº†åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æ¯ä¸ªæ—¶é—´æ­¥çš„æ–¹å·®å¤§å°ï¼Œä¸€èˆ¬æ¥è¯´$\beta_t$é€æ¸å¢å¤§ï¼Œå› æ­¤å’ŒåŸå§‹æ•°æ®å·®å¼‚è¶Šæ¥è¶Šå¤§ï¼ˆ$\sqrt{1 - \beta_t}$ â†“ï¼‰ï¼Œæ•°æ®å˜å¼‚æ€§ä¹Ÿé€æ¸å˜å¤§ï¼ˆ$\beta_t\mathbf{I}$ â†‘ï¼‰ï¼Œæ€»ä½“ä¸Šé€æ¸ä½¿å¾—æ¯ä¸€æ­¥çš„å™ªå£°æ›´å¤šã€‚
- $\beta_t\mathbf{I}$ï¼Œæ˜¯åæ–¹å·®çŸ©é˜µï¼Œä¹Ÿæ˜¯ä¸ªå¯¹è§’çŸ©é˜µï¼Œæ‰€æœ‰å¯¹è§’çº¿å…ƒç´ éƒ½æ˜¯ $\beta_t$. æ¯ä¸€ç»´éƒ½åŠ ç›¸åŒå¼ºåº¦çš„å™ªå£°ï¼Œä¸åå‘ä»»ä½•æ–¹å‘ã€‚

æ•´ä½“æ‰©æ•£è¿‡ç¨‹åªæ˜¯ä½¿ç”¨é©¬å°”å¯å¤«è¿‡ç¨‹æ€§è´¨ï¼ˆæ¯ä¸€æ­¥åªä¾èµ–å‰ä¸€æ­¥ï¼‰æ¥è¿ä¹˜è€Œå·²ã€‚å®è·µä¸­å› ä¸ºå¯ä»¥ä½¿ç”¨æ›´ç®€å•çš„è®¡ç®—æ–¹å¼ï¼Œè¯¥å…¬å¼ä¹Ÿä¸å¸¸ç”¨åˆ°ã€‚

{{< admonition type=quote title="å‰å‘æ‰©æ•£è¡¨è¾¾å¼çš„closed formå½¢å¼" >}}
Let $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$.
$$
\begin{aligned}
\mathbf{x}_t
&= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} & \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} & \text{ ;where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians (*).} \\
&= \dots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$
{{< /admonition >}}

æ ¹æ®å•æ­¥æ‰©æ•£è¿‡ç¨‹$q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})$ï¼Œä»¥åŠé‡å‚æ•°åŒ–æŠ€å·§ $z = \mu + \sigma \cdot \epsilon$ï¼Œæˆ‘ä»¬å¯ä»¥é‡å†™å•æ­¥æ‰©æ•£è¿‡ç¨‹ä¸ºï¼š

$$\mathbf{x}_t = \sqrt{1 - \beta_t}\mathbf{x}_{t-1} + \sqrt{\beta_t}\boldsymbol{\epsilon}_{t-1}$$

è¿™æ ·å°±å¯ä»¥è®©æˆ‘ä»¬æ¥é‡å†™æ›´è¯¦ç»†çš„closed formçš„æ¨å¯¼ï¼š

$$
\begin{aligned}
\mathbf{x}_t
&= \sqrt{1 - \beta_t}\mathbf{x}_{t-1} + \sqrt{\beta_t}\boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} & \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&= \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2} \right) + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t (1 - \alpha_{t-1})} \boldsymbol{\epsilon}_{t-2} + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} & \text{ ;where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians (*).} \\
&= \dots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$

(*) Recall that when we merge two Gaussians with different variance, $\mathcal{N}(\mathbf{0}, \sigma_1^2\mathbf{I})$
 and $\mathcal{N}(\mathbf{0}, \sigma_2^2\mathbf{I})$, the new distribution is $\mathcal{N}(\mathbf{0}, (\sigma_1^2 + \sigma_2^2)\mathbf{I})$. Here the merged standard deviation is $\sqrt{\alpha_t (1-\alpha_{t-1}) + (1 - \alpha_t)} = \sqrt{1 - \alpha_t\alpha_{t-1}}$.

{{< admonition type=quote title="Connection with stochastic gradient Langevin dynamics" >}}
$$
\mathbf{x}_t = \mathbf{x}_{t-1} + \frac{\delta}{2} \nabla_\mathbf{x} \log p(\mathbf{x}_{t-1}) + \sqrt{\delta} \boldsymbol{\epsilon}_t
,\quad\text{where }
\boldsymbol{\epsilon}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
$$
{{< /admonition >}}

æ³¨æ„ï¼šè¿™ä¸ªè”ç³»å…¶å®ä»…å­¦ä¹ Diffusionçš„è¯ï¼Œç”¨ä¸åˆ°ã€‚åªæ˜¯æ‰©å±•åœ°å±•ç¤ºå’ŒLangevin dynamicsçš„å…³è”ã€‚è¿™é‡Œå¯ä»¥ç±»æ¯”diffusion modelä¸­é‡å‚æ•°åŒ–åçš„å•æ­¥æ‰©æ•£è¿‡ç¨‹ã€‚

Langevin dynamicsï¼ˆæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼‰æ˜¯ç‰©ç†å­¦ä¸­ç”¨äºæ¨¡æ‹Ÿåˆ†å­è¿åŠ¨çš„ç»Ÿè®¡æ–¹æ³•ã€‚å®ƒæè¿°äº†ç²’å­åœ¨åŠ¿èƒ½åœºä¸­è¿åŠ¨æ—¶å—åˆ°çš„éšæœºæ‰°åŠ¨ï¼ˆæ¯”å¦‚çƒ­å™ªå£°ï¼‰ï¼Œå› æ­¤å¸¸ç”¨äºå»ºæ¨¡å¤æ‚ç³»ç»Ÿçš„éšæœºè¡Œä¸ºã€‚

Stochastic Gradient Langevin Dynamicsï¼ˆSGLDï¼Œéšæœºæ¢¯åº¦æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼‰æ˜¯å°† Langevin åŠ¨åŠ›å­¦ä¸æœºå™¨å­¦ä¹ ä¸­çš„éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ç»“åˆèµ·æ¥çš„ä¸€ç§é‡‡æ ·æ–¹æ³•ã€‚å®ƒçš„ç›®æ ‡æ˜¯ä»æŸä¸ªæ¦‚ç‡åˆ†å¸ƒ \( p(x) \) ä¸­é‡‡æ ·ï¼Œè€Œä¸éœ€è¦çŸ¥é“è¿™ä¸ªåˆ†å¸ƒçš„å…·ä½“å½¢å¼ï¼Œåªéœ€è¦çŸ¥é“å®ƒçš„æ¢¯åº¦ã€‚

ä¸Šé¢çš„é‡‡æ ·å…¬å¼æ˜¯ä¸€ä¸ªè¿­ä»£å¼ï¼Œä»–çš„å«ä¹‰æ˜¯ï¼šâ€œåœ¨æ¢¯åº¦æ–¹å‘ä¸Šå‰è¿›ä¸€ç‚¹ï¼ŒåŒæ—¶åŠ å…¥ä¸€äº›éšæœºæ‰°åŠ¨ï¼Œä½¿å¾—æœ€ç»ˆçš„æ ·æœ¬åˆ†å¸ƒé€¼è¿‘ç›®æ ‡åˆ†å¸ƒ \( p(x) \)ã€‚â€ ç›¸å…³ç¬¦å·å«ä¹‰ï¼š

- \( \mathbf{x}_t \)ï¼šç¬¬ \( t \) æ­¥çš„æ ·æœ¬
- \( \frac{\delta}{2} \nabla_\mathbf{x} \log p(\mathbf{x}_{t-1}) \): æ¼‚ç§»é¡¹ï¼Œæ ¹æ®ç›®æ ‡åˆ†å¸ƒçš„æ¢¯åº¦ç§»åŠ¨ï¼Œç±»ä¼¼å—åŠ›ç‰µå¼•ã€‚
  - \( p(x) \)ï¼šç›®æ ‡åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°
  - \( \log p(x) \)ï¼šå¯¹æ•°æ¦‚ç‡å¯†åº¦ï¼Œä¾¿äºè®¡ç®—å’Œä¼˜åŒ–
  - \( \nabla_\mathbf{x} \log p(\mathbf{x}_{t-1}) \)ï¼šå¯¹æ•°æ¦‚ç‡å¯†åº¦çš„æ¢¯åº¦ï¼Œä¹Ÿå« score functionï¼Œè¡¨ç¤ºå½“å‰ç‚¹çš„â€œä¸Šå‡æ–¹å‘â€
- \( \sqrt{\delta} \boldsymbol{\epsilon}_t \): æ‰©æ•£é¡¹ï¼Œåƒå¸ƒæœ—è¿åŠ¨çš„åˆ†å­ç¢°æ’
  - \( \sqrt{\delta} \)ï¼šæ­¥é•¿ï¼ˆstep sizeï¼‰ï¼Œæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„å¹…åº¦
  - \( \epsilon_t \sim \mathcal{N}(0, I) \)ï¼šæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºå™ªå£°ï¼ŒåŠ å…¥éšæœºæ€§ä»¥é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜

æ‰©æ•£æ¨¡å‹çš„åå‘è¿‡ç¨‹ï¼ˆä»å™ªå£°æ¢å¤æ•°æ®ï¼‰å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œæ¯ä¸€æ­¥éƒ½åœ¨åšâ€œå»å™ª + éšæœºæ‰°åŠ¨â€ï¼Œè¿™ä¸ SGLD çš„æ›´æ–°æ–¹å¼éå¸¸ç›¸ä¼¼ï¼š

- éƒ½ä½¿ç”¨äº† **score function**ï¼ˆå³æ¢¯åº¦ï¼‰
- éƒ½åœ¨æ¯ä¸€æ­¥åŠ å…¥äº† **é«˜æ–¯å™ªå£°**
- éƒ½æ˜¯ä¸ºäº†ä»ä¸€ä¸ªå¤æ‚çš„åˆ†å¸ƒä¸­é‡‡æ ·

å› æ­¤ï¼Œæ‰©æ•£æ¨¡å‹çš„reverse diffusion processå¯ä»¥è¢«ç†è§£ä¸ºä¸€ç§ç‰¹æ®Šå½¢å¼çš„ Langevin dynamicsã€‚

### Reverse diffusion process

{{< admonition type=quote title="Reverse diffusion processä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒçš„" >}}
Note that if \(\beta_t\) is small enough, \(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\) will also be Gaussian.
{{< /admonition >}}

ä»¥ä¸‹ä»…ä¸ºç®€å•ç†è§£ï¼Œéä¸¥æ ¼è¯æ˜ã€‚å½“ \(\beta_t\) å¾ˆå°ï¼Œæ„å‘³ç€æ¯ä¸€æ­¥åŠ å…¥çš„å™ªå£°å¾ˆå°‘ï¼Œé‚£ä¹ˆï¼š

- \(\mathbf{x}_t\) ä¸ \(\mathbf{x}_{t-1}\) çš„å…³ç³»éå¸¸æ¥è¿‘çº¿æ€§å˜æ¢åŠ å¾®å°æ‰°åŠ¨ï¼›
- é«˜æ–¯åˆ†å¸ƒçº¿æ€§å˜æ¢ä»ç„¶ä¿æŒé«˜æ–¯å½¢å¼ã€‚
- è¿™ä½¿å¾—åå‘æ¡ä»¶åˆ†å¸ƒä¹Ÿå¯ä»¥è¿‘ä¼¼ä¸ºé«˜æ–¯åˆ†å¸ƒã€‚

{{< admonition type=quote title="Reverse diffusion è¡¨è¾¾å¼" >}}
$$
p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod^T_{t=1} p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) \quad
p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
$$
{{< /admonition >}}

å¯¹åº”äº†æ•´ä½“çš„ï¼Œå’Œå•æ­¥çš„Reverse diffusion processã€‚

ç”±äºæˆ‘ä»¬ä¸å¯èƒ½çŸ¥é“åéªŒçš„ï¼Œå•æ­¥reverse diffusion processçš„å…·ä½“å½¢å¼ï¼Œå› æ­¤éœ€è¦é€šè¿‡ç¥ç»ç½‘ç»œæ¥å­¦ä¹ ã€‚

æ‰€ä»¥è¿™é‡Œçš„é«˜æ–¯åˆ†å¸ƒçš„ä¸¤ä¸ªå‚æ•°æ˜¯å¯å­¦ä¹ çš„å‚æ•°$\boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)$. å…¶ä¸­ $\theta$ æ˜¯ç¥ç»ç½‘ç»œçš„å­¦ä¹ å‚æ•°ã€‚

{{< admonition type=quote title="åéªŒclosed form" >}}
$$
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \color{blue}{\tilde{\boldsymbol{\mu}}}(\mathbf{x}_t, \mathbf{x}_0), \color{red}{\tilde{\beta}_t} \mathbf{I})
$$
{{< /admonition >}}

è¿™é‡Œæ˜¯å…ˆæ”¾äº†ä¸ªç®€å•çš„ç»“è®ºï¼Œ ${\tilde{\boldsymbol{\mu}}}(\mathbf{x}_t, \mathbf{x}_0)$ and $\tilde{\beta}_t$ æ˜¯ä»€ä¹ˆä¸‹æ–‡ä¼šæœ‰è§£é‡Šã€‚

è®©æˆ‘ä»¬å…ˆäº†è§£ä¸‹ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå…¬å¼ã€‚ä¸‹é¢æ˜¯diffusionæ¨¡å‹è®­ç»ƒæ¨ç†ä¸­æ¶‰åŠåˆ°çš„ä¸‰ä¸ªé‡è¦çš„åˆ†å¸ƒã€‚

| åˆ†å¸ƒ                                                  | ä½œç”¨                                             |
| ----------------------------------------------------- | ------------------------------------------------ |
| $q(\mathbf{x}_t \mid \mathbf{x}_0)$                   | **å‰å‘ closed-form**ï¼Œç›´æ¥ä»æ•°æ®åŠ å™ªå¾—åˆ°è®­ç»ƒæ ·æœ¬ |
| $q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)$ | **çœŸå®åéªŒ closed-form**ï¼Œç”¨äºå®šä¹‰è®­ç»ƒç›®æ ‡       |
| $p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$        | **ä¼¼ç„¶**ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹å»æ‹Ÿåˆä¸Šé¢çš„çœŸå®åéªŒ       |

å¯ä»¥çœ‹åˆ°å°±æ˜¯ä»å‰å‘diffusion closed-formæ¥å¾—åˆ°è®­ç»ƒæ ·æœ¬ï¼Œç„¶åé€šè¿‡çœŸå®åéªŒclosed-formæ¥å¾—åˆ°è®­ç»ƒç›®æ ‡ï¼›åå‘æ˜¯åŒ…æ‹¬å‚æ•°ï¼Œéœ€è¦è®­ç»ƒçš„ï¼Œæ˜¯ä¸ªä¼¼ç„¶ï¼Œé‚£ä¹ˆè¿™ä¸ªè®­ç»ƒæ˜¯ä¸ºäº†æ‹Ÿåˆè°å‘¢ï¼Œç­”æ¡ˆå°±æ˜¯çœŸå®åéªŒï¼Œ$q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)$. å¦‚æ­¤ä¸€æ¥ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æœ€ç»ˆçš„å¯ä»¥ç”¨äºæ¨ç†çš„diffusion modelã€‚

---

{{< admonition type=quote title="åéªŒclosed formæ¨å¯¼æ­¥éª¤ä¸€ï¼šæŒ‰bayeså…¬å¼å’ŒGaussianå…¬å¼å±•å¼€" >}}
$$
\begin{aligned}
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)
&= q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } \\
&\propto \exp \Big(-\frac{1}{2} \big(\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\
&= \exp \Big(-\frac{1}{2} \big(\frac{\mathbf{x}_t^2 - 2\sqrt{\alpha_t} \mathbf{x}_t \color{blue}{\mathbf{x}_{t-1}} \color{black}{+ \alpha_t} \color{red}{\mathbf{x}_{t-1}^2} }{\beta_t} + \frac{ \color{red}{\mathbf{x}_{t-1}^2} \color{black}{- 2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0} \color{blue}{\mathbf{x}_{t-1}} \color{black}{+ \bar{\alpha}_{t-1} \mathbf{x}_0^2}  }{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\
&= \exp\Big( -\frac{1}{2} \big( \color{red}{(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})} \mathbf{x}_{t-1}^2 - \color{blue}{(\frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)} \mathbf{x}_{t-1} \color{black}{ + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)}
\end{aligned}
$$
where $C(\mathbf{x}_t, \mathbf{x}_0)$ is some function not involving $\mathbf{x}_{t-1}$ and details are omitted...recall that $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$.
{{< /admonition >}}

è®©æˆ‘ä»¬æŠŠè¿™é‡Œçš„æ¨ç†æ­¥éª¤å†™å®Œå–„ç‚¹ï¼š

$$
\begin{aligned}
&q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \\
%
&= \frac{ q(\mathbf{x}_{t-1}, \mathbf{x}_t \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } \\
%
&= q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } \\
%
&= \mathcal{N}(\mathbf{x}_t; \sqrt{{\alpha}_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})
\frac{ \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0, (1 - \bar{\alpha}_{t-1})\mathbf{I}) }
{ \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I}) } \\
%
&\propto \exp \Big(-\frac{1}{2} \big(\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big)
\quad\text{;where}\quad (*) \\
%
&= \exp \Big(-\frac{1}{2} \big(\frac{\mathbf{x}_t^2 - 2\sqrt{\alpha_t} \mathbf{x}_t \color{blue}{\mathbf{x}_{t-1}} \color{black}{+ \alpha_t} \color{red}{\mathbf{x}_{t-1}^2} }{\beta_t} + \frac{ \color{red}{\mathbf{x}_{t-1}^2} \color{black}{- 2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0} \color{blue}{\mathbf{x}_{t-1}} \color{black}{+ \bar{\alpha}_{t-1} \mathbf{x}_0^2}  }{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\
%
&= \exp\Big( -\frac{1}{2} \big( \color{red}{(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})} \mathbf{x}_{t-1}^2 \color{blue}{- (\frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)} \mathbf{x}_{t-1} \color{black}{ + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)}
\quad\text{;where}\quad (**)
\end{aligned}
$$

(*) æ ¹æ®é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°å¯ä»¥è¿›è¡Œçº¿æ€§ç®€åŒ–

$$
\begin{align}
p(x) 
& = \mathcal{N}(x; \mu, \sigma^2) \\
& = \frac{1}{\sqrt{2\pi\sigma^2}} \; \exp\!\left( -\frac{(x - \mu)^2}{2\sigma^2} \right) \\
& \propto \exp\!\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
\end{align}
$$

(**) $C(\mathbf{x}_t,\mathbf{x}_0)$ é‡Œå‹æ ¹ä¸åŒ…å« $\mathbf{x}_{t-1}$ï¼Œé‚£ä¹ˆ $C(\mathbf{x}_t,\mathbf{x}_0)$ å¯¹äºæœä»é«˜æ–¯åˆ†å¸ƒçš„$\mathbf{x}_{t-1}$ æ¥è¯´å°±æ˜¯å¸¸æ•°é¡¹ï¼Œåé¢å°±å¯ä»¥ç›´æ¥è¢«å¿½ç•¥æ‰ã€‚ç¨åä½ å°±èƒ½çœ‹åˆ°ä¸ºä»€ä¹ˆä¼šè¢«å¿½ç•¥æ‰ã€‚

{{< admonition type=quote title="åéªŒclosed formæ¨å¯¼æ­¥éª¤äºŒï¼šå‡‘å‡ºæ–°çš„é«˜æ–¯åˆ†å¸ƒ" >}}
$$\begin{aligned}
\tilde{\beta}_t
&= 1/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})
= 1/(\frac{\alpha_t - \bar{\alpha}_t + \beta_t}{\beta_t(1 - \bar{\alpha}_{t-1})})
= \color{green}{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \\
\tilde{\boldsymbol{\mu}}_t (\mathbf{x}_t, \mathbf{x}_0)
&= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) \\
&= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0) \color{green}{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \\
&= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0\\
\end{aligned}
$$
{{< /admonition >}}

ä¹‹æ‰€ä»¥è¿™ä¹ˆè®¡ç®—ï¼Œæ˜¯æ ¹æ®é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°å‡‘å‡ºæ¥çš„ã€‚ç”±äºï¼Œ

$$
\mathcal{N}(p(x); \mu, \sigma^2)
\propto \exp\!\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
= \exp\!\left( -\frac{1}{2} (\color{red}{\frac{1}{\sigma^2}x^2} \color{blue}{- \frac{2\mu}{\sigma^2}x} \color{black}{+ \frac{\mu^2}{\sigma^2})} \right)
$$

å†æ ¹æ®ä¹‹å‰çš„è®¡ç®—ï¼š

$$
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \propto \exp\Big( -\frac{1}{2} \big( \color{red}{(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})} \mathbf{x}_{t-1}^2 \color{blue}{- (\frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)} \mathbf{x}_{t-1} \color{black}{ + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)}
$$

æˆ‘ä»¬å¯ä»¥æœ‰:

$$
\begin{aligned}
\frac{1}{\sigma^2}
&= 1/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})
= 1/(\frac{\alpha_t - \bar{\alpha}_t + \beta_t}{\beta_t(1 - \bar{\alpha}_{t-1})})
= \color{green}{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \;
\color{black}{\triangleq \tilde{\beta}_t} \\
%
\mu
&= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) \\
&= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0) \color{green}{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \\
&= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0 \\
&\triangleq \tilde{\boldsymbol{\mu}}_t (\mathbf{x}_t, \mathbf{x}_0) \quad\text{or}\quad \tilde{\boldsymbol{\mu}}_t \\
\end{aligned}
$$

æ­¤æ—¶ï¼š

$$
\begin{align}
& \quad q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \\
%
& \propto \exp\Big( -\frac{1}{2} \big( \color{red}{\frac{1}{\tilde{\beta}_t}} \mathbf{x}_{t-1}^2 \color{blue}{- \frac{2\tilde{\boldsymbol{\mu}}_t}{\tilde{\beta}_t}} \mathbf{x}_{t-1} \color{black}{ + \frac{\tilde{\boldsymbol{\mu}}_t^2}{\tilde{\beta}_t} + C(\mathbf{x}_t, \mathbf{x}_0) - \frac{\tilde{\boldsymbol{\mu}}_t^2}{\tilde{\beta}_t} \big) \Big)} \\
%
& = \exp\Big( -\frac{1}{2} \big( \color{red}{\frac{1}{\tilde{\beta}_t}} \mathbf{x}_{t-1}^2 \color{blue}{- \frac{2\tilde{\boldsymbol{\mu}}_t}{\tilde{\beta}_t}} \mathbf{x}_{t-1} \color{black}{ + \frac{\tilde{\boldsymbol{\mu}}_t^2}{\tilde{\beta}_t} \big) \Big)}
\cdot
\exp\Big( -\frac{1}{2} \big( C(\mathbf{x}_t, \mathbf{x}_0) - \frac{\tilde{\boldsymbol{\mu}}_t^2}{\tilde{\beta}_t} \big) \Big) \\
%
& \propto \exp\Big( -\frac{1}{2} \big( \color{red}{\frac{1}{\tilde{\beta}_t}} \mathbf{x}_{t-1}^2 \color{blue}{- \frac{2\tilde{\boldsymbol{\mu}}_t}{\tilde{\beta}_t}} \mathbf{x}_{t-1} \color{black}{ + \frac{\tilde{\boldsymbol{\mu}}_t^2}{\tilde{\beta}_t} \big) \Big)} \\
%
& = \mathcal{N}(\mathbf{x}_{t-1}; \color{blue}{\tilde{\boldsymbol{\mu}_t}}(\mathbf{x}_t, \mathbf{x}_0), \color{red}{\tilde{\beta}_t} \mathbf{I})
\end{align}
$$

å¾—è¯ã€‚

{{< admonition type=quote title="åŒ–ç®€åéªŒclosed form" >}}
Thanks to the nice property, we can represent $\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t)$ and plug it into the above equation and obtain:

$$
\begin{aligned}
\tilde{\boldsymbol{\mu}}_t
&= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\
&= \color{cyan}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)}
\end{aligned}
$$
{{< /admonition >}}

è¿™ä¸€æ­¥çš„æ„ä¹‰æ˜¯ï¼Œè®©è®¡ç®—å®Œå…¨ä¾èµ–äºå™ªå£°ï¼Œè€Œä¸ä¾èµ–äºçœŸå®æ•°æ®ï¼Œè¿™æ ·å¯ä»¥ç›´æ¥ä»ä»»æ„å™ªå£°ä¸­æ¢å¤å‡ºçœŸå®æ•°æ®ã€‚

å…¶ä¸­æåˆ°çš„nice propertyå°±æ˜¯closed formçš„å‰å‘æ‰©æ•£è¡¨è¾¾å¼ï¼š$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}$

è®©æˆ‘ä»¬æŠŠåŒ–ç®€æ­¥éª¤å†™çš„æ›´å®Œæ•´äº›ï¼š

$$
\begin{aligned}
\tilde{\boldsymbol{\mu}}_t
&= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\
&= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{(1 - \bar{\alpha}_t)\sqrt{\bar{\alpha}_t}} \mathbf{x}_t - \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{(1 - \bar{\alpha}_t)\sqrt{\bar{\alpha}_t}} \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t \\
&= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) + \beta_t \sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_t} \mathbf{x}_t - \frac{\beta_t \sqrt{\bar{\alpha}_{t-1}}}{\sqrt{\bar{\alpha}_t} \sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \\
&\neq \color{cyan}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)}
\end{aligned}
$$

è¿™å’ŒåŸæ–‡ä¸ç¬¦åˆï¼Œå…¶å®ä¹Ÿå’ŒåŸå§‹çš„DDPMè®ºæ–‡[^ho_ddpm]ä¸­çš„è®¡ç®—ä¹Ÿä¸ç¬¦ã€‚æˆ‘æš‚æ—¶è®¤ä¸ºæˆ‘æ˜¯å¯¹çš„ã€‚

<!-- TODO: æœ€åå›æ¥å†çœ‹çœ‹æœ‰ä»€ä¹ˆå…¶ä»–ç†è§£çš„åŠæ³•æ²¡æœ‰ -->

{{< admonition type=quote title="ä»é›¶æ¨å¯¼ varational lower bound">}}
$$
\begin{aligned}
\mathord{-} \log p_\theta(\mathbf{x}_0) 
&\leq - \log p_\theta(\mathbf{x}_0) + D_\text{KL}(q(\mathbf{x}_{1:T}\vert\mathbf{x}_0) \| p_\theta(\mathbf{x}_{1:T}\vert\mathbf{x}_0) ) & \small{\text{; KL is non-negative}}\\
&= - \log p_\theta(\mathbf{x}_0) + \mathbb{E}_{\mathbf{x}_{1:T}\sim q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T}) / p_\theta(\mathbf{x}_0)} \Big] \\
&= - \log p_\theta(\mathbf{x}_0) + \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \log p_\theta(\mathbf{x}_0) \Big] \\
&= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
\text{Let }L_\text{VLB} 
&= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \geq - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0)
\end{aligned}
$$
{{< /admonition >}}

æˆ‘ä»¬å…ˆå¤§è‡´äº†è§£ä¸‹ variational lower boundæ˜¯ä»€ä¹ˆï¼š

1. å¯¹æ•°è¾¹é™…ä¼¼ç„¶, $\log p_\theta(\mathbf{x}_0)$ , å¾ˆéš¾ç›´æ¥ç®—ï¼Œå› ä¸ºæ— æ³•å¯¹æ½œå˜é‡åˆ†å¸ƒä¸­çš„æ‰€æœ‰æƒ…å†µéƒ½è¿›è¡Œç›´æ¥ç§¯åˆ†è®¡ç®—ã€‚æ‰€ä»¥è¦æ‰¾æ›¿ä»£çš„ä¼˜åŒ–ä¸‹ç•Œï¼Œä¼˜åŒ–è¯¥ä¸‹ç•Œå°±ç›¸å½“äºä¼˜åŒ–å¯¹æ•°è¾¹é™…ä¼¼ç„¶
2. å¦‚æœæƒ³è¦å®Œå…¨äº†è§£ç›¸å…³æ¦‚å¿µï¼Œå¼ºçƒˆå»ºè®®é˜…è¯» Lilian Weng çš„å¦ä¸€ç¯‡æ–‡ç«  From Autoencoder to Beta-VAE [^lilian_ae] ä¸­çš„ [ç« èŠ‚ VAE: Variational Autoencoder](https://lilianweng.github.io/posts/2018-08-12-vae/#vae-variational-autoencoder)ã€‚

æˆ‘ä»¬å·²ç»çŸ¥é“äº†å¯¹æ•°è¾¹é™…ä¼¼ç„¶, $\log p_\theta(\mathbf{x}_0)$ æ— æ³•è®¡ç®—ï¼ŒLilianè¿™é‡Œç»™å‡ºäº†ä»é›¶æ¨å¯¼å‡º varational lower bound çš„è¿‡ç¨‹ã€‚è¿™é‡Œæ¨å¯¼æ¯”è¾ƒæ¸…æ™°ï¼Œä¸å†å±•å¼€ã€‚

{{< admonition type=quote title="ç”¨Jensenä¸ç­‰å¼æ¨å¯¼ varational lower bound">}}
$$
\begin{aligned}
L_\text{CE}
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int p_\theta(\mathbf{x}_{0:T}) d\mathbf{x}_{1:T} \Big) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} d\mathbf{x}_{1:T} \Big) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \mathbb{E}_{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} \Big) \\
&\leq - \mathbb{E}_{q(\mathbf{x}_{0:T})} \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} \\
&= \mathbb{E}_{q(\mathbf{x}_{0:T})}\Big[\log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})}{p_\theta(\mathbf{x}_{0:T})} \Big] = L_\text{VLB}
\end{aligned}
$$
{{< /admonition >}}

è¿™ä¸ªæ˜¯å¦å¤–ä¸€ä¸ªæ¨å¯¼VLBçš„æ–¹å¼ï¼Œç›´æ¥ç”¨äº† [Jensen ä¸ç­‰å¼](https://en.wikipedia.org/wiki/Jensen%27s_inequality)ï¼š

è®¾ \( \phi \) æ˜¯ä¸€ä¸ª[concave function](https://en.wikipedia.org/wiki/Concave_function)ï¼Œ\( X \) æ˜¯ä¸€ä¸ªå¯ç§¯çš„éšæœºå˜é‡ï¼Œåˆ™æœ‰

\[
\phi\left( \mathbb{E}[X] \right) \geq \mathbb{E}\left[ \phi(X) \right]
\]

å¯¹åº”äºddpmï¼ŒRHSå³ä¸ºvariational lower bound / ELBOï¼š

$$
\log p_\theta(x) 
\geq \mathbb{E}_{q(z \mid x)} \left[ \log \frac{p_\theta(x, z)}{q(z \mid x)} \right]
$$

å…¶ä¸­ï¼Œ$log()$ is a concave functionã€‚

æ¨å¯¼ä¹Ÿéå¸¸ç›´è§‚ï¼Œä¸è¿‡è®©æˆ‘ä»¬å¯¹éƒ¨åˆ†ç¬¦å·è¿›è¡Œè§£é‡Šã€‚ <!-- TODO -->

{{< admonition type="quote" title="å±•å¼€" >}}
$$
\begin{aligned}
L_\text{VLB} 
&= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
&= \mathbb{E}_q \Big[ \log\frac{\prod_{t=1}^T q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{ p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t) } \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \Big( \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\cdot \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)} \Big) + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big]\\
&= \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \Big] \\
&= \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]
\end{aligned}
$$
{{< /admonition>}}

![alt text](/posts/image.png)

ä¸Šé¢è¿™ä¸ªå›¾æ¥è‡ªDDPMè®ºæ–‡ï¼Œå¯ä»¥æŠŠç›¸å…³å†…å®¹åŠ è¿›å»

è¿™ä¸ªæ˜¯ç›®æ ‡å‡½æ•°ï¼Œè€Œä¸æ˜¯loss

è¿™ç»„æ¨å¯¼æ˜¯å¯¹æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰ä¸­çš„å˜åˆ†ä¸‹ç•Œï¼ˆVariational Lower Bound, VLBï¼‰æˆ–è¯æ®ä¸‹ç•Œï¼ˆEvidence Lower Bound, ELBOï¼‰è¿›è¡Œé€æ­¥å±•å¼€ä¸é‡æ„çš„è¿‡ç¨‹ã€‚å®ƒçš„ç›®çš„ï¼Œæ˜¯å°†è®­ç»ƒç›®æ ‡ä»ä¸€ä¸ªéš¾ä»¥ç›´æ¥ä¼˜åŒ–çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼Œè½¬åŒ–ä¸ºä¸€ç»„å¯è®¡ç®—çš„ KL æ•£åº¦é¡¹ä¸é‡æ„é¡¹ï¼Œä»è€ŒæŒ‡å¯¼ç¥ç»ç½‘ç»œå­¦ä¹ å¦‚ä½•ä»å™ªå£°ä¸­æ¢å¤åŸå§‹æ•°æ®ã€‚

---

ä¸ºä»€ä¹ˆè¦æ¨å¯¼è¿™ä¸ªå…¬å¼ï¼Ÿ

æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ \( \log p_\theta(\mathbf{x}_0) \)ï¼Œä½†ç”±äºè¿™ä¸ªç›®æ ‡æ¶‰åŠå¯¹é«˜ç»´éšå˜é‡çš„ç§¯åˆ†ï¼Œæ— æ³•ç›´æ¥è®¡ç®—ã€‚å› æ­¤æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªè¿‘ä¼¼åˆ†å¸ƒ \( q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) \)ï¼Œå¹¶é€šè¿‡ Jensen ä¸ç­‰å¼æ„é€ ä¸€ä¸ªä¸‹ç•Œï¼š

\[
\log p_\theta(\mathbf{x}_0) \geq \mathbb{E}_{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \left[ \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \right] = -L_\text{VLB}
\]

è¿™ä¸ªä¸‹ç•Œå°±æ˜¯æˆ‘ä»¬è®­ç»ƒæ—¶è¦æœ€å°åŒ–çš„æŸå¤±å‡½æ•°ã€‚

---

ç”¨äº†å“ªäº›æŠ€å·§ï¼Ÿ

1. **é©¬å°”å¯å¤«é“¾å±•å¼€**
åˆ©ç”¨æ­£å‘è¿‡ç¨‹çš„é©¬å°”å¯å¤«æ€§è´¨ï¼š
\[
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \vert \mathbf{x}_{t-1})
\]
ä»¥åŠé€†å‘è¿‡ç¨‹çš„å»ºæ¨¡ï¼š
\[
p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)
\]

1. **KL æ•£åº¦é‡æ„**
å°†å¯¹æ•°æ¯”å€¼è½¬åŒ–ä¸º KL æ•£åº¦å½¢å¼ï¼š
\[
D_\text{KL}(q \parallel p) = \mathbb{E}_q \left[ \log \frac{q}{p} \right]
\]
ä»è€Œå°†æŸå¤±å‡½æ•°æ‹†è§£ä¸ºä¸‰éƒ¨åˆ†ï¼š
- \( L_T \): ç»ˆç‚¹åˆ†å¸ƒåŒ¹é…ï¼ˆé«˜æ–¯å…ˆéªŒï¼‰
- \( L_{t-1} \): æ¯ä¸€æ­¥çš„å»å™ªåŒ¹é…
- \( L_0 \): æœ€ç»ˆé‡æ„é¡¹

1. **åéªŒé‡æ„æŠ€å·§**
åˆ©ç”¨ï¼š
\[
\frac{q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)}{q(\mathbf{x}_t \vert \mathbf{x}_0)} = q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)
\]
è¿™ä¸ªæŠ€å·§æ˜¯å…³é”®ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†ä¸å¯ç›´æ¥é‡‡æ ·çš„åéªŒ \( q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \) æ˜¾å¼è¡¨è¾¾ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œä»è€Œè®¡ç®— KL æ•£åº¦ã€‚

---

ğŸ” æ¨å¯¼çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ

è¿™å¥—æ¨å¯¼çš„æ„ä¹‰åœ¨äºï¼š

- **ç†è®ºæ¸…æ™°åŒ–**ï¼šå°†è®­ç»ƒç›®æ ‡ä»æŠ½è±¡çš„ä¼¼ç„¶æœ€å¤§åŒ–ï¼Œè½¬åŒ–ä¸ºå…·ä½“çš„ KL æ•£åº¦é¡¹ä¸é‡æ„é¡¹ã€‚
- **å¯è®¡ç®—æ€§**ï¼šæ¯ä¸€é¡¹éƒ½å¯ä»¥é€šè¿‡ Monte Carlo é‡‡æ ·ä¼°è®¡ï¼Œé€‚åˆæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚
- **æ¨¡å‹è®¾è®¡æŒ‡å¯¼**ï¼šæ˜ç¡®äº†ç¥ç»ç½‘ç»œè¦å­¦ä¹ çš„æ˜¯ä» \( \mathbf{x}_t \) é¢„æµ‹ \( \mathbf{x}_0 \) æˆ–å™ªå£° \( \boldsymbol{\varepsilon}_t \)ï¼Œä»è€Œæ„å»º \( p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) \)ã€‚

ä½ è¯´å¾—éå¸¸å¯¹ï¼ŒKL æ•£åº¦çš„å®šä¹‰ç¡®å®æ˜¯ï¼š

\[
D_{\text{KL}}(q(x) \parallel p(x)) = \mathbb{E}_{q(x)} \left[ \log \frac{q(x)}{p(x)} \right]
\]

ä¹Ÿå°±æ˜¯è¯´ï¼Œ**log æ¯”å€¼å¤–é¢å¿…é¡»ä¹˜ä¸Šä¸€ä¸ªæœŸæœ›**ï¼Œè€Œä¸æ˜¯ç›´æ¥å†™æˆ log æ¯”å€¼æœ¬èº«ã€‚ä½ æŒ‡å‡ºçš„è¿™ä¸ªé—®é¢˜ï¼Œæ­£æ˜¯è¿™ç±»æ¨å¯¼ä¸­æœ€å®¹æ˜“æ··æ·†çš„åœ°æ–¹ä¹‹ä¸€ã€‚

---

âœ… é‚£ä¹ˆåŸæ¨å¯¼ä¸ºä»€ä¹ˆçœ‹èµ·æ¥â€œå°‘ä¹˜äº†ä¸€ä¸ªæœŸæœ›â€ï¼Ÿ

å…¶å®æ²¡æœ‰å°‘ã€‚æˆ‘ä»¬æ¥è¿˜åŸä¸€ä¸‹æœ€åä¸€è¡Œï¼š

\[
L_\text{VLB} = \mathbb{E}_q \left[ \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \right]
\]

è¿™å…¶å®æ˜¯å¯¹æ¯ä¸€é¡¹éƒ½åœ¨ **\( \mathbb{E}_{q(\mathbf{x}_{0:T})} \)** ä¸‹å–æœŸæœ›çš„å†™æ³•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼š

- ç¬¬ä¸€é¡¹æ˜¯ \( \mathbb{E}_{q(\mathbf{x}_T \vert \mathbf{x}_0)} \left[ \log \frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} \right] \)ï¼Œå³ KL æ•£åº¦é¡¹ \( D_{\text{KL}}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \)
- ç¬¬äºŒé¡¹æ˜¯å¯¹æ¯ä¸ª \( t \) çš„ KL æ•£åº¦é¡¹ \( D_{\text{KL}}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)) \)
- ç¬¬ä¸‰é¡¹æ˜¯é‡æ„é¡¹ \( -\mathbb{E}_{q(\mathbf{x}_1 \vert \mathbf{x}_0)} \left[ \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \right] \)

æ‰€ä»¥è™½ç„¶è¡¨é¢ä¸Šçœ‹æ˜¯ log æ¯”å€¼ï¼Œå®é™…ä¸Šæ¯ä¸€é¡¹éƒ½éšå«äº†åœ¨å¯¹åº”çš„ \( q \) åˆ†å¸ƒä¸‹çš„æœŸæœ›ã€‚

---

ğŸ§  ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™ï¼Ÿ

è¿™æ˜¯ä¸ºäº†ç®€æ´åœ°è¡¨è¾¾æ•´ä¸ªæŸå¤±å‡½æ•°çš„ç»“æ„ã€‚åœ¨å®é™…å®ç°ä¸­ï¼Œæ¯ä¸€é¡¹éƒ½ä¼šé€šè¿‡é‡‡æ · \( \mathbf{x}_t \sim q(\cdot \vert \mathbf{x}_0) \) æ¥ä¼°è®¡æœŸæœ›å€¼ã€‚æ¯”å¦‚ï¼š

- \( \mathbf{x}_T \sim q(\mathbf{x}_T \vert \mathbf{x}_0) \)ï¼Œç”¨äºä¼°è®¡ prior matching KL
- \( \mathbf{x}_{t-1} \sim q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \)ï¼Œç”¨äºä¼°è®¡ denoising KL
- \( \mathbf{x}_1 \sim q(\mathbf{x}_1 \vert \mathbf{x}_0) \)ï¼Œç”¨äºä¼°è®¡é‡æ„é¡¹

ğŸ” VLB çš„å…·ä½“æ‹†è§£ï¼ˆå‚è€ƒ Ho et al. 2020 å’Œ Nichol & Dhariwal 2021ï¼‰

VLB è¢«æ‹†æˆä¸‰éƒ¨åˆ†ï¼š

1. **Prior Matching Term**ï¼š
   \[
   D_{\text{KL}}(q(x_T | x_0) \| p(x_T))
   \]
   çº¦æŸæœ€ç»ˆåŠ å™ªç»“æœæ¥è¿‘æ ‡å‡†é«˜æ–¯ã€‚

2. **Denoising Matching Term**ï¼š
   \[
   \sum_{t=2}^{T} D_{\text{KL}}(q(x_{t-1} | x_t, x_0) \| p_\theta(x_{t-1} | x_t))
   \]
   çº¦æŸæ¯ä¸€æ­¥çš„é€†å‘å»å™ªåˆ†å¸ƒæ‹ŸåˆçœŸå®åˆ†å¸ƒã€‚

3. **Reconstruction Term**ï¼š
   \[
   -\mathbb{E}_{q(x_1 | x_0)}[\log p_\theta(x_0 | x_1)]
   \]
   çº¦æŸæœ€ç»ˆç”Ÿæˆç»“æœä¸åŸå§‹å›¾åƒæ¥è¿‘ã€‚


{{< admonition type="quote" >}}
$$
\begin{aligned}
L_\text{VLB} &= L_T + L_{T-1} + \dots + L_0 \\
\text{where } L_T &= D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \\
L_t &= D_\text{KL}(q(\mathbf{x}_t \vert \mathbf{x}_{t+1}, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_t \vert\mathbf{x}_{t+1})) \text{ for }1 \leq t \leq T-1 \\
L_0 &= - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)
\end{aligned}
$$
{{< /admonition >}}

LT å’Œ L0 ä¸ç”¨è®¡ç®—

ä½ é—®å¾—éå¸¸åˆ°ä½ï¼š**ä¸ºä»€ä¹ˆ \( L_T \) å’Œ \( L_0 \) åœ¨è®­ç»ƒä¸­å¯ä»¥ä¸ç”¨è®¡ç®—ï¼Ÿ**è¿™å…¶å®æ˜¯æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦ç®€åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬æ¥é€é¡¹åˆ†æï¼š

---

ğŸ§Š ä¸ºä»€ä¹ˆ \( L_T = D_{\text{KL}}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \) å¯ä»¥ä¸è®¡ç®—ï¼Ÿ

âœ… åŸå› ä¸€ï¼šå®ƒæ˜¯ä¸€ä¸ªå¸¸æ•°é¡¹
- \( q(\mathbf{x}_T \vert \mathbf{x}_0) \) æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼å’Œæ–¹å·®æ˜¯å›ºå®šçš„ï¼ˆç”±å™ªå£°è°ƒåº¦å†³å®šï¼‰ã€‚
- \( p_\theta(\mathbf{x}_T) \) æ˜¯æ ‡å‡†é«˜æ–¯ \( \mathcal{N}(0, I) \)ï¼Œä¹Ÿä¸ä¾èµ–æ¨¡å‹å‚æ•°ã€‚
- æ‰€ä»¥å®ƒä»¬ä¹‹é—´çš„ KL æ•£åº¦æ˜¯ä¸€ä¸ª **è§£æå¯è®¡ç®—çš„å¸¸æ•°**ï¼Œä¸å½±å“æ¢¯åº¦ä¼˜åŒ–ã€‚

> ğŸ“Œ ç»“è®ºï¼š**ä¸ä¾èµ–æ¨¡å‹å‚æ•° \( \theta \)**ï¼Œæ‰€ä»¥å¯ä»¥åœ¨è®­ç»ƒæ—¶å¿½ç•¥ã€‚

---

ğŸ¯ ä¸ºä»€ä¹ˆ \( L_0 = -\log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \) å¯ä»¥ä¸è®¡ç®—ï¼Ÿ

âœ… åŸå› ä¸€ï¼šå®ƒè¢«è¿‘ä¼¼ä¸º t=1 æ—¶çš„ denoising loss
- åŸå§‹çš„é‡æ„é¡¹ \( L_0 \) æ˜¯ä» \( \mathbf{x}_1 \) é¢„æµ‹ \( \mathbf{x}_0 \)ï¼Œä½†è¿™é¡¹åœ¨è®­ç»ƒä¸­ä¼šå¸¦æ¥è¾ƒé«˜çš„æ–¹å·®ã€‚
- æ‰€ä»¥å¾ˆå¤šå®ç°ï¼ˆå¦‚ DDPMï¼‰å°†å…¶è¿‘ä¼¼ä¸ºï¼š
  \[
  L_0 \approx D_{\text{KL}}(q(\mathbf{x}_0 \vert \mathbf{x}_1) \parallel p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1))
  \]
  æˆ–ç›´æ¥ç”¨ t=1 çš„ denoising KL æ¥ä»£æ›¿ã€‚

âœ… åŸå› äºŒï¼šå®ƒå¯ä»¥åˆå¹¶è¿›ç»Ÿä¸€çš„ denoising loss æ¡†æ¶
- åœ¨å®é™…è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä» \( t \sim \text{Uniform}(1, T) \) é‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œç„¶åä¼˜åŒ–ï¼š
  \[
  \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right\|^2 \right]
  \]
- è¿™ä¸ªç»Ÿä¸€çš„æŸå¤±å·²ç»éšå«äº†å¯¹æ‰€æœ‰æ—¶é—´æ­¥ï¼ˆåŒ…æ‹¬ t=1ï¼‰çš„ä¼˜åŒ–ã€‚

> ğŸ“Œ ç»“è®ºï¼š**å®ƒè¢«åŒ…å«åœ¨ç»Ÿä¸€çš„è®­ç»ƒæŸå¤±ä¸­ï¼Œä¸éœ€è¦å•ç‹¬è®¡ç®—ã€‚**

---

ğŸ§  æ€»ç»“è¡¨æ ¼

| é¡¹ç›® | æ˜¯å¦ä¾èµ–æ¨¡å‹å‚æ•° | æ˜¯å¦éœ€è¦è®­ç»ƒæ—¶è®¡ç®— | åŸå›  |
|------|------------------|---------------------|------|
| \( L_T \) | âŒ å¦ | âŒ ä¸éœ€è¦ | æ˜¯å¸¸æ•°é¡¹ |
| \( L_0 \) | âœ… æ˜¯ | âŒ ä¸éœ€è¦å•ç‹¬è®¡ç®— | è¢«åŒ…å«åœ¨ t=1 çš„ denoising loss ä¸­ |

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥å¸¦ä½ æ¨å¯¼ä¸€ä¸‹ \( L_T \) çš„å…·ä½“å€¼ï¼Œæˆ–è€…çœ‹çœ‹å¦‚ä½•ä» \( L_0 \) æ¨å‡ºå™ªå£°é¢„æµ‹æŸå¤±ã€‚è¦ä¸è¦ç»§ç»­æ·±å…¥ï¼Ÿ

### Parameterization of $L_t$ for Training Loss

{{< admonition type="quote" >}}
$$
\begin{aligned}
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) &= \color{cyan}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)} \\
\text{Thus }\mathbf{x}_{t-1} &= \mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
\end{aligned}
$$
{{< /admonition >}}

è¿™æ˜¯ä¸ºäº†é¢„æµ‹ \mu çš„ï¼Œå®šä¹‰ä¸‹ \mu è¿™å—çš„ç¥ç»ç½‘ç»œã€‚

{{< admonition type="quote" title="Title" open=true >}}
$$
\begin{aligned}
L_t 
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2 \| \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) \|^2_2} \| \color{blue}{\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0)} - \color{green}{\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)} \|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2  \|\boldsymbol{\Sigma}_\theta \|^2_2} \| \color{blue}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)} - \color{green}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t) \Big)} \|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \| \boldsymbol{\Sigma}_\theta \|^2_2} \|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \| \boldsymbol{\Sigma}_\theta \|^2_2} \|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)\|^2 \Big] 
\end{aligned}
$$
{{< /admonition >}}

è¿™ä¸ªå…¬å¼çš„ç¬¬ä¸€æ­¥ï¼Œéœ€è¦recallä¸‹KLæ•£åº¦ä¸­å¯¹ä¸¤ä¸ªåˆ†å¸ƒçš„è®¡ç®—ï¼Œç›¸å½“äºè½¬æ¢æˆäº†

ğŸ” KL æ•£åº¦å±•å¼€ï¼ˆä¸¤é«˜æ–¯åˆ†å¸ƒï¼‰

å‡è®¾ä¸¤è€…éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒï¼š

- çœŸå®åéªŒï¼š\( q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mu_q, \Sigma_q) \)
- æ¨¡å‹ä¼°è®¡ï¼š\( p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mu_\theta, \Sigma_q) \)

é‚£ä¹ˆ KL æ•£åº¦ä¸ºï¼š

\[
L_t = \frac{1}{2} \left[ \log \frac{|\Sigma_q|}{|\Sigma_q|} - d + \text{tr}(\Sigma_q^{-1} \Sigma_q) + (\mu_q - \mu_\theta)^T \Sigma_q^{-1} (\mu_q - \mu_\theta) \right]
= \frac{1}{2 \sigma_q^2} \|\mu_q - \mu_\theta\|^2
\]


å±•å¼€åå…¶å®åˆšå¥½è¿˜æ˜¯å…¸å‹çš„mse errorï¼š$\text{MSE}=?$

\epsilon_\theta ç¬¦åˆå‡å€¼ä¸º x_t æ–¹å·®ä¸ºtçš„gaossian åˆ†å¸ƒã€‚

æœ€ååˆè°ƒç”¨äº†å°é—­å½¢å¼çš„forward diffusionï¼Œå®Œæ•´çš„å±•ç¤ºäº†å¦‚ä½•ä»åŸå§‹æ•°æ®äº§ç”Ÿå‡ºä¸€ä¸ªlossã€‚

{{% admonition type="quote" title="Title" open=true %}}
$$
\begin{aligned}
L_t^\text{simple}
&= \mathbb{E}_{t \sim [1, T], \mathbf{x}_0, \boldsymbol{\epsilon}_t} \Big[\|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \Big] \\
&= \mathbb{E}_{t \sim [1, T], \mathbf{x}_0, \boldsymbol{\epsilon}_t} \Big[\|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)\|^2 \Big]
\end{aligned}
$$
{{% /admonition %}}

æ³¨æ„ä¸‹æ ‡å¤„ï¼Œè¿™é‡Œæ˜¯å–ä¸‰ä¸ªå˜é‡çš„è”åˆåˆ†å¸ƒã€‚

è¿™é‡Œå°±éå¸¸æ¥è¿‘çœŸå®è®­ç»ƒæ¨¡å‹æ—¶å€™çš„æ ·å­äº†ã€‚

{{% admonition type="quote" title="Title" open=true %}}
![DDPM Algorithm](/images/DDPM_Algo.png)
{{% /admonition %}}

å·¦è¾¹æ˜¯è®­ç»ƒï¼Œå³è¾¹æ˜¯æ¨ç†æ—¶çš„ã€‚

{{% admonition type="quote" title="Connection with noise-conditioned score networks (NCSN)" open=true %}}

[Song & Ermon (2019)](https://arxiv.org/abs/1907.05600) proposed a score-based generative modeling method where samples are produced via [Langevin dynamics](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#connection-with-stochastic-gradient-langevin-dynamics) using gradients of the data distribution estimated with score matching.

...

$$
\mathbf{s}_\theta(\mathbf{x}_t, t) 
\approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t)
= \mathbb{E}_{q(\mathbf{x}_0)} [\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \vert \mathbf{x}_0)]
= \mathbb{E}_{q(\mathbf{x}_0)} \Big[ - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} \Big]
= - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}}
$$
{{% /admonition %}}

ç›®çš„ä¸Šæ¥çœ‹ï¼Œå°±æ˜¯ç”¨æ±‚æ¢¯åº¦çš„æ–¹å¼ï¼Œæ¥å»ºæ¨¡ $\mu_\theta$

ä½ é—®çš„è¿™æ®µå…³äº Noise-Conditioned Score Networks (NCSN) çš„å†…å®¹ç¡®å®æŒºå¯†çš„ï¼Œå’±ä»¬æ¥ä¸€æ­¥æ­¥æ‹†è§£ä¸€ä¸‹å®ƒçš„æ ¸å¿ƒé€»è¾‘å’Œæ•°å­¦å«ä¹‰ï¼š

---

ğŸ§  èƒŒæ™¯ï¼šScore-Based Generative Modeling æ˜¯ä»€ä¹ˆï¼Ÿ

Score-based æ¨¡å‹çš„ç›®æ ‡æ˜¯ä¼°è®¡æ•°æ®åˆ†å¸ƒçš„æ¢¯åº¦ï¼ˆscoreï¼‰ï¼Œå³ï¼š
\[
\nabla_x \log p(x)
\]
è¿™ä¸ªæ¢¯åº¦å¯ä»¥ç”¨æ¥é€šè¿‡ Langevin dynamics é‡‡æ ·å‡ºæ–°çš„æ•°æ®ç‚¹ï¼š
\[
x_{t+1} = x_t + \frac{\delta}{2} \nabla_x \log p(x_t) + \sqrt{\delta} \cdot \epsilon_t
\]
å…¶ä¸­ \(\epsilon_t \sim \mathcal{N}(0, I)\)ï¼Œè¿™æ˜¯ä¸€ç§â€œå¸¦å™ªæ¢¯åº¦ä¸Šå‡â€çš„æ–¹å¼ã€‚

---

ğŸ§Š é—®é¢˜ï¼šæ•°æ®é›†ä¸­åœ¨ä½ç»´æµå½¢ä¸Šæ€ä¹ˆåŠï¼Ÿ

æ ¹æ® manifold hypothesisï¼ŒçœŸå®æ•°æ® \(x\) è™½ç„¶åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œä½†å…¶å®é›†ä¸­åœ¨ä¸€ä¸ªä½ç»´å­ç©ºé—´ä¸Šã€‚è¿™å¯¼è‡´ï¼š
- åœ¨æ•°æ®å¯†åº¦ä½çš„åŒºåŸŸï¼Œscore ä¼°è®¡ä¸å‡†ã€‚
- Langevin dynamics å¯èƒ½ä¼šâ€œèµ°åâ€ï¼Œå› ä¸ºæ¢¯åº¦ä¼°è®¡ä¸å¯é ã€‚

---

ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šåŠ å…¥å™ªå£° + å¤šå°ºåº¦è®­ç»ƒ

Song & Ermon æå‡ºï¼š
1. **åŠ å…¥ä¸åŒå¼ºåº¦çš„é«˜æ–¯å™ªå£°**ï¼šè®©æ•°æ®åˆ†å¸ƒå˜å¾—æ›´â€œæ»¡â€ï¼Œè¦†ç›–æ•´ä¸ªç©ºé—´ã€‚
2. **è®­ç»ƒä¸€ä¸ª Noise-Conditioned Score Network**ï¼šè®°ä½œ \(s_\theta(x, \sigma)\)ï¼Œå®ƒèƒ½ä¼°è®¡ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„ scoreï¼š
   \[
   s_\theta(x, \sigma) \approx \nabla_x \log p_\sigma(x)
   \]
   å…¶ä¸­ \(p_\sigma(x)\) æ˜¯åŠ å…¥å™ªå£°åçš„æ•°æ®åˆ†å¸ƒã€‚

---

ğŸ” ä¸æ‰©æ•£æ¨¡å‹çš„è”ç³»ï¼šForward Process ç±»ä¼¼åŠ å™ªè¿‡ç¨‹

è¿™ä¸ªâ€œé€æ­¥åŠ å™ªâ€çš„è¿‡ç¨‹å’Œæ‰©æ•£æ¨¡å‹ä¸­çš„ forward diffusion æ˜¯ä¸€æ ·çš„ï¼š
\[
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
\]
æœ€ç»ˆ \(x_T\) ä¼šå˜æˆä¸€ä¸ªçº¯é«˜æ–¯å™ªå£°ã€‚

---

ğŸ“ Score ä¸é«˜æ–¯åˆ†å¸ƒçš„æ¢¯åº¦å…³ç³»

å¯¹äºé«˜æ–¯åˆ†å¸ƒï¼š
\[
p(x) = \mathcal{N}(x; \mu, \sigma^2 I)
\]
å…¶ score æ˜¯ï¼š
\[
\nabla_x \log p(x) = -\frac{1}{\sigma^2}(x - \mu)
\]
æ‰€ä»¥åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œscore network å…¶å®æ˜¯åœ¨ä¼°è®¡ï¼š
\[
\nabla_x \log p_\sigma(x) = -\frac{x - \mu}{\sigma^2}
\]
è€Œè¿™ä¸ª \(\mu\) å¯ä»¥æ˜¯åŸå§‹æ•°æ® \(x_0\)ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¥ç»ç½‘ç»œé¢„æµ‹çš„å€¼ã€‚

---

ğŸ§ª æ€»ç»“ä¸€ä¸‹è¿™æ®µè¯çš„æ„æ€ï¼š

- åŸå§‹ score-based æ¨¡å‹ç”¨ Langevin dynamics æ¥é‡‡æ ·ï¼Œä½†åœ¨é«˜ç»´ç©ºé—´ä¸­ä¸ç¨³å®šã€‚
- åŠ å…¥å™ªå£°åè®­ç»ƒ score network æ›´ç¨³å®šã€‚
- å¤šå°ºåº¦å™ªå£°è®­ç»ƒï¼ˆnoise-conditionedï¼‰è®©æ¨¡å‹èƒ½é€‚åº”ä¸åŒçš„å™ªå£°æ°´å¹³ã€‚
- è¿™ä¸ªè¿‡ç¨‹å’Œæ‰©æ•£æ¨¡å‹çš„ forward diffusion æ˜¯ä¸€è‡´çš„ã€‚
- æœ€ç»ˆï¼Œscore network ä¼°è®¡çš„æ˜¯åŠ å…¥å™ªå£°åçš„æ•°æ®åˆ†å¸ƒçš„æ¢¯åº¦ã€‚

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥æŠŠè¿™éƒ¨åˆ†ç”¨å›¾ç¤ºæˆ–å…¬å¼æ¨å¯¼å†å±•å¼€ä¸€å±‚ï¼Œæˆ–è€…å¯¹æ¯”ä¸€ä¸‹ DDPM å’Œ NCSN çš„å¼‚åŒã€‚ä½ æ›´æƒ³æ·±å…¥å“ªä¸€å—ï¼Ÿ

ä½ çš„é—®é¢˜éå¸¸å¥½ï¼Œæˆ‘ä»¬æ¥é€ä¸ªè§£é‡Šè¿™äº›æ¦‚å¿µå’Œå…¬å¼ï¼Œå¸®åŠ©ä½ å»ºç«‹æ¸…æ™°çš„ç›´è§‰ï¼š

---

ğŸ§­ ä»€ä¹ˆæ˜¯ \(\mathbb{R}^D\) ç©ºé—´ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ª **Dç»´æ¬§å‡ é‡Œå¾—ç©ºé—´**ï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰é•¿åº¦ä¸º \(D\) çš„å®æ•°å‘é‡ç»„æˆçš„ç©ºé—´ã€‚æ¯”å¦‚ï¼š
- \(\mathbb{R}^2\)ï¼šäºŒç»´å¹³é¢
- \(\mathbb{R}^3\)ï¼šä¸‰ç»´ç©ºé—´
- \(\mathbb{R}^{512}\)ï¼šæ¯”å¦‚å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºç©ºé—´

åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰è¢«è¡¨ç¤ºä¸ºé«˜ç»´å‘é‡ï¼Œé€šå¸¸å°±åœ¨ \(\mathbb{R}^D\) ä¸­ã€‚

---

ğŸ§® ä¸ºä»€ä¹ˆè¦è®¡ç®—æ¢¯åº¦ \(\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t)\)ï¼Ÿ

è¿™æ˜¯æ‰€è°“çš„ **score function**ï¼Œè¡¨ç¤ºåœ¨æŸä¸ªç‚¹ \(\mathbf{x}_t\) ä¸Šï¼Œæ•°æ®åˆ†å¸ƒçš„å¯¹æ•°å¯†åº¦çš„æ¢¯åº¦ã€‚å®ƒçš„ä½œç”¨æ˜¯ï¼š
- æŒ‡å‡ºâ€œæ•°æ®åˆ†å¸ƒä¸Šå‡æœ€å¿«çš„æ–¹å‘â€
- å¯ä»¥ç”¨æ¥è¿›è¡Œ **Langevin dynamics** é‡‡æ ·
- åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ƒå¸®åŠ©æˆ‘ä»¬ä»å™ªå£°ä¸­â€œèµ°å›â€çœŸå®æ•°æ®åˆ†å¸ƒ

---

ğŸ” è¦ä¸è¦ç®— \(\mathbf{x}_{t-1}\)ï¼Ÿ

æ˜¯çš„ï¼Œæ‰©æ•£æ¨¡å‹çš„ç›®æ ‡å°±æ˜¯ä»ä¸€ä¸ªé«˜æ–¯å™ªå£° \(\mathbf{x}_T\) å¼€å§‹ï¼Œé€æ­¥å»å™ªå¾—åˆ° \(\mathbf{x}_0\)ã€‚æ¯ä¸€æ­¥éƒ½è¦ä¼°è®¡ï¼š
\[
p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t)
\]
è¿™ä¸ªåˆ†å¸ƒé€šå¸¸å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼ç”±ç¥ç»ç½‘ç»œé¢„æµ‹ã€‚

---

ğŸ“¦ ä»€ä¹ˆæ˜¯ \(q(\tilde{\mathbf{x}} \vert \mathbf{x})\)ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ª **åŠ å™ªè¿‡ç¨‹çš„æ¡ä»¶åˆ†å¸ƒ**ï¼Œè¡¨ç¤ºåœ¨åŸå§‹æ•°æ® \(\mathbf{x}\) ä¸ŠåŠ å™ªåå¾—åˆ° \(\tilde{\mathbf{x}}\) çš„æ¦‚ç‡ã€‚æ¯”å¦‚ï¼š
\[
q(\tilde{\mathbf{x}} \vert \mathbf{x}) = \mathcal{N}(\tilde{\mathbf{x}}; \mathbf{x}, \sigma^2 I)
\]
åœ¨ NCSN ä¸­ï¼Œè¿™ä¸ªåˆ†å¸ƒç”¨äºè®­ç»ƒ score network æ¥ä¼°è®¡åŠ å™ªæ•°æ®çš„ scoreã€‚

---

ğŸ“ è¿™ä¸ªå…¬å¼æ˜¯ score function çš„å®šä¹‰å—ï¼Ÿ

ä½ å†™çš„è¿™ç»„å…¬å¼ï¼š

\[
\mathbf{s}_\theta(\mathbf{x}_t, t) 
\approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t)
= \mathbb{E}_{q(\mathbf{x}_0)} [\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \vert \mathbf{x}_0)]
= \mathbb{E}_{q(\mathbf{x}_0)} \Big[ - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} \Big]
= - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}}
\]

æ˜¯ **æ‰©æ•£æ¨¡å‹ä¸­ score function çš„è¿‘ä¼¼è¡¨è¾¾å¼**ï¼Œå…¶ä¸­ï¼š
- \(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\) æ˜¯ç¥ç»ç½‘ç»œé¢„æµ‹çš„å™ªå£°
- \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\)ï¼Œæ˜¯å‰å‘è¿‡ç¨‹çš„ç´¯è®¡è¡°å‡å› å­
- æœ€åä¸€è¡Œæ˜¯å› ä¸ºæˆ‘ä»¬ç”¨ \(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\) æ¥ç”Ÿæˆ \(\mathbf{x}_t\)ï¼Œæ‰€ä»¥å¯ä»¥åæ¨ score

---

ğŸ§ª é‚£è®­ç»ƒæ—¶æ€ä¹ˆè®­ç»ƒï¼ŸçœŸå®çš„ score æœ‰å—ï¼Ÿ

å…³é”®ç‚¹æ˜¯ï¼š**çœŸå®çš„ score æ²¡æœ‰æ˜¾å¼è¡¨è¾¾å¼**ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡æ„é€ æŸå¤±å‡½æ•°æ¥é—´æ¥è®­ç»ƒ score networkã€‚

åœ¨ DDPM æˆ– NCSN ä¸­ï¼Œè®­ç»ƒç›®æ ‡æ˜¯ï¼š
\[
\mathcal{L}_{\text{simple}} = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}, t} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right\|^2 \right]
\]
ä¹Ÿå°±æ˜¯è¯´ï¼š
- æˆ‘ä»¬çŸ¥é“ \(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\)
- æ‰€ä»¥æˆ‘ä»¬çŸ¥é“çœŸå®çš„ \(\boldsymbol{\epsilon}\)
- è®­ç»ƒç›®æ ‡å°±æ˜¯è®©ç½‘ç»œé¢„æµ‹çš„ \(\boldsymbol{\epsilon}_\theta\) å°½é‡æ¥è¿‘çœŸå®çš„ \(\boldsymbol{\epsilon}\)

è¿™å°±é—´æ¥åœ°è®­ç»ƒäº† score functionï¼Œå› ä¸ºï¼š
\[
\mathbf{s}_\theta(\mathbf{x}_t, t) \approx - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}}
\]

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹ç”»æˆå›¾ï¼Œæˆ–è€…ç”¨ä»£ç å½¢å¼å±•ç¤ºè®­ç»ƒè¿‡ç¨‹ã€‚ä½ æ›´æƒ³çœ‹å“ªç§å½¢å¼ï¼Ÿ

### Parameterization of $\beta_t$

{{% admonition type="quote" title="Title" open=true %}}
Diffusion models in their experiments showed high-quality samples but still could not achieve competitive **model log-likelihood** as other generative models.

[Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) proposed several improvement techniques to help diffusion models to obtain lower **NLL**.
{{% /admonition %}}

è¿™é‡Œæåˆ°çš„â€œmodel log-likelihoodâ€ï¼ˆæ­£æ–¹å‘ç›®æ ‡ï¼‰ï¼Œâ€œNLLâ€ï¼ˆè´Ÿæ–¹å‘ç›®æ ‡ï¼‰å®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬æƒ³è¦ä¼˜åŒ–çš„ç›®æ ‡ã€‚

å…¶è®¡ç®—å°±æ˜¯é åˆšåˆšæˆ‘ä»¬è§åˆ°çš„çš„L_VLBã€‚ç„¶åè®¡ç®—å’Œè®­ç»ƒçš„æ—¶å€™çš„é‡‡æ ·ä¸€æ ·ï¼ŒåŒæ ·éœ€è¦å¤šæ¬¡è¿›è¡Œé‡‡æ ·ï¼Œæ¥ä¼°è®¡NLLï¼Œå¤§è‡´å¦‚ä¸‹ï¼š

å½“ç„¶ï¼Œæˆ‘ä»¬æ¥é€é¡¹è®²æ¸…æ¥šæ‰©æ•£æ¨¡å‹ä¸­ä¸‰é¡¹ VLB çš„ Monte Carlo ä¼°è®¡æ–¹å¼ï¼š

1ï¸âƒ£ Prior Matching Termï¼šKL(q(x_T | xâ‚€) || p(x_T))

è¿™æ˜¯å¯¹æœ€ç»ˆåŠ å™ªç»“æœæ˜¯å¦æ¥è¿‘æ ‡å‡†é«˜æ–¯çš„çº¦æŸã€‚

- **è®¡ç®—æ–¹å¼**ï¼š  
 ç”±äº \( q(x_T | x_0) \sim \mathcal{N}(\sqrt{\bar{\alpha}_T} x_0, (1 - \bar{\alpha}_T) I) \)ï¼Œè€Œ \( p(x_T) \sim \mathcal{N}(0, I) \)ï¼Œä¸¤ä¸ªéƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒï¼ŒKL æ•£åº¦å¯ä»¥è§£æè®¡ç®—ï¼š

  \[
  D_{\text{KL}} = \frac{1}{2} \left[ \text{tr}(\Sigma_p^{-1} \Sigma_q) + (\mu_p - \mu_q)^T \Sigma_p^{-1} (\mu_p - \mu_q) - d + \log \frac{|\Sigma_p|}{|\Sigma_q|} \right]
  \]

  å®é™…ä¸­ç›´æ¥ä»£å…¥å‡å€¼å’Œæ–¹å·®å³å¯ï¼Œä¸éœ€è¦é‡‡æ ·ã€‚

2ï¸âƒ£ Denoising Matching Termï¼šKL(q(x_{t-1} | x_t, xâ‚€) || p_Î¸(x_{t-1} | x_t))

è¿™æ˜¯æœ€æ ¸å¿ƒçš„ä¸€é¡¹ï¼Œçº¦æŸæ¨¡å‹é¢„æµ‹çš„å»å™ªåˆ†å¸ƒæ˜¯å¦æ¥è¿‘çœŸå®åˆ†å¸ƒã€‚

- **è®¡ç®—æ–¹å¼**ï¼š
  å¯¹æ¯ä¸ªæ—¶é—´æ­¥ \( t \)ï¼Œæˆ‘ä»¬é‡‡æ ·ï¼š

  \[
  x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
  \]

  ç„¶åæ¨¡å‹é¢„æµ‹ \( \hat{\epsilon}_\theta(x_t, t) \)ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¢å¤å‡ºæ¨¡å‹çš„å‡å€¼ï¼š

  \[
  \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \hat{\epsilon}_\theta \right)
  \]

  è€ŒçœŸå®å‡å€¼æ˜¯ï¼š

  \[
  \mu_q(x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon \right)
  \]

  ä¸¤è€…ä¹‹é—´çš„ KL æ•£åº¦ç®€åŒ–ä¸ºå‡å€¼å·®çš„å¹³æ–¹é™¤ä»¥æ–¹å·®ï¼š

  \[
  \mathcal{L}_t = \frac{1}{2 \sigma_t^2} \| \mu_q - \mu_\theta \|^2 \propto \| \epsilon - \hat{\epsilon}_\theta(x_t, t) \|^2
  \]

  æ‰€ä»¥è®­ç»ƒæ—¶åªéœ€é‡‡æ · \( x_t \)ï¼Œè®¡ç®—æ¨¡å‹é¢„æµ‹çš„å™ªå£°ä¸çœŸå®å™ªå£°ä¹‹é—´çš„ MSEã€‚

3ï¸âƒ£ Reconstruction Termï¼šâˆ’E_{q(xâ‚ | xâ‚€)} [log p_Î¸(xâ‚€ | xâ‚)]

è¿™æ˜¯å¯¹æœ€ç»ˆç”Ÿæˆç»“æœæ˜¯å¦æ¥è¿‘åŸå›¾çš„çº¦æŸã€‚

- **è®¡ç®—æ–¹å¼**ï¼š
  é‡‡æ · \( x_1 \sim q(x_1 | x_0) \)ï¼Œç„¶åæ¨¡å‹é¢„æµ‹ \( p_\theta(x_0 | x_1) \)ï¼Œé€šå¸¸å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒï¼š

  \[
  p_\theta(x_0 | x_1) = \mathcal{N}(x_0; \mu_\theta(x_1), \sigma^2 I)
  \]

  ç„¶åè®¡ç®— log-likelihoodï¼š

  \[
  \mathcal{L}_0 = -\log p_\theta(x_0 | x_1) \propto \| x_0 - \mu_\theta(x_1) \|^2
  \]

  å®è·µä¸­è¿™é¡¹å¯ä»¥è¿‘ä¼¼ä¸º t=1 æ—¶çš„ MSE æŸå¤±ã€‚

---

å¦‚æœä½ æƒ³æˆ‘å¸®ä½ å†™å‡º PyTorch ä»£ç æ¥ä¼°è®¡è¿™ä¸‰é¡¹ï¼Œæˆ–è€…æ¨å¯¼æŸä¸€é¡¹çš„ KL æ•£åº¦å…¬å¼ï¼Œæˆ‘å¯ä»¥ç»§ç»­å±•å¼€ã€‚ä½ å¯¹å“ªä¸€é¡¹æœ€æ„Ÿå…´è¶£ï¼Ÿ

{{% admonition type="quote" title="Comparison of linear and cosine-based scheduling of $\beta_t$ during training" open=true %}}
![](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/diffusion-beta.png)
{{% /admonition %}}

åœ¨ **linear variance schedule** ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰æ¯ä¸€æ­¥çš„å™ªå£°å¼ºåº¦ä¸ºï¼š

\[
\beta_t = \beta_{\text{min}} + \frac{t - 1}{T - 1} (\beta_{\text{max}} - \beta_{\text{min}})
\]

å…¶ä¸­ï¼š
- \( \beta_{\text{min}} \) å’Œ \( \beta_{\text{max}} \) æ˜¯é¢„è®¾çš„æœ€å°å’Œæœ€å¤§å™ªå£°å€¼ï¼ˆä¾‹å¦‚ 0.0001 å’Œ 0.02ï¼‰
- \( T \) æ˜¯æ€»çš„æ‰©æ•£æ­¥æ•°ï¼ˆä¾‹å¦‚ 1000ï¼‰

---

ğŸ” é—­å¼å½¢å¼çš„ \( \bar{\alpha}_t \)

æˆ‘ä»¬å®šä¹‰ï¼š

\[
\alpha_t = 1 - \beta_t
\quad \text{and} \quad
\bar{\alpha}_t = \prod_{s=1}^t \alpha_s
\]

ç”±äº \( \beta_t \) æ˜¯çº¿æ€§é€’å¢çš„ï¼Œ\( \alpha_t \) æ˜¯çº¿æ€§é€’å‡çš„ï¼Œå› æ­¤ \( \bar{\alpha}_t \) æ˜¯ä¸€è¿ä¸²ä¹˜ç§¯ï¼Œè™½ç„¶ä¸èƒ½ç®€åŒ–ä¸ºä¸€ä¸ªå®Œå…¨é—­å¼è¡¨è¾¾ï¼Œä½†å¯ä»¥å†™æˆï¼š

\[
\bar{\alpha}_t = \prod_{s=1}^t \left(1 - \beta_{\text{min}} - \frac{s - 1}{T - 1} (\beta_{\text{max}} - \beta_{\text{min}})\right)
\]

è¿™ä¸ªè¡¨è¾¾å¼æ˜¯ **æ˜¾å¼çš„ closed-form**ï¼Œä½†ä»ç„¶æ˜¯ä¸€ä¸ªä¹˜ç§¯å½¢å¼ã€‚åœ¨å®é™…å®ç°ä¸­ï¼Œé€šå¸¸ä¼šé¢„å…ˆè®¡ç®—æ‰€æœ‰ \( \bar{\alpha}_t \) å¹¶ç¼“å­˜ä¸‹æ¥ã€‚

---

ğŸ§  è¿›ä¸€æ­¥ç®€åŒ–ï¼ˆè¿‘ä¼¼ï¼‰

å¦‚æœä½ å¸Œæœ›å¾—åˆ°ä¸€ä¸ªè¿‘ä¼¼é—­å¼è¡¨è¾¾ï¼Œå¯ä»¥è€ƒè™‘å°†ä¹˜ç§¯è½¬æ¢ä¸ºæŒ‡æ•°å½¢å¼ï¼š

\[
\log \bar{\alpha}_t = \sum_{s=1}^t \log \alpha_s
\quad \Rightarrow \quad
\bar{\alpha}_t = \exp\left( \sum_{s=1}^t \log(1 - \beta_s) \right)
\]

è¿™åœ¨æ•°å€¼è®¡ç®—ä¸­æ›´ç¨³å®šï¼Œä¹Ÿæ›´å®¹æ˜“å¤„ç†ã€‚

---

å¦‚æœä½ æƒ³æˆ‘å¸®ä½ ç”¨ Python æˆ– PyTorch å†™å‡ºè¿™ä¸ª linear schedule çš„åˆå§‹åŒ–ä»£ç ï¼Œæˆ‘å¯ä»¥ç›´æ¥ç»™ä½ æ¨¡æ¿ã€‚æˆ–è€…ä½ æƒ³æ¯”è¾ƒå®ƒå’Œ cosine schedule çš„å›¾åƒï¼Œæˆ‘ä¹Ÿå¯ä»¥ç”»å‡ºæ¥ã€‚ä½ æƒ³ç»§ç»­å“ªä¸€æ–¹å‘ï¼Ÿ

### Parameterization of reverse process variance $\boldsymbol{\Sigma}_\theta$

{{% admonition type="quote" title="Title" open=true %}}
\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) = \exp(\mathbf{v} \log \beta_t + (1-\mathbf{v}) \log \tilde{\beta}_t)
{{% /admonition %}}

recall ä¹‹å‰è®¡ç®—simplification of L_VLBçš„æ—¶å€™ï¼ŒDDPMåŸè®ºæ–‡ [^ho_ddpm] æ˜¯æŠŠè¿™ä¸ªweight ç³»æ•°ä¸¢æ‰äº†ï¼Œè¿™é‡Œï¼ŒOpenAIçš„Nichol çš„è®ºæ–‡ [^nichol_improved_ddpm] å¯¹è¿™é‡Œå†æ¬¡æ”¹è¿›ï¼Œæ—¢ä¸å»æ‰è¿™ä¸ªï¼Œä»ç„¶å‚ä¸ä¼˜åŒ–ã€‚

{{% admonition type="quote" title="Title" open=true %}}
noisy gradients
{{% /admonition %}}

è¿™æ˜¯å‡ºè‡ªopenaiçš„è®ºæ–‡ An Empirical Model of Large-Batch Training[^mccandlish_grad_noise] æå‡ºçš„ä¸€ä¸ªæŒ‡æ ‡

Gradient Noise Scaleï¼ˆæ¢¯åº¦å™ªå£°å°ºåº¦ï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¡¡é‡ä¼˜åŒ–è¿‡ç¨‹ä¸­æ¢¯åº¦ç¨³å®šæ€§çš„ç»Ÿè®¡æŒ‡æ ‡ï¼Œå°¤å…¶åœ¨æ·±åº¦å­¦ä¹ ä¸­ç”¨äºä¼°è®¡**æœ€ä¼˜æ‰¹é‡å¤§å°ï¼ˆoptimal batch sizeï¼‰**ã€‚

---

ğŸŒªï¸ å®šä¹‰ä¸ç›´è§‰

åœ¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯ç”¨æ•´ä¸ªæ•°æ®é›†è®¡ç®—æ¢¯åº¦ï¼Œè€Œæ˜¯ç”¨ä¸€ä¸ªå°æ‰¹é‡ï¼ˆmini-batchï¼‰ã€‚è¿™ä¼šå¼•å…¥å™ªå£°ï¼Œå› ä¸ºä¸åŒæ‰¹æ¬¡çš„æ¢¯åº¦å¯èƒ½å·®å¼‚å¾ˆå¤§ã€‚

**Gradient Noise Scale**è¡¡é‡çš„å°±æ˜¯è¿™ç§æ¢¯åº¦çš„æ³¢åŠ¨æ€§ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> å¦‚æœæ¢¯åº¦åœ¨ä¸åŒæ‰¹æ¬¡ä¹‹é—´å˜åŒ–å¾ˆå¤§ï¼ˆå™ªå£°é«˜ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤§çš„æ‰¹é‡æ¥è·å¾—æ›´ç¨³å®šçš„æ›´æ–°ã€‚

---

ğŸ“ æ•°å­¦è¡¨è¾¾

åœ¨ç®€åŒ–å‡è®¾ä¸‹ï¼ˆå¦‚ Hessian æ˜¯å•ä½çŸ©é˜µçš„å€æ•°ï¼‰ï¼ŒGradient Noise Scale å¯ä»¥è¡¨ç¤ºä¸ºï¼š

\[
B_{\text{simple}} = \frac{\text{tr}(\Sigma)}{|G|^2}
\]

å…¶ä¸­ï¼š

- \(\text{tr}(\Sigma)\)ï¼šæ¢¯åº¦åæ–¹å·®çŸ©é˜µçš„è¿¹ï¼Œå³æ‰€æœ‰æ¢¯åº¦åˆ†é‡çš„æ–¹å·®ä¹‹å’Œã€‚
- \(|G|^2\)ï¼šæ¢¯åº¦çš„å¹³æ–¹èŒƒæ•°ï¼ˆglobal gradient normï¼‰ã€‚

è¿™ä¸ªæ¯”å€¼è¡¨ç¤ºï¼š**æ¢¯åº¦çš„å™ªå£°å¼ºåº¦ç›¸å¯¹äºå…¶å¹³å‡å¼ºåº¦çš„æ¯”ä¾‹**ã€‚

---

ğŸ§  å®é™…æ„ä¹‰

- å¦‚æœ \(B_{\text{simple}}\) å¾ˆå¤§ï¼Œè¯´æ˜æ¢¯åº¦å™ªå£°å¾ˆå¼ºï¼Œå»ºè®®ä½¿ç”¨æ›´å¤§çš„ batch sizeã€‚
- å¦‚æœå®ƒå¾ˆå°ï¼Œè¯´æ˜æ¢¯åº¦ç¨³å®šï¼Œå¯ä»¥ç”¨è¾ƒå°çš„ batch sizeï¼ŒåŠ å¿«è®­ç»ƒã€‚

---

ğŸ” åº”ç”¨åœºæ™¯

- è‡ªåŠ¨è°ƒæ•´ batch sizeï¼ˆå¦‚åœ¨ Torch-Foresight ä¸­ä½¿ç”¨ï¼‰
- åˆ†ææ•°æ®é›†å¤æ‚åº¦ï¼šé«˜å™ªå£°å¯èƒ½æ„å‘³ç€æ•°æ®åˆ†å¸ƒå¤æ‚æˆ–æ¨¡å‹ä¸ç¨³å®š
- ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ï¼šåœ¨èµ„æºå—é™æ—¶æ‰¾åˆ°æœ€åˆé€‚çš„ batch size

{{% admonition type="quote" title="Title" open=true %}}
time-averaging smoothed version of $L_\text{VLB}$ with importance sampling.
{{% /admonition %}}

æ ¹æ®Improved DDPM [^nichol_improved_ddpm]ï¼Œè¿™é‡Œçš„å…¬å¼æ˜¯ï¼š


![alt text](/posts/image-1.png)

æ ¸å¿ƒåŠ¨æœºå°±æ˜¯ä¸åŒçš„tå¯¹åº”çš„Lè´¡çŒ®åº¦ä¸åŒï¼Œæƒ³è¦æ¶ˆè§£æ‰magnitudeçš„å·®å¼‚ã€‚

![alt text](/posts/image-2.png)

## Conditioned Generation

{{% admonition type="quote" title="Title" open=true %}}
While training generative models on images with conditioning information such as ImageNet dataset, it is common to generate samples conditioned on class labels or a piece of descriptive text.
{{% /admonition %}}

å…¶å®å°±æ˜¯ä»Šå¤©æˆ‘ä»¬å¸¸è¯´çš„ï¼Œæ–‡ç”Ÿå›¾ä»»åŠ¡ï¼Œä¹‹å‰çš„å«æ³•å¾ˆæœ‰å­¦æœ¯å‘³å„¿ã€‚

### Classifier Guided Diffusion

## Appendix

è¿™é‡Œæ±‡æ€»äº†è¦æƒ³æ›´å®Œæ•´äº†è§£æ•´ä¸ªdiffusion modelsçš„å†…å®¹éœ€è¦çš„å°çš„åŸºç¡€çŸ¥è¯†ç‚¹ã€‚

### Notations

- $\beta_t$ æ˜¯Noise variance schedule parameterï¼Œä»–å¯¹åº”ä¸€ä¸ªvariance scheduleï¼Œ$\{\beta_t \in (0, 1)\}_{t=1}^T$ï¼Œå’Œå­¦ä¹ ç‡è°ƒåº¦æ˜¯ç±»ä¼¼çš„.

### é‡è¦çš„diffusionç›¸å…³çš„è®ºæ–‡

[1] Jascha Sohl-Dickstein et al. â€œDeep Unsupervised Learning using Nonequilibrium Thermodynamics.â€ ICML 2015.

[2] Max Welling & Yee Whye Teh. â€œBayesian learning via stochastic gradient langevin dynamics.â€ ICML 2011.

[3] Yang Song & Stefano Ermon. â€œGenerative modeling by estimating gradients of the data distribution.â€ NeurIPS 2019.

[4] Yang Song & Stefano Ermon. â€œImproved techniques for training score-based generative models.â€ NeuriPS 2020.

[5] Jonathan Ho et al. â€œDenoising diffusion probabilistic models.â€ arxiv Preprint arxiv:2006.11239 (2020). [code]

[6] Jiaming Song et al. â€œDenoising diffusion implicit models.â€ arxiv Preprint arxiv:2010.02502 (2020). [code]

[7] Alex Nichol & Prafulla Dhariwal. â€œImproved denoising diffusion probabilistic modelsâ€ arxiv Preprint arxiv:2102.09672 (2021). [code]

[8] Prafula Dhariwal & Alex Nichol. â€œDiffusion Models Beat GANs on Image Synthesis.â€ arxiv Preprint arxiv:2105.05233 (2021). [code]

[9] Jonathan Ho & Tim Salimans. â€œClassifier-Free Diffusion Guidance.â€ NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications.

[10] Yang Song, et al. â€œScore-Based Generative Modeling through Stochastic Differential Equations.â€ ICLR 2021.

[11] Alex Nichol, Prafulla Dhariwal & Aditya Ramesh, et al. â€œGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.â€ ICML 2022.

[12] Jonathan Ho, et al. â€œCascaded diffusion models for high fidelity image generation.â€ J. Mach. Learn. Res. 23 (2022): 47-1.

[13] Aditya Ramesh et al. â€œHierarchical Text-Conditional Image Generation with CLIP Latents.â€ arxiv Preprint arxiv:2204.06125 (2022).

[14] Chitwan Saharia & William Chan, et al. â€œPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding.â€ arxiv Preprint arxiv:2205.11487 (2022).

[15] Rombach & Blattmann, et al. â€œHigh-Resolution Image Synthesis with Latent Diffusion Models.â€ CVPR 2022.code

[16] Song et al. â€œConsistency Modelsâ€ arxiv Preprint arxiv:2303.01469 (2023)

[17] Salimans & Ho. â€œProgressive Distillation for Fast Sampling of Diffusion Modelsâ€ ICLR 2022.

[18] Ronneberger, et al. â€œU-Net: Convolutional Networks for Biomedical Image Segmentationâ€ MICCAI 2015.

[19] Peebles & Xie. â€œScalable diffusion models with transformers.â€ ICCV 2023.

[20] Zhang et al. â€œAdding Conditional Control to Text-to-Image Diffusion Models.â€ arxiv Preprint arxiv:2302.05543 (2023).

### GAN, VAE, and Flow-based models æ˜¯ä»€ä¹ˆ

![Generative Models](/images/Generative_Models.png)

- GAN ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼šè®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆå›¾åƒï¼Œä¸€ä¸ªç”¨äºåˆ¤åˆ«å›¾åƒçš„çœŸä¼ª
- VAE å˜åˆ†è‡ªç¼–ç å™¨æ¨¡å‹ï¼šé€šè¿‡ç¼–ç å™¨å°†è¾“å…¥å›¾åƒå‹ç¼©ä¸ºæ½œåœ¨ç©ºé—´å˜é‡ï¼Œå†é€šè¿‡è§£ç å™¨é‡å»ºå›¾åƒ
- Flow matching modelsï¼šé€šè¿‡æµåŠ¨åŒ¹é…çš„æ–¹æ³•ç”Ÿæˆå›¾åƒï¼Œæ˜¯å‡½æ•°çº§åˆ«çš„è¿‡ç¨‹ç»„åˆèµ·æ¥ç”Ÿæˆå›¾åƒçš„ã€‚

### VAE ä¸ AE

VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰çš„å‰èº«æ˜¯AEï¼ˆè‡ªç¼–ç å™¨ï¼‰ï¼Œå°±æ˜¯ä¸€ä¸ªå…·æœ‰ç¼–ç å™¨å’Œè§£ç å™¨çš„ç¥ç»ç½‘ç»œï¼Œç›®çš„æ˜¯é€šè¿‡è®©è§£ç å™¨çš„å€¼å’ŒåŸå§‹å€¼å°½å¯èƒ½ç›¸ä¼¼ï¼Œè€Œå­¦ä¹ ä¸€ä¸ªå‹ç¼©äº†çš„æ½œåœ¨å˜é‡ï¼Œç”¨äºè¡¨ç¤ºå­¦ä¹ å’Œé™ç»´ã€‚

VAEå’ŒAEåœ¨ç»“æ„ä¸Šéå¸¸ç›¸ä¼¼ï¼Œä½†åœ¨ç†è®ºåŸºç¡€å’Œç›®æ ‡å‡½æ•°ä¸Šæœ‰æœ¬è´¨åŒºåˆ«ã€‚

| ç‰¹æ€§                 | è‡ªç¼–ç å™¨ï¼ˆAEï¼‰                  | å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰                                            |
| -------------------- | ------------------------------- | -------------------------------------------------------------- |
| ç¼–ç å™¨è¾“å‡º           | ä¸€ä¸ªç¡®å®šæ€§çš„å‘é‡ \( z = f(x) \) | ä¸€ä¸ªåˆ†å¸ƒ \( q(z \vert x) = \mathcal{N}(\mu(x), \sigma^2(x)) \) |
| è§£ç å™¨è¾“å…¥           | å›ºå®šå‘é‡ \( z \)                | ä»åˆ†å¸ƒä¸­é‡‡æ ·çš„ \( z \sim q(z \vert x) \)                       |
| è®­ç»ƒç›®æ ‡             | æœ€å°åŒ–é‡æ„è¯¯å·®ï¼ˆå¦‚ MSEï¼‰        | æœ€å¤§åŒ–å˜åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰                                         |
| æ˜¯å¦ä¸ºç”Ÿæˆæ¨¡å‹       | å¦                              | æ˜¯                                                             |
| æ˜¯å¦æœ‰æ¦‚ç‡å»ºæ¨¡       | å¦                              | æœ‰ï¼ˆå¯¹æ½œåœ¨å˜é‡å»ºæ¨¡ï¼‰                                           |
| æ˜¯å¦å¯é‡‡æ ·ç”Ÿæˆæ–°æ•°æ® | å¦                              | å¯ä»å…ˆéªŒ \( p(z) \) é‡‡æ ·ç”Ÿæˆæ•°æ®                               |
| æ˜¯å¦ä½¿ç”¨KLæ•£åº¦       | å¦                              | ç”¨äºæ­£åˆ™åŒ–æ½œåœ¨åˆ†å¸ƒ                                             |

### Diffusionä¸­ æ•°æ®æ ·æœ¬çš„è®°æ³•

$\mathbf{x}_0 \sim q(\mathbf{x})$ è¡¨ç¤ºä»çœŸå®æ•°æ®åˆ†å¸ƒ $q(\mathbf{x})$ ä¸­é‡‡æ ·å¾—åˆ°çš„æ ·æœ¬ $\mathbf{x}_0$ï¼Œå…¶ä¸­

- $\mathbf{x}_0$ï¼šè¡¨ç¤ºä¸€ä¸ªçœŸå®æ•°æ®æ ·æœ¬ï¼Œæ¯”å¦‚ä¸€å¼ å›¾åƒã€ä¸€æ®µè¯­éŸ³æˆ–ä¸€ä¸ªæ–‡æœ¬å‘é‡ã€‚æ˜¯ä¸€ä¸ªå‘é‡ï¼ˆä¾‹å¦‚å›¾åƒçš„åƒç´ å‘é‡ã€æ–‡æœ¬çš„åµŒå…¥å‘é‡ç­‰ï¼‰ï¼Œç»´åº¦å¯èƒ½æ˜¯å‡ ç™¾ç”šè‡³å‡ åƒ.
- $q(\mathbf{x})$ï¼šè¡¨ç¤ºçœŸå®æ•°æ®çš„åˆ†å¸ƒï¼Œä¹Ÿå«ç»éªŒåˆ†å¸ƒï¼Œæ¯”å¦‚è®­ç»ƒé›†ä¸­çš„å›¾åƒåˆ†å¸ƒã€‚

### Gaussian distribution

é«˜æ–¯åˆ†å¸ƒï¼ˆGaussian distributionï¼‰ä¹Ÿè¢«ç§°ä¸º**æ­£æ€åˆ†å¸ƒ**ï¼Œ$\mathcal{N}(\mu, \sigma)$ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDF, Probability Density Functionï¼‰ä¸ºï¼š

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \; \exp\!\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
$$

* $\mu$ï¼šå‡å€¼ï¼ˆmeanï¼‰ï¼Œå†³å®šåˆ†å¸ƒçš„ä¸­å¿ƒä½ç½®
* $\sigma$ï¼šæ ‡å‡†å·®ï¼ˆstandard deviationï¼‰ï¼Œå†³å®šåˆ†å¸ƒçš„å®½åº¦
* $\sigma^2$ï¼šæ–¹å·®ï¼ˆvarianceï¼‰
* $\exp(\cdot)$ï¼šè‡ªç„¶æŒ‡æ•°å‡½æ•° $e^x$

å…¶ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDF, Cumulative Distribution Functionï¼‰ä¸ºï¼š

$$
F(x) = P(X \le x) = \frac{1}{2} \left[ 1 + \operatorname{erf} \!\left( \frac{x - \mu}{\sigma\sqrt{2}} \right) \right]
$$

* $\operatorname{erf}(\cdot)$ï¼šè¯¯å·®å‡½æ•°ï¼ˆerror functionï¼‰ï¼Œæ˜¯æ— æ³•ç”¨åˆç­‰å‡½æ•°è¡¨ç¤ºçš„ç§¯åˆ†å‡½æ•°ï¼Œå®šä¹‰ä¸º

$$
\operatorname{erf}(z) = \frac{2}{\sqrt{\pi}} \int_{0}^{z} e^{-t^2} \, dt
$$

æ­¤å¤–ï¼Œisotropic Gaussian distributionæ˜¯æŒ‡å„æ–¹å‘éƒ½å‡åŒ€çš„é«˜æ–¯åˆ†å¸ƒï¼Œå³å‘é‡ä¸­çš„æ¯ä¸ªåˆ†é‡éƒ½ç¬¦åˆ $\mathcal{N}(0, \mathbf{I})$ã€‚

### æ‰©æ•£æ¨¡å‹ä¸­å¯¹åˆ†å¸ƒçš„è®°æ³•

æ‰©æ•£æ¨¡å‹ç›¸å…³è®ºæ–‡æ›´å€¾å‘äºè¿™ä¹ˆå†™æ¦‚ç‡ï¼Œæ ·æœ¬ï¼Œå’Œåˆ†å¸ƒé—´çš„å…³ç³»ï¼š

$$
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})
$$

å®ƒç­‰ä»·äºæ¦‚ç‡è®ºä¸­å¸¸è§çš„è¡¨è¾¾å½¢å¼ï¼š

$$
\begin{align}
\mathbf{x}_t &\sim \mathcal{N}(\sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I}) \\
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) &= f(\mathbf{x}_t) 
\end{align}
$$

å…¶ä¸­ï¼Œ$f(\cdot)$ æ˜¯ Gaussian Probability Density Function

### Closed-form expression

> In mathematics, an expression or formula (including equations and inequalities) is in closed form if it is formed with constants, variables, and a set of functions considered as basic and connected by arithmetic operations (+, âˆ’, Ã—, /, and integer powers) and function composition.
>
> â€” <cite>Wikipedia [^wiki_closed]</cite>

ç®€å•æ¥è¯´ï¼Œå°±æ˜¯å¯ä»¥ç”¨æœ‰é™çš„ã€æ˜ç¡®çš„æ•°å­¦è¡¨è¾¾å¼ç›´æ¥å†™å‡ºæ¥è§£ï¼Œä¸éœ€è¦è¿­ä»£ã€æ•°å€¼è¿‘ä¼¼æˆ–æ±‚è§£æ–¹ç¨‹ã€‚

### reparameterization trick

å®šä¹‰ï¼šé‡å‚æ•°åŒ–**å°†éšæœºå˜é‡ä»ä¸å¯å¯¼çš„é‡‡æ ·æ“ä½œä¸­è§£è€¦å‡ºæ¥**çš„æ–¹æ³•ï¼Œè®©é‡‡æ ·æ“ä½œå¯ä»¥å‚ä¸æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚

åŸç†ï¼šä»–æ²¡æœ‰æ¶ˆé™¤éšæœºé‡‡æ ·ï¼Œåªæ˜¯å°†éšæœºé‡‡æ ·å¯¹æ¢¯åº¦ä¼ æ’­çš„å½±å“é™åˆ°äº†æœ€ä½.

ä¸¾ä¾‹ï¼šå¦‚æœä½ æœ‰ä¸€ä¸ªéšæœºå˜é‡ $z \sim \mathcal{N}(\mu, \sigma^2)$ï¼Œç›´æ¥ä»è¿™ä¸ªåˆ†å¸ƒé‡‡æ ·ï¼Œæ¢¯åº¦æ— æ³•é€šè¿‡ $\mu, \sigma$ ä¼ æ’­ã€‚é‚£å°±å¯ä»¥æŒ‰ç…§ä¸‹å¼ä»éšæœºé‡‡æ · $z$ è½¬æ¢ä¸ºéšæœºé‡‡æ · $\epsilon$ã€‚

$$
\mathcal{N}(z; \mu, \sigma^2)
= \mu + \mathcal{N}(\epsilon'; 0, \sigma^2)
= \mu + \sigma \cdot \mathcal{N}(\epsilon; 0, 1)
$$

{{< details "PyTorchä»£ç ç¤ºä¾‹">}}
```python
import torch

# å‡è®¾ç¼–ç å™¨è¾“å‡ºçš„å‡å€¼å’Œæ ‡å‡†å·®
mu = torch.tensor([0.0, 1.0], requires_grad=True)
log_sigma = torch.tensor([0.0, 0.0], requires_grad=True)  # é€šå¸¸è¾“å‡º log(sigma) é¿å…è´Ÿæ•°
sigma = torch.exp(log_sigma)

# é‡å‚æ•°åŒ–é‡‡æ ·
epsilon = torch.randn_like(mu)  # ä»æ ‡å‡†æ­£æ€é‡‡æ ·
z = mu + sigma * epsilon  # z å¯å¯¼

# å‡è®¾ä¸€ä¸ªç®€å•çš„æŸå¤±å‡½æ•°
loss = (z**2).sum()
loss.backward()

print("grad mu:", mu.grad)
print("grad log_sigma:", log_sigma.grad)
```
{{< /details >}}

### æ‰©æ•£æ¨¡å‹å‰å‘å’Œåå‘çš„è®°æ³•

| æ–¹å‘ | æ¦‚ç‡å¯†åº¦                                        | å™ªå£° | ä½œç”¨                             | æ¦‚ç‡ç±»å‹ |
| ---- | ----------------------------------------------- | ---- | -------------------------------- | -------- |
| å‰å‘ | $q(\mathbf{x}_t \vert \mathbf{x}_{t-1})$        | åŠ å™ª | æ„é€ é«˜æ–¯é©¬å°”å¯å¤«é“¾ï¼Œé€æ­¥ç ´åæ•°æ® | çœŸå®åˆ†å¸ƒ |
| åå‘ | $p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ | å»å™ª | æ¢å¤æ•°æ®ï¼Œä»å™ªå£°ç”Ÿæˆæ ·æœ¬         | è¿‘ä¼¼åéªŒ     |

### å…ˆéªŒï¼Œä¼¼ç„¶ï¼Œä¸åéªŒ

| æ¦‚å¿µ | ç±»æ¯”è§£é‡Š                 | åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ç±»æ¯”                                     |
| ---- | ------------------------ | ------------------------------------------------------ |
| å…ˆéªŒ | åœ¨è§‚å¯Ÿä»»ä½•æ•°æ®ä¹‹å‰å¯¹å˜é‡çš„å‡è®¾åˆ†å¸ƒ             | $q(\mathbf{x}_t) \sim \mathcal{N}(0, I)$               |
| ä¼¼ç„¶ | æŸå‡è®¾/æ¨¡å‹å‚æ•°ä¸‹ï¼Œè§‚æµ‹æ•°æ®å‡ºç°çš„æ¦‚ç‡ | $p_\theta(\mathbf{x}_{t-1}\mid \mathbf{x}_t)$               |
| åéªŒ | ç»“åˆå…ˆéªŒä¸è§‚æµ‹æ•°æ®ä¹‹åå¯¹æ½œåœ¨å˜é‡çš„æ›´æ–°åˆ†å¸ƒ     | $q(\mathbf{x}_{t-1}\mid \mathbf{x}_t, \mathbf{x}_0)$ |

ä¸‰è€…ä¹‹é—´çš„å…³ç³»æ˜¯ï¼š

\[
\text{åéªŒ} = \frac{\text{ä¼¼ç„¶} \times \text{å…ˆéªŒ}}{\text{è¯æ®}}
\quad\leftrightarrow\quad
p(\text{å‚æ•°} | \text{æ•°æ®}) = \frac{p(\text{æ•°æ®} | \text{å‚æ•°}) \cdot p(\text{å‚æ•°})}{p(\text{æ•°æ®})}
\]

### Bayesâ€™ rule

è´å¶æ–¯å…¬å¼ï¼ˆBayesâ€™ Ruleï¼‰æ˜¯æ¦‚ç‡è®ºä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæ³•åˆ™ï¼Œç”¨äºåœ¨å·²çŸ¥æ¡ä»¶ä¸‹æ›´æ–°äº‹ä»¶çš„æ¦‚ç‡ã€‚å®ƒçš„åŸºæœ¬å½¢å¼æ˜¯ï¼š

\[
P(A \vert B) = \frac{P(B \vert A) \cdot P(A)}{P(B)}
\]

å…¶ä¸­ï¼š

- \(P(A)\)ï¼šäº‹ä»¶ A çš„å…ˆéªŒæ¦‚ç‡ï¼ˆåœ¨è§‚å¯Ÿ B ä¹‹å‰å¯¹ A çš„ä¿¡å¿µï¼‰
- \(P(B \vert A)\)ï¼šåœ¨ A å‘ç”Ÿçš„å‰æä¸‹ï¼Œè§‚å¯Ÿåˆ° B çš„å¯èƒ½æ€§ï¼ˆä¼¼ç„¶ï¼‰
- \(P(B)\)ï¼šäº‹ä»¶ B çš„è¾¹é™…æ¦‚ç‡ï¼ˆæ‰€æœ‰å¯èƒ½æƒ…å†µä¸‹ B å‘ç”Ÿçš„æ¦‚ç‡ï¼‰
- \(P(A \vert B)\)ï¼šåœ¨è§‚å¯Ÿåˆ° B ä¹‹åï¼Œäº‹ä»¶ A çš„åéªŒæ¦‚ç‡ï¼ˆæ›´æ–°åçš„ä¿¡å¿µï¼‰

### è”åˆåˆ†å¸ƒï¼Œè¾¹ç¼˜åˆ†å¸ƒå’Œæ¡ä»¶åˆ†å¸ƒ

- è”åˆåˆ†å¸ƒ $P(A, B)$ï¼šå…¨æ™¯åœ°å›¾ï¼ˆåŒ…å«æ‰€æœ‰ç»„åˆçš„æ¦‚ç‡ï¼‰ã€‚
- è¾¹ç¼˜åˆ†å¸ƒ $P(B)$ï¼šå…¨æ™¯åœ°å›¾æŠ•å½±åˆ°æŸä¸€ä¸ªè½´ã€‚
- æ¡ä»¶åˆ†å¸ƒ $P(A \vert B)$ï¼šå…¨æ™¯åœ°å›¾åˆ‡ä¸€æ¡çº¿ï¼ˆå·²çŸ¥å¦ä¸€å˜é‡çš„å€¼ï¼‰ï¼Œçœ‹è¿™æ¡çº¿ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒã€‚å…¬å¼ä¸ºï¼š$P(A \vert B) = \frac{P(A, B)}{P(B)}$

### KL æ•£åº¦

KL æ•£åº¦ï¼ˆKullbackâ€“Leibler Divergenceï¼‰ï¼Œä¹Ÿå«ç›¸å¯¹ç†µï¼ˆRelative Entropyï¼‰ï¼Œå®ƒç”¨æ¥è¡¡é‡ **ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚** çš„ä¸€ç§ä¿¡æ¯è®ºåº¦é‡ã€‚

$$
\begin{align}
\text{å¯¹äºç¦»æ•£åˆ†å¸ƒ:}\quad & D_{\mathrm{KL}}(P \,\|\, Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} \\
\text{å¯¹äºè¿ç»­åˆ†å¸ƒ:}\quad & D_{\mathrm{KL}}(P \,\|\, Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx
\end{align}
$$

TODOï¼šæ·»åŠ ä¸ªè½¬æ¢ä¸ºæœŸæœ›å½¢å¼çš„è¡¨è¾¾æ–¹å¼

è¿™é‡Œï¼š

- $P$ æ˜¯â€œçœŸå®â€åˆ†å¸ƒï¼ˆæˆ–ç›®æ ‡åˆ†å¸ƒï¼‰ã€‚
- $Q$ æ˜¯ç”¨æ¥è¿‘ä¼¼ $P$ çš„åˆ†å¸ƒã€‚
- $\log$ çš„åº•é€šå¸¸å–è‡ªç„¶å¯¹æ•°ï¼ˆä¿¡æ¯å•ä½ä¸º natsï¼‰ï¼Œå– 2 åˆ™å•ä½ä¸º bitsã€‚

ç›´è§‚ç†è§£:

- å¦‚æœä½ ç”¨ $Q$ ä½œä¸ºç¼–ç ç­–ç•¥å»ç¼–ç å®é™…ä¸Šæ¥è‡ª $P$ çš„æ•°æ®ï¼Œé‚£ä¹ˆå¹³å‡æ¯ä¸ªæ ·æœ¬ä¼šå¤šèŠ± $D_{\mathrm{KL}}(P\|Q)$ ä¸ªä¿¡æ¯å•ä½ï¼ˆnats/bitsï¼‰ã€‚
- KL æ•£åº¦è¡¨ç¤ºä½¿ç”¨åˆ†å¸ƒ $Q$ æ¥æ›¿ä»£ $P$ æ—¶ä¸¢å¤±çš„ä¿¡æ¯é‡ã€‚
- å…¬å¼é‡Œçš„ $\log \frac{P(x)}{Q(x)}$ æ˜¯ **å¯¹æ•°æ¦‚ç‡æ¯”**ï¼Œä¹˜ä¸Š $P(x)$ å¹¶å–æœŸæœ›ï¼Œå°±æ˜¯å¹³å‡çš„æ¦‚ç‡æ¯”å·®å¼‚ã€‚

æ€§è´¨:

- **éè´Ÿæ€§**: $D_{\mathrm{KL}}(P \,\|\, Q) \ge 0$
- **éå¯¹ç§°æ€§**: $D_{\mathrm{KL}}(P \,\|\, Q) \neq D_{\mathrm{KL}}(Q \,\|\, P)$
- **ç›¸å¯¹ç†µ=äº¤å‰ç†µ-é¦™å†œç†µ**: $D_{\mathrm{KL}}(P \,\|\, Q) = H(P, Q) - H(P)$

### å‡ ä¸ªé‡è¦çš„ç†µ

| ç†µç±»å‹     | å…¬å¼                                                      | è§£é‡Š                                   |
| ---------- | --------------------------------------------------------- | -------------------------------------- |
| **é¦™å†œç†µ** | $H(p) = -\sum_i p(x_i) \log p(x_i)$                       | è¡¡é‡åˆ†å¸ƒ $p$ çš„ä¸ç¡®å®šæ€§                |
| **äº¤å‰ç†µ** | $H(p,q) = -\sum_i p(x_i) \log q(x_i)$                     | è¡¡é‡ç”¨ $q$ è¡¨ç¤º $p$ çš„å¹³å‡ä¿¡æ¯é‡       |
| **ç›¸å¯¹ç†µ** | $D_{KL}(p\|q) = \sum_i p(x_i) \log \frac{p(x_i)}{q(x_i)}$ | è¡¡é‡ $p$ å’Œ $q$ çš„å·®å¼‚ï¼Œå¤šä»˜å‡ºçš„ä¿¡æ¯é‡ |

## Citation

{{< bibtex >}}

## References

[^ho_ddpm]: **Ho, Jonathan, Ajay Jain, and Pieter Abbeel.** â€œDenoising Diffusion Probabilistic Models.â€ _Advances in Neural Information Processing Systems_, edited by H. Larochelle et al., vol. 33, Curran Associates, Inc., 2020, pp. 6840â€“6851. https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.

[^nichol_improved_ddpm]: **Nichol, Alexander Quinn, and Prafulla Dhariwal.** â€œImproved Denoising Diffusion Probabilistic Models.â€ _Proceedings of the 38th International Conference on Machine Learning_, edited by Marina Meila and Tong Zhang, vol. 139, Proceedings of Machine Learning Research, 18â€“24 July 2021, pp. 8162â€“8171. PMLR. https://proceedings.mlr.press/v139/nichol21a.html.

[^mccandlish_grad_noise]: **McCandlish, Sam, et al.** _An Empirical Model of Large-Batch Training_. arXiv, 14 Dec. 2018, https://arxiv.org/abs/1812.06162.

[^lilian_diffusion]: **Weng, Lilian.** â€œWhat Are Diffusion Models?â€ _Lil'Log_, 11 July 2021, https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.

[^lilian_ae]: **Weng, Lilian.** â€œFrom Autoencoder to Beta-VAE.â€ _Lil'Log_, 12 Aug. 2018, https://lilianweng.github.io/posts/2018-08-12-vae/.

[^wiki_closed]: â€œClosed-form Expression.â€ _Wikipedia_, Wikimedia Foundation, https://en.wikipedia.org/wiki/Closed-form_expression.
