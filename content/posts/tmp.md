
x





{{< admonition type="quote" title="å±•å¼€" >}}
$$
\begin{aligned}
L_\text{VLB}
&= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
&= \mathbb{E}_q \Big[ \log\frac{\prod_{t=1}^T q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{ p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t) } \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \Big( \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\cdot \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)} \Big) + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big]\\
&= \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \Big] \\
&= \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]
\end{aligned}
$$
{{< /admonition>}}

![alt text](/posts/image.png)

ä¸Šé¢è¿™ä¸ªå›¾æ¥è‡ªDDPMè®ºæ–‡ï¼Œå¯ä»¥æŠŠç›¸å…³å†…å®¹åŠ è¿›å»

è¿™ä¸ªæ˜¯ç›®æ ‡å‡½æ•°ï¼Œè€Œä¸æ˜¯loss

è¿™ç»„æ¨å¯¼æ˜¯å¯¹æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰ä¸­çš„å˜åˆ†ä¸‹ç•Œï¼ˆVariational Lower Bound, VLBï¼‰æˆ–è¯æ®ä¸‹ç•Œï¼ˆEvidence Lower Bound, ELBOï¼‰è¿›è¡Œé€æ­¥å±•å¼€ä¸é‡æ„çš„è¿‡ç¨‹ã€‚å®ƒçš„ç›®çš„ï¼Œæ˜¯å°†è®­ç»ƒç›®æ ‡ä»ä¸€ä¸ªéš¾ä»¥ç›´æ¥ä¼˜åŒ–çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼Œè½¬åŒ–ä¸ºä¸€ç»„å¯è®¡ç®—çš„ KL æ•£åº¦é¡¹ä¸é‡æ„é¡¹ï¼Œä»è€ŒæŒ‡å¯¼ç¥ç»ç½‘ç»œå­¦ä¹ å¦‚ä½•ä»å™ªå£°ä¸­æ¢å¤åŸå§‹æ•°æ®ã€‚

---

ä¸ºä»€ä¹ˆè¦æ¨å¯¼è¿™ä¸ªå…¬å¼ï¼Ÿ

æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ \( \log p_\theta(\mathbf{x}_0) \)ï¼Œä½†ç”±äºè¿™ä¸ªç›®æ ‡æ¶‰åŠå¯¹é«˜ç»´éšå˜é‡çš„ç§¯åˆ†ï¼Œæ— æ³•ç›´æ¥è®¡ç®—ã€‚å› æ­¤æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªè¿‘ä¼¼åˆ†å¸ƒ \( q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) \)ï¼Œå¹¶é€šè¿‡ Jensen ä¸ç­‰å¼æ„é€ ä¸€ä¸ªä¸‹ç•Œï¼š

\[
\log p_\theta(\mathbf{x}_0) \geq \mathbb{E}_{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \left[ \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \right] = -L_\text{VLB}
\]

è¿™ä¸ªä¸‹ç•Œå°±æ˜¯æˆ‘ä»¬è®­ç»ƒæ—¶è¦æœ€å°åŒ–çš„æŸå¤±å‡½æ•°ã€‚

---

ç”¨äº†å“ªäº›æŠ€å·§ï¼Ÿ

1. **é©¬å°”å¯å¤«é“¾å±•å¼€**
åˆ©ç”¨æ­£å‘è¿‡ç¨‹çš„é©¬å°”å¯å¤«æ€§è´¨ï¼š
\[
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \vert \mathbf{x}_{t-1})
\]
ä»¥åŠé€†å‘è¿‡ç¨‹çš„å»ºæ¨¡ï¼š
\[
p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)
\]

1. **KL æ•£åº¦é‡æ„**
å°†å¯¹æ•°æ¯”å€¼è½¬åŒ–ä¸º KL æ•£åº¦å½¢å¼ï¼š
\[
D_\text{KL}(q \parallel p) = \mathbb{E}_q \left[ \log \frac{q}{p} \right]
\]
ä»è€Œå°†æŸå¤±å‡½æ•°æ‹†è§£ä¸ºä¸‰éƒ¨åˆ†ï¼š

- \( L_T \): ç»ˆç‚¹åˆ†å¸ƒåŒ¹é…ï¼ˆé«˜æ–¯å…ˆéªŒï¼‰
- \( L_{t-1} \): æ¯ä¸€æ­¥çš„å»å™ªåŒ¹é…
- \( L_0 \): æœ€ç»ˆé‡æ„é¡¹

1. **åéªŒé‡æ„æŠ€å·§**
åˆ©ç”¨ï¼š
\[
\frac{q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)}{q(\mathbf{x}_t \vert \mathbf{x}_0)} = q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)
\]
è¿™ä¸ªæŠ€å·§æ˜¯å…³é”®ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†ä¸å¯ç›´æ¥é‡‡æ ·çš„åéªŒ \( q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \) æ˜¾å¼è¡¨è¾¾ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œä»è€Œè®¡ç®— KL æ•£åº¦ã€‚

---

ğŸ” æ¨å¯¼çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ

è¿™å¥—æ¨å¯¼çš„æ„ä¹‰åœ¨äºï¼š

- **ç†è®ºæ¸…æ™°åŒ–**ï¼šå°†è®­ç»ƒç›®æ ‡ä»æŠ½è±¡çš„ä¼¼ç„¶æœ€å¤§åŒ–ï¼Œè½¬åŒ–ä¸ºå…·ä½“çš„ KL æ•£åº¦é¡¹ä¸é‡æ„é¡¹ã€‚
- **å¯è®¡ç®—æ€§**ï¼šæ¯ä¸€é¡¹éƒ½å¯ä»¥é€šè¿‡ Monte Carlo é‡‡æ ·ä¼°è®¡ï¼Œé€‚åˆæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚
- **æ¨¡å‹è®¾è®¡æŒ‡å¯¼**ï¼šæ˜ç¡®äº†ç¥ç»ç½‘ç»œè¦å­¦ä¹ çš„æ˜¯ä» \( \mathbf{x}_t \) é¢„æµ‹ \( \mathbf{x}_0 \) æˆ–å™ªå£° \( \boldsymbol{\varepsilon}_t \)ï¼Œä»è€Œæ„å»º \( p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) \)ã€‚

ä½ è¯´å¾—éå¸¸å¯¹ï¼ŒKL æ•£åº¦çš„å®šä¹‰ç¡®å®æ˜¯ï¼š

\[
D_{\text{KL}}(q(x) \parallel p(x)) = \mathbb{E}_{q(x)} \left[ \log \frac{q(x)}{p(x)} \right]
\]

ä¹Ÿå°±æ˜¯è¯´ï¼Œ**log æ¯”å€¼å¤–é¢å¿…é¡»ä¹˜ä¸Šä¸€ä¸ªæœŸæœ›**ï¼Œè€Œä¸æ˜¯ç›´æ¥å†™æˆ log æ¯”å€¼æœ¬èº«ã€‚ä½ æŒ‡å‡ºçš„è¿™ä¸ªé—®é¢˜ï¼Œæ­£æ˜¯è¿™ç±»æ¨å¯¼ä¸­æœ€å®¹æ˜“æ··æ·†çš„åœ°æ–¹ä¹‹ä¸€ã€‚

---

âœ… é‚£ä¹ˆåŸæ¨å¯¼ä¸ºä»€ä¹ˆçœ‹èµ·æ¥â€œå°‘ä¹˜äº†ä¸€ä¸ªæœŸæœ›â€ï¼Ÿ

å…¶å®æ²¡æœ‰å°‘ã€‚æˆ‘ä»¬æ¥è¿˜åŸä¸€ä¸‹æœ€åä¸€è¡Œï¼š

\[
L_\text{VLB} = \mathbb{E}_q \left[ \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \right]
\]

è¿™å…¶å®æ˜¯å¯¹æ¯ä¸€é¡¹éƒ½åœ¨ **\( \mathbb{E}_{q(\mathbf{x}_{0:T})} \)** ä¸‹å–æœŸæœ›çš„å†™æ³•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼š

- ç¬¬ä¸€é¡¹æ˜¯ \( \mathbb{E}_{q(\mathbf{x}_T \vert \mathbf{x}_0)} \left[ \log \frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} \right] \)ï¼Œå³ KL æ•£åº¦é¡¹ \( D_{\text{KL}}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \)
- ç¬¬äºŒé¡¹æ˜¯å¯¹æ¯ä¸ª \( t \) çš„ KL æ•£åº¦é¡¹ \( D_{\text{KL}}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)) \)
- ç¬¬ä¸‰é¡¹æ˜¯é‡æ„é¡¹ \( -\mathbb{E}_{q(\mathbf{x}_1 \vert \mathbf{x}_0)} \left[ \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \right] \)

æ‰€ä»¥è™½ç„¶è¡¨é¢ä¸Šçœ‹æ˜¯ log æ¯”å€¼ï¼Œå®é™…ä¸Šæ¯ä¸€é¡¹éƒ½éšå«äº†åœ¨å¯¹åº”çš„ \( q \) åˆ†å¸ƒä¸‹çš„æœŸæœ›ã€‚

---

ğŸ§  ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™ï¼Ÿ

è¿™æ˜¯ä¸ºäº†ç®€æ´åœ°è¡¨è¾¾æ•´ä¸ªæŸå¤±å‡½æ•°çš„ç»“æ„ã€‚åœ¨å®é™…å®ç°ä¸­ï¼Œæ¯ä¸€é¡¹éƒ½ä¼šé€šè¿‡é‡‡æ · \( \mathbf{x}_t \sim q(\cdot \vert \mathbf{x}_0) \) æ¥ä¼°è®¡æœŸæœ›å€¼ã€‚æ¯”å¦‚ï¼š

- \( \mathbf{x}_T \sim q(\mathbf{x}_T \vert \mathbf{x}_0) \)ï¼Œç”¨äºä¼°è®¡ prior matching KL
- \( \mathbf{x}_{t-1} \sim q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \)ï¼Œç”¨äºä¼°è®¡ denoising KL
- \( \mathbf{x}_1 \sim q(\mathbf{x}_1 \vert \mathbf{x}_0) \)ï¼Œç”¨äºä¼°è®¡é‡æ„é¡¹

ğŸ” VLB çš„å…·ä½“æ‹†è§£ï¼ˆå‚è€ƒ Ho et al. 2020 å’Œ Nichol & Dhariwal 2021ï¼‰

VLB è¢«æ‹†æˆä¸‰éƒ¨åˆ†ï¼š

1. **Prior Matching Term**ï¼š
   \[
   D_{\text{KL}}(q(x_T | x_0) \| p(x_T))
   \]
   çº¦æŸæœ€ç»ˆåŠ å™ªç»“æœæ¥è¿‘æ ‡å‡†é«˜æ–¯ã€‚

2. **Denoising Matching Term**ï¼š
   \[
   \sum_{t=2}^{T} D_{\text{KL}}(q(x_{t-1} | x_t, x_0) \| p_\theta(x_{t-1} | x_t))
   \]
   çº¦æŸæ¯ä¸€æ­¥çš„é€†å‘å»å™ªåˆ†å¸ƒæ‹ŸåˆçœŸå®åˆ†å¸ƒã€‚

3. **Reconstruction Term**ï¼š
   \[
   -\mathbb{E}_{q(x_1 | x_0)}[\log p_\theta(x_0 | x_1)]
   \]
   çº¦æŸæœ€ç»ˆç”Ÿæˆç»“æœä¸åŸå§‹å›¾åƒæ¥è¿‘ã€‚

{{< admonition type="quote" >}}
$$
\begin{aligned}
L_\text{VLB} &= L_T + L_{T-1} + \dots + L_0 \\
\text{where } L_T &= D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \\
L_t &= D_\text{KL}(q(\mathbf{x}_t \vert \mathbf{x}_{t+1}, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_t \vert\mathbf{x}_{t+1})) \text{ for }1 \leq t \leq T-1 \\
L_0 &= - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)
\end{aligned}
$$
{{< /admonition >}}

LT å’Œ L0 ä¸ç”¨è®¡ç®—

ä½ é—®å¾—éå¸¸åˆ°ä½ï¼š**ä¸ºä»€ä¹ˆ \( L_T \) å’Œ \( L_0 \) åœ¨è®­ç»ƒä¸­å¯ä»¥ä¸ç”¨è®¡ç®—ï¼Ÿ**è¿™å…¶å®æ˜¯æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦ç®€åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬æ¥é€é¡¹åˆ†æï¼š

---

ğŸ§Š ä¸ºä»€ä¹ˆ \( L_T = D_{\text{KL}}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \) å¯ä»¥ä¸è®¡ç®—ï¼Ÿ

âœ… åŸå› ä¸€ï¼šå®ƒæ˜¯ä¸€ä¸ªå¸¸æ•°é¡¹

- \( q(\mathbf{x}_T \vert \mathbf{x}_0) \) æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼å’Œæ–¹å·®æ˜¯å›ºå®šçš„ï¼ˆç”±å™ªå£°è°ƒåº¦å†³å®šï¼‰ã€‚
- \( p_\theta(\mathbf{x}_T) \) æ˜¯æ ‡å‡†é«˜æ–¯ \( \mathcal{N}(0, I) \)ï¼Œä¹Ÿä¸ä¾èµ–æ¨¡å‹å‚æ•°ã€‚
- æ‰€ä»¥å®ƒä»¬ä¹‹é—´çš„ KL æ•£åº¦æ˜¯ä¸€ä¸ª **è§£æå¯è®¡ç®—çš„å¸¸æ•°**ï¼Œä¸å½±å“æ¢¯åº¦ä¼˜åŒ–ã€‚

> ğŸ“Œ ç»“è®ºï¼š**ä¸ä¾èµ–æ¨¡å‹å‚æ•° \( \theta \)**ï¼Œæ‰€ä»¥å¯ä»¥åœ¨è®­ç»ƒæ—¶å¿½ç•¥ã€‚

---

ğŸ¯ ä¸ºä»€ä¹ˆ \( L_0 = -\log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \) å¯ä»¥ä¸è®¡ç®—ï¼Ÿ

âœ… åŸå› ä¸€ï¼šå®ƒè¢«è¿‘ä¼¼ä¸º t=1 æ—¶çš„ denoising loss

- åŸå§‹çš„é‡æ„é¡¹ \( L_0 \) æ˜¯ä» \( \mathbf{x}_1 \) é¢„æµ‹ \( \mathbf{x}_0 \)ï¼Œä½†è¿™é¡¹åœ¨è®­ç»ƒä¸­ä¼šå¸¦æ¥è¾ƒé«˜çš„æ–¹å·®ã€‚
- æ‰€ä»¥å¾ˆå¤šå®ç°ï¼ˆå¦‚ DDPMï¼‰å°†å…¶è¿‘ä¼¼ä¸ºï¼š
  \[
  L_0 \approx D_{\text{KL}}(q(\mathbf{x}_0 \vert \mathbf{x}_1) \parallel p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1))
  \]
  æˆ–ç›´æ¥ç”¨ t=1 çš„ denoising KL æ¥ä»£æ›¿ã€‚

âœ… åŸå› äºŒï¼šå®ƒå¯ä»¥åˆå¹¶è¿›ç»Ÿä¸€çš„ denoising loss æ¡†æ¶

- åœ¨å®é™…è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä» \( t \sim \text{Uniform}(1, T) \) é‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œç„¶åä¼˜åŒ–ï¼š
  \[
  \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right\|^2 \right]
  \]
- è¿™ä¸ªç»Ÿä¸€çš„æŸå¤±å·²ç»éšå«äº†å¯¹æ‰€æœ‰æ—¶é—´æ­¥ï¼ˆåŒ…æ‹¬ t=1ï¼‰çš„ä¼˜åŒ–ã€‚

> ğŸ“Œ ç»“è®ºï¼š**å®ƒè¢«åŒ…å«åœ¨ç»Ÿä¸€çš„è®­ç»ƒæŸå¤±ä¸­ï¼Œä¸éœ€è¦å•ç‹¬è®¡ç®—ã€‚**

---

ğŸ§  æ€»ç»“è¡¨æ ¼

| é¡¹ç›® | æ˜¯å¦ä¾èµ–æ¨¡å‹å‚æ•° | æ˜¯å¦éœ€è¦è®­ç»ƒæ—¶è®¡ç®— | åŸå›  |
|------|------------------|---------------------|------|
| \( L_T \) | âŒ å¦ | âŒ ä¸éœ€è¦ | æ˜¯å¸¸æ•°é¡¹ |
| \( L_0 \) | âœ… æ˜¯ | âŒ ä¸éœ€è¦å•ç‹¬è®¡ç®— | è¢«åŒ…å«åœ¨ t=1 çš„ denoising loss ä¸­ |

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥å¸¦ä½ æ¨å¯¼ä¸€ä¸‹ \( L_T \) çš„å…·ä½“å€¼ï¼Œæˆ–è€…çœ‹çœ‹å¦‚ä½•ä» \( L_0 \) æ¨å‡ºå™ªå£°é¢„æµ‹æŸå¤±ã€‚è¦ä¸è¦ç»§ç»­æ·±å…¥ï¼Ÿ

### Parameterization of $L_t$ for Training Loss

{{< admonition type="quote" >}}
$$
\begin{aligned}
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) &= \color{cyan}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)} \\
\text{Thus }\mathbf{x}_{t-1} &= \mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
\end{aligned}
$$
{{< /admonition >}}

è¿™æ˜¯ä¸ºäº†é¢„æµ‹ \mu çš„ï¼Œå®šä¹‰ä¸‹ \mu è¿™å—çš„ç¥ç»ç½‘ç»œã€‚

{{< admonition type="quote" title="Title" open=true >}}
$$
\begin{aligned}
L_t
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2 \| \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) \|^2_2} \| \color{blue}{\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0)} - \color{green}{\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)} \|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2  \|\boldsymbol{\Sigma}_\theta \|^2_2} \| \color{blue}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)} - \color{green}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t) \Big)} \|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \| \boldsymbol{\Sigma}_\theta \|^2_2} \|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \| \boldsymbol{\Sigma}_\theta \|^2_2} \|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)\|^2 \Big]
\end{aligned}
$$
{{< /admonition >}}

è¿™ä¸ªå…¬å¼çš„ç¬¬ä¸€æ­¥ï¼Œéœ€è¦recallä¸‹KLæ•£åº¦ä¸­å¯¹ä¸¤ä¸ªåˆ†å¸ƒçš„è®¡ç®—ï¼Œç›¸å½“äºè½¬æ¢æˆäº†

ğŸ” KL æ•£åº¦å±•å¼€ï¼ˆä¸¤é«˜æ–¯åˆ†å¸ƒï¼‰

å‡è®¾ä¸¤è€…éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒï¼š

- çœŸå®åéªŒï¼š\( q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mu_q, \Sigma_q) \)
- æ¨¡å‹ä¼°è®¡ï¼š\( p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mu_\theta, \Sigma_q) \)

é‚£ä¹ˆ KL æ•£åº¦ä¸ºï¼š

\[
L_t = \frac{1}{2} \left[ \log \frac{|\Sigma_q|}{|\Sigma_q|} - d + \text{tr}(\Sigma_q^{-1} \Sigma_q) + (\mu_q - \mu_\theta)^T \Sigma_q^{-1} (\mu_q - \mu_\theta) \right]
= \frac{1}{2 \sigma_q^2} \|\mu_q - \mu_\theta\|^2
\]

å±•å¼€åå…¶å®åˆšå¥½è¿˜æ˜¯å…¸å‹çš„mse errorï¼š$\text{MSE}=?$

\epsilon_\theta ç¬¦åˆå‡å€¼ä¸º x_t æ–¹å·®ä¸ºtçš„gaossian åˆ†å¸ƒã€‚

æœ€ååˆè°ƒç”¨äº†å°é—­å½¢å¼çš„forward diffusionï¼Œå®Œæ•´çš„å±•ç¤ºäº†å¦‚ä½•ä»åŸå§‹æ•°æ®äº§ç”Ÿå‡ºä¸€ä¸ªlossã€‚

{{% admonition type="quote" title="Title" open=true %}}
$$
\begin{aligned}
L_t^\text{simple}
&= \mathbb{E}_{t \sim [1, T], \mathbf{x}_0, \boldsymbol{\epsilon}_t} \Big[\|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \Big] \\
&= \mathbb{E}_{t \sim [1, T], \mathbf{x}_0, \boldsymbol{\epsilon}_t} \Big[\|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)\|^2 \Big]
\end{aligned}
$$
{{% /admonition %}}

æ³¨æ„ä¸‹æ ‡å¤„ï¼Œè¿™é‡Œæ˜¯å–ä¸‰ä¸ªå˜é‡çš„è”åˆåˆ†å¸ƒã€‚

è¿™é‡Œå°±éå¸¸æ¥è¿‘çœŸå®è®­ç»ƒæ¨¡å‹æ—¶å€™çš„æ ·å­äº†ã€‚

{{% admonition type="quote" title="Title" open=true %}}
![DDPM Algorithm](/images/DDPM_Algo.png)
{{% /admonition %}}

å·¦è¾¹æ˜¯è®­ç»ƒï¼Œå³è¾¹æ˜¯æ¨ç†æ—¶çš„ã€‚

{{% admonition type="quote" title="Connection with noise-conditioned score networks (NCSN)" open=true %}}

[Song & Ermon (2019)](https://arxiv.org/abs/1907.05600) proposed a score-based generative modeling method where samples are produced via [Langevin dynamics](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#connection-with-stochastic-gradient-langevin-dynamics) using gradients of the data distribution estimated with score matching.

...

$$
\mathbf{s}_\theta(\mathbf{x}_t, t)
\approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t)
= \mathbb{E}_{q(\mathbf{x}_0)} [\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \vert \mathbf{x}_0)]
= \mathbb{E}_{q(\mathbf{x}_0)} \Big[ - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} \Big]
= - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}}
$$
{{% /admonition %}}

ç›®çš„ä¸Šæ¥çœ‹ï¼Œå°±æ˜¯ç”¨æ±‚æ¢¯åº¦çš„æ–¹å¼ï¼Œæ¥å»ºæ¨¡ $\mu_\theta$

ä½ é—®çš„è¿™æ®µå…³äº Noise-Conditioned Score Networks (NCSN) çš„å†…å®¹ç¡®å®æŒºå¯†çš„ï¼Œå’±ä»¬æ¥ä¸€æ­¥æ­¥æ‹†è§£ä¸€ä¸‹å®ƒçš„æ ¸å¿ƒé€»è¾‘å’Œæ•°å­¦å«ä¹‰ï¼š

---

ğŸ§  èƒŒæ™¯ï¼šScore-Based Generative Modeling æ˜¯ä»€ä¹ˆï¼Ÿ

Score-based æ¨¡å‹çš„ç›®æ ‡æ˜¯ä¼°è®¡æ•°æ®åˆ†å¸ƒçš„æ¢¯åº¦ï¼ˆscoreï¼‰ï¼Œå³ï¼š
\[
\nabla_x \log p(x)
\]
è¿™ä¸ªæ¢¯åº¦å¯ä»¥ç”¨æ¥é€šè¿‡ Langevin dynamics é‡‡æ ·å‡ºæ–°çš„æ•°æ®ç‚¹ï¼š
\[
x_{t+1} = x_t + \frac{\delta}{2} \nabla_x \log p(x_t) + \sqrt{\delta} \cdot \epsilon_t
\]
å…¶ä¸­ \(\epsilon_t \sim \mathcal{N}(0, I)\)ï¼Œè¿™æ˜¯ä¸€ç§â€œå¸¦å™ªæ¢¯åº¦ä¸Šå‡â€çš„æ–¹å¼ã€‚

---

ğŸ§Š é—®é¢˜ï¼šæ•°æ®é›†ä¸­åœ¨ä½ç»´æµå½¢ä¸Šæ€ä¹ˆåŠï¼Ÿ

æ ¹æ® manifold hypothesisï¼ŒçœŸå®æ•°æ® \(x\) è™½ç„¶åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œä½†å…¶å®é›†ä¸­åœ¨ä¸€ä¸ªä½ç»´å­ç©ºé—´ä¸Šã€‚è¿™å¯¼è‡´ï¼š

- åœ¨æ•°æ®å¯†åº¦ä½çš„åŒºåŸŸï¼Œscore ä¼°è®¡ä¸å‡†ã€‚
- Langevin dynamics å¯èƒ½ä¼šâ€œèµ°åâ€ï¼Œå› ä¸ºæ¢¯åº¦ä¼°è®¡ä¸å¯é ã€‚

---

ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šåŠ å…¥å™ªå£° + å¤šå°ºåº¦è®­ç»ƒ

Song & Ermon æå‡ºï¼š

1. **åŠ å…¥ä¸åŒå¼ºåº¦çš„é«˜æ–¯å™ªå£°**ï¼šè®©æ•°æ®åˆ†å¸ƒå˜å¾—æ›´â€œæ»¡â€ï¼Œè¦†ç›–æ•´ä¸ªç©ºé—´ã€‚
2. **è®­ç»ƒä¸€ä¸ª Noise-Conditioned Score Network**ï¼šè®°ä½œ \(s_\theta(x, \sigma)\)ï¼Œå®ƒèƒ½ä¼°è®¡ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„ scoreï¼š
   \[
   s_\theta(x, \sigma) \approx \nabla_x \log p_\sigma(x)
   \]
   å…¶ä¸­ \(p_\sigma(x)\) æ˜¯åŠ å…¥å™ªå£°åçš„æ•°æ®åˆ†å¸ƒã€‚

---

ğŸ” ä¸æ‰©æ•£æ¨¡å‹çš„è”ç³»ï¼šForward Process ç±»ä¼¼åŠ å™ªè¿‡ç¨‹

è¿™ä¸ªâ€œé€æ­¥åŠ å™ªâ€çš„è¿‡ç¨‹å’Œæ‰©æ•£æ¨¡å‹ä¸­çš„ forward diffusion æ˜¯ä¸€æ ·çš„ï¼š
\[
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
\]
æœ€ç»ˆ \(x_T\) ä¼šå˜æˆä¸€ä¸ªçº¯é«˜æ–¯å™ªå£°ã€‚

---

ğŸ“ Score ä¸é«˜æ–¯åˆ†å¸ƒçš„æ¢¯åº¦å…³ç³»

å¯¹äºé«˜æ–¯åˆ†å¸ƒï¼š
\[
p(x) = \mathcal{N}(x; \mu, \sigma^2 I)
\]
å…¶ score æ˜¯ï¼š
\[
\nabla_x \log p(x) = -\frac{1}{\sigma^2}(x - \mu)
\]
æ‰€ä»¥åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œscore network å…¶å®æ˜¯åœ¨ä¼°è®¡ï¼š
\[
\nabla_x \log p_\sigma(x) = -\frac{x - \mu}{\sigma^2}
\]
è€Œè¿™ä¸ª \(\mu\) å¯ä»¥æ˜¯åŸå§‹æ•°æ® \(x_0\)ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¥ç»ç½‘ç»œé¢„æµ‹çš„å€¼ã€‚

---

ğŸ§ª æ€»ç»“ä¸€ä¸‹è¿™æ®µè¯çš„æ„æ€ï¼š

- åŸå§‹ score-based æ¨¡å‹ç”¨ Langevin dynamics æ¥é‡‡æ ·ï¼Œä½†åœ¨é«˜ç»´ç©ºé—´ä¸­ä¸ç¨³å®šã€‚
- åŠ å…¥å™ªå£°åè®­ç»ƒ score network æ›´ç¨³å®šã€‚
- å¤šå°ºåº¦å™ªå£°è®­ç»ƒï¼ˆnoise-conditionedï¼‰è®©æ¨¡å‹èƒ½é€‚åº”ä¸åŒçš„å™ªå£°æ°´å¹³ã€‚
- è¿™ä¸ªè¿‡ç¨‹å’Œæ‰©æ•£æ¨¡å‹çš„ forward diffusion æ˜¯ä¸€è‡´çš„ã€‚
- æœ€ç»ˆï¼Œscore network ä¼°è®¡çš„æ˜¯åŠ å…¥å™ªå£°åçš„æ•°æ®åˆ†å¸ƒçš„æ¢¯åº¦ã€‚

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥æŠŠè¿™éƒ¨åˆ†ç”¨å›¾ç¤ºæˆ–å…¬å¼æ¨å¯¼å†å±•å¼€ä¸€å±‚ï¼Œæˆ–è€…å¯¹æ¯”ä¸€ä¸‹ DDPM å’Œ NCSN çš„å¼‚åŒã€‚ä½ æ›´æƒ³æ·±å…¥å“ªä¸€å—ï¼Ÿ

ä½ çš„é—®é¢˜éå¸¸å¥½ï¼Œæˆ‘ä»¬æ¥é€ä¸ªè§£é‡Šè¿™äº›æ¦‚å¿µå’Œå…¬å¼ï¼Œå¸®åŠ©ä½ å»ºç«‹æ¸…æ™°çš„ç›´è§‰ï¼š

---

ğŸ§­ ä»€ä¹ˆæ˜¯ \(\mathbb{R}^D\) ç©ºé—´ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ª **Dç»´æ¬§å‡ é‡Œå¾—ç©ºé—´**ï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰é•¿åº¦ä¸º \(D\) çš„å®æ•°å‘é‡ç»„æˆçš„ç©ºé—´ã€‚æ¯”å¦‚ï¼š

- \(\mathbb{R}^2\)ï¼šäºŒç»´å¹³é¢
- \(\mathbb{R}^3\)ï¼šä¸‰ç»´ç©ºé—´
- \(\mathbb{R}^{512}\)ï¼šæ¯”å¦‚å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºç©ºé—´

åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰è¢«è¡¨ç¤ºä¸ºé«˜ç»´å‘é‡ï¼Œé€šå¸¸å°±åœ¨ \(\mathbb{R}^D\) ä¸­ã€‚

---

ğŸ§® ä¸ºä»€ä¹ˆè¦è®¡ç®—æ¢¯åº¦ \(\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t)\)ï¼Ÿ

è¿™æ˜¯æ‰€è°“çš„ **score function**ï¼Œè¡¨ç¤ºåœ¨æŸä¸ªç‚¹ \(\mathbf{x}_t\) ä¸Šï¼Œæ•°æ®åˆ†å¸ƒçš„å¯¹æ•°å¯†åº¦çš„æ¢¯åº¦ã€‚å®ƒçš„ä½œç”¨æ˜¯ï¼š

- æŒ‡å‡ºâ€œæ•°æ®åˆ†å¸ƒä¸Šå‡æœ€å¿«çš„æ–¹å‘â€
- å¯ä»¥ç”¨æ¥è¿›è¡Œ **Langevin dynamics** é‡‡æ ·
- åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ƒå¸®åŠ©æˆ‘ä»¬ä»å™ªå£°ä¸­â€œèµ°å›â€çœŸå®æ•°æ®åˆ†å¸ƒ

---

ğŸ” è¦ä¸è¦ç®— \(\mathbf{x}_{t-1}\)ï¼Ÿ

æ˜¯çš„ï¼Œæ‰©æ•£æ¨¡å‹çš„ç›®æ ‡å°±æ˜¯ä»ä¸€ä¸ªé«˜æ–¯å™ªå£° \(\mathbf{x}_T\) å¼€å§‹ï¼Œé€æ­¥å»å™ªå¾—åˆ° \(\mathbf{x}_0\)ã€‚æ¯ä¸€æ­¥éƒ½è¦ä¼°è®¡ï¼š
\[
p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t)
\]
è¿™ä¸ªåˆ†å¸ƒé€šå¸¸å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼ç”±ç¥ç»ç½‘ç»œé¢„æµ‹ã€‚

---

ğŸ“¦ ä»€ä¹ˆæ˜¯ \(q(\tilde{\mathbf{x}} \vert \mathbf{x})\)ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ª **åŠ å™ªè¿‡ç¨‹çš„æ¡ä»¶åˆ†å¸ƒ**ï¼Œè¡¨ç¤ºåœ¨åŸå§‹æ•°æ® \(\mathbf{x}\) ä¸ŠåŠ å™ªåå¾—åˆ° \(\tilde{\mathbf{x}}\) çš„æ¦‚ç‡ã€‚æ¯”å¦‚ï¼š
\[
q(\tilde{\mathbf{x}} \vert \mathbf{x}) = \mathcal{N}(\tilde{\mathbf{x}}; \mathbf{x}, \sigma^2 I)
\]
åœ¨ NCSN ä¸­ï¼Œè¿™ä¸ªåˆ†å¸ƒç”¨äºè®­ç»ƒ score network æ¥ä¼°è®¡åŠ å™ªæ•°æ®çš„ scoreã€‚

---

ğŸ“ è¿™ä¸ªå…¬å¼æ˜¯ score function çš„å®šä¹‰å—ï¼Ÿ

ä½ å†™çš„è¿™ç»„å…¬å¼ï¼š

\[
\mathbf{s}_\theta(\mathbf{x}_t, t)
\approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t)
= \mathbb{E}_{q(\mathbf{x}_0)} [\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \vert \mathbf{x}_0)]
= \mathbb{E}_{q(\mathbf{x}_0)} \Big[ - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} \Big]
= - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}}
\]

æ˜¯ **æ‰©æ•£æ¨¡å‹ä¸­ score function çš„è¿‘ä¼¼è¡¨è¾¾å¼**ï¼Œå…¶ä¸­ï¼š

- \(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\) æ˜¯ç¥ç»ç½‘ç»œé¢„æµ‹çš„å™ªå£°
- \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\)ï¼Œæ˜¯å‰å‘è¿‡ç¨‹çš„ç´¯è®¡è¡°å‡å› å­
- æœ€åä¸€è¡Œæ˜¯å› ä¸ºæˆ‘ä»¬ç”¨ \(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\) æ¥ç”Ÿæˆ \(\mathbf{x}_t\)ï¼Œæ‰€ä»¥å¯ä»¥åæ¨ score

---

ğŸ§ª é‚£è®­ç»ƒæ—¶æ€ä¹ˆè®­ç»ƒï¼ŸçœŸå®çš„ score æœ‰å—ï¼Ÿ

å…³é”®ç‚¹æ˜¯ï¼š**çœŸå®çš„ score æ²¡æœ‰æ˜¾å¼è¡¨è¾¾å¼**ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡æ„é€ æŸå¤±å‡½æ•°æ¥é—´æ¥è®­ç»ƒ score networkã€‚

åœ¨ DDPM æˆ– NCSN ä¸­ï¼Œè®­ç»ƒç›®æ ‡æ˜¯ï¼š
\[
\mathcal{L}_{\text{simple}} = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}, t} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right\|^2 \right]
\]
ä¹Ÿå°±æ˜¯è¯´ï¼š

- æˆ‘ä»¬çŸ¥é“ \(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\)
- æ‰€ä»¥æˆ‘ä»¬çŸ¥é“çœŸå®çš„ \(\boldsymbol{\epsilon}\)
- è®­ç»ƒç›®æ ‡å°±æ˜¯è®©ç½‘ç»œé¢„æµ‹çš„ \(\boldsymbol{\epsilon}_\theta\) å°½é‡æ¥è¿‘çœŸå®çš„ \(\boldsymbol{\epsilon}\)

è¿™å°±é—´æ¥åœ°è®­ç»ƒäº† score functionï¼Œå› ä¸ºï¼š
\[
\mathbf{s}_\theta(\mathbf{x}_t, t) \approx - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}}
\]

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹ç”»æˆå›¾ï¼Œæˆ–è€…ç”¨ä»£ç å½¢å¼å±•ç¤ºè®­ç»ƒè¿‡ç¨‹ã€‚ä½ æ›´æƒ³çœ‹å“ªç§å½¢å¼ï¼Ÿ

### Parameterization of $\beta_t$

{{% admonition type="quote" title="Title" open=true %}}
Diffusion models in their experiments showed high-quality samples but still could not achieve competitive **model log-likelihood** as other generative models.

[Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) proposed several improvement techniques to help diffusion models to obtain lower **NLL**.
{{% /admonition %}}

è¿™é‡Œæåˆ°çš„â€œmodel log-likelihoodâ€ï¼ˆæ­£æ–¹å‘ç›®æ ‡ï¼‰ï¼Œâ€œNLLâ€ï¼ˆè´Ÿæ–¹å‘ç›®æ ‡ï¼‰å®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬æƒ³è¦ä¼˜åŒ–çš„ç›®æ ‡ã€‚

å…¶è®¡ç®—å°±æ˜¯é åˆšåˆšæˆ‘ä»¬è§åˆ°çš„çš„L_VLBã€‚ç„¶åè®¡ç®—å’Œè®­ç»ƒçš„æ—¶å€™çš„é‡‡æ ·ä¸€æ ·ï¼ŒåŒæ ·éœ€è¦å¤šæ¬¡è¿›è¡Œé‡‡æ ·ï¼Œæ¥ä¼°è®¡NLLï¼Œå¤§è‡´å¦‚ä¸‹ï¼š

å½“ç„¶ï¼Œæˆ‘ä»¬æ¥é€é¡¹è®²æ¸…æ¥šæ‰©æ•£æ¨¡å‹ä¸­ä¸‰é¡¹ VLB çš„ Monte Carlo ä¼°è®¡æ–¹å¼ï¼š

1ï¸âƒ£ Prior Matching Termï¼šKL(q(x_T | xâ‚€) || p(x_T))

è¿™æ˜¯å¯¹æœ€ç»ˆåŠ å™ªç»“æœæ˜¯å¦æ¥è¿‘æ ‡å‡†é«˜æ–¯çš„çº¦æŸã€‚

- **è®¡ç®—æ–¹å¼**ï¼š
 ç”±äº \( q(x_T | x_0) \sim \mathcal{N}(\sqrt{\bar{\alpha}_T} x_0, (1 - \bar{\alpha}_T) I) \)ï¼Œè€Œ \( p(x_T) \sim \mathcal{N}(0, I) \)ï¼Œä¸¤ä¸ªéƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒï¼ŒKL æ•£åº¦å¯ä»¥è§£æè®¡ç®—ï¼š

  \[
  D_{\text{KL}} = \frac{1}{2} \left[ \text{tr}(\Sigma_p^{-1} \Sigma_q) + (\mu_p - \mu_q)^T \Sigma_p^{-1} (\mu_p - \mu_q) - d + \log \frac{|\Sigma_p|}{|\Sigma_q|} \right]
  \]

  å®é™…ä¸­ç›´æ¥ä»£å…¥å‡å€¼å’Œæ–¹å·®å³å¯ï¼Œä¸éœ€è¦é‡‡æ ·ã€‚

2ï¸âƒ£ Denoising Matching Termï¼šKL(q(x_{t-1} | x_t, xâ‚€) || p_Î¸(x_{t-1} | x_t))

è¿™æ˜¯æœ€æ ¸å¿ƒçš„ä¸€é¡¹ï¼Œçº¦æŸæ¨¡å‹é¢„æµ‹çš„å»å™ªåˆ†å¸ƒæ˜¯å¦æ¥è¿‘çœŸå®åˆ†å¸ƒã€‚

- **è®¡ç®—æ–¹å¼**ï¼š
  å¯¹æ¯ä¸ªæ—¶é—´æ­¥ \( t \)ï¼Œæˆ‘ä»¬é‡‡æ ·ï¼š

  \[
  x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
  \]

  ç„¶åæ¨¡å‹é¢„æµ‹ \( \hat{\epsilon}_\theta(x_t, t) \)ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¢å¤å‡ºæ¨¡å‹çš„å‡å€¼ï¼š

  \[
  \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \hat{\epsilon}_\theta \right)
  \]

  è€ŒçœŸå®å‡å€¼æ˜¯ï¼š

  \[
  \mu_q(x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon \right)
  \]

  ä¸¤è€…ä¹‹é—´çš„ KL æ•£åº¦ç®€åŒ–ä¸ºå‡å€¼å·®çš„å¹³æ–¹é™¤ä»¥æ–¹å·®ï¼š

  \[
  \mathcal{L}_t = \frac{1}{2 \sigma_t^2} \| \mu_q - \mu_\theta \|^2 \propto \| \epsilon - \hat{\epsilon}_\theta(x_t, t) \|^2
  \]

  æ‰€ä»¥è®­ç»ƒæ—¶åªéœ€é‡‡æ · \( x_t \)ï¼Œè®¡ç®—æ¨¡å‹é¢„æµ‹çš„å™ªå£°ä¸çœŸå®å™ªå£°ä¹‹é—´çš„ MSEã€‚

3ï¸âƒ£ Reconstruction Termï¼šâˆ’E_{q(xâ‚ | xâ‚€)} [log p_Î¸(xâ‚€ | xâ‚)]

è¿™æ˜¯å¯¹æœ€ç»ˆç”Ÿæˆç»“æœæ˜¯å¦æ¥è¿‘åŸå›¾çš„çº¦æŸã€‚

- **è®¡ç®—æ–¹å¼**ï¼š
  é‡‡æ · \( x_1 \sim q(x_1 | x_0) \)ï¼Œç„¶åæ¨¡å‹é¢„æµ‹ \( p_\theta(x_0 | x_1) \)ï¼Œé€šå¸¸å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒï¼š

  \[
  p_\theta(x_0 | x_1) = \mathcal{N}(x_0; \mu_\theta(x_1), \sigma^2 I)
  \]

  ç„¶åè®¡ç®— log-likelihoodï¼š

  \[
  \mathcal{L}_0 = -\log p_\theta(x_0 | x_1) \propto \| x_0 - \mu_\theta(x_1) \|^2
  \]

  å®è·µä¸­è¿™é¡¹å¯ä»¥è¿‘ä¼¼ä¸º t=1 æ—¶çš„ MSE æŸå¤±ã€‚

---

å¦‚æœä½ æƒ³æˆ‘å¸®ä½ å†™å‡º PyTorch ä»£ç æ¥ä¼°è®¡è¿™ä¸‰é¡¹ï¼Œæˆ–è€…æ¨å¯¼æŸä¸€é¡¹çš„ KL æ•£åº¦å…¬å¼ï¼Œæˆ‘å¯ä»¥ç»§ç»­å±•å¼€ã€‚ä½ å¯¹å“ªä¸€é¡¹æœ€æ„Ÿå…´è¶£ï¼Ÿ

{{% admonition type="quote" title="Comparison of linear and cosine-based scheduling of $\beta_t$ during training" open=true %}}
![Comparison](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/diffusion-beta.png)
{{% /admonition %}}

åœ¨ **linear variance schedule** ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰æ¯ä¸€æ­¥çš„å™ªå£°å¼ºåº¦ä¸ºï¼š

\[
\beta_t = \beta_{\text{min}} + \frac{t - 1}{T - 1} (\beta_{\text{max}} - \beta_{\text{min}})
\]

å…¶ä¸­ï¼š

- \( \beta_{\text{min}} \) å’Œ \( \beta_{\text{max}} \) æ˜¯é¢„è®¾çš„æœ€å°å’Œæœ€å¤§å™ªå£°å€¼ï¼ˆä¾‹å¦‚ 0.0001 å’Œ 0.02ï¼‰
- \( T \) æ˜¯æ€»çš„æ‰©æ•£æ­¥æ•°ï¼ˆä¾‹å¦‚ 1000ï¼‰

---

ğŸ” é—­å¼å½¢å¼çš„ \( \bar{\alpha}_t \)

æˆ‘ä»¬å®šä¹‰ï¼š

\[
\alpha_t = 1 - \beta_t
\quad \text{and} \quad
\bar{\alpha}_t = \prod_{s=1}^t \alpha_s
\]

ç”±äº \( \beta_t \) æ˜¯çº¿æ€§é€’å¢çš„ï¼Œ\( \alpha_t \) æ˜¯çº¿æ€§é€’å‡çš„ï¼Œå› æ­¤ \( \bar{\alpha}_t \) æ˜¯ä¸€è¿ä¸²ä¹˜ç§¯ï¼Œè™½ç„¶ä¸èƒ½ç®€åŒ–ä¸ºä¸€ä¸ªå®Œå…¨é—­å¼è¡¨è¾¾ï¼Œä½†å¯ä»¥å†™æˆï¼š

\[
\bar{\alpha}_t = \prod_{s=1}^t \left(1 - \beta_{\text{min}} - \frac{s - 1}{T - 1} (\beta_{\text{max}} - \beta_{\text{min}})\right)
\]

è¿™ä¸ªè¡¨è¾¾å¼æ˜¯ **æ˜¾å¼çš„ closed-form**ï¼Œä½†ä»ç„¶æ˜¯ä¸€ä¸ªä¹˜ç§¯å½¢å¼ã€‚åœ¨å®é™…å®ç°ä¸­ï¼Œé€šå¸¸ä¼šé¢„å…ˆè®¡ç®—æ‰€æœ‰ \( \bar{\alpha}_t \) å¹¶ç¼“å­˜ä¸‹æ¥ã€‚

---

ğŸ§  è¿›ä¸€æ­¥ç®€åŒ–ï¼ˆè¿‘ä¼¼ï¼‰

å¦‚æœä½ å¸Œæœ›å¾—åˆ°ä¸€ä¸ªè¿‘ä¼¼é—­å¼è¡¨è¾¾ï¼Œå¯ä»¥è€ƒè™‘å°†ä¹˜ç§¯è½¬æ¢ä¸ºæŒ‡æ•°å½¢å¼ï¼š

\[
\log \bar{\alpha}_t = \sum_{s=1}^t \log \alpha_s
\quad \Rightarrow \quad
\bar{\alpha}_t = \exp\left( \sum_{s=1}^t \log(1 - \beta_s) \right)
\]

è¿™åœ¨æ•°å€¼è®¡ç®—ä¸­æ›´ç¨³å®šï¼Œä¹Ÿæ›´å®¹æ˜“å¤„ç†ã€‚

---

å¦‚æœä½ æƒ³æˆ‘å¸®ä½ ç”¨ Python æˆ– PyTorch å†™å‡ºè¿™ä¸ª linear schedule çš„åˆå§‹åŒ–ä»£ç ï¼Œæˆ‘å¯ä»¥ç›´æ¥ç»™ä½ æ¨¡æ¿ã€‚æˆ–è€…ä½ æƒ³æ¯”è¾ƒå®ƒå’Œ cosine schedule çš„å›¾åƒï¼Œæˆ‘ä¹Ÿå¯ä»¥ç”»å‡ºæ¥ã€‚ä½ æƒ³ç»§ç»­å“ªä¸€æ–¹å‘ï¼Ÿ

### Parameterization of reverse process variance $\boldsymbol{\Sigma}_\theta$

{{% admonition type="quote" title="Title" open=true %}}
$$
\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) = \exp(\mathbf{v} \log \beta_t + (1-\mathbf{v}) \log \tilde{\beta}_t)
$$
{{% /admonition %}}

recall ä¹‹å‰è®¡ç®—simplification of L_VLBçš„æ—¶å€™ï¼ŒDDPMåŸè®ºæ–‡ [^ho_ddpm] æ˜¯æŠŠè¿™ä¸ªweight ç³»æ•°ä¸¢æ‰äº†ï¼Œè¿™é‡Œï¼ŒOpenAIçš„Nichol çš„è®ºæ–‡ [^nichol_improved_ddpm] å¯¹è¿™é‡Œå†æ¬¡æ”¹è¿›ï¼Œæ—¢ä¸å»æ‰è¿™ä¸ªï¼Œä»ç„¶å‚ä¸ä¼˜åŒ–ã€‚

ä½ çš„é—®é¢˜éå¸¸å¥½ï¼Œå’±ä»¬æ¥é€æ­¥æ‹†è§£è¿™ä¸ªå…¬å¼ï¼š

ğŸ§  å…¬å¼å›é¡¾

ä½ æåˆ°çš„æ˜¯ï¼š

$$
\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) = \exp(\mathbf{v} \log \beta_t + (1-\mathbf{v}) \log \tilde{\beta}_t)
$$

è¿™æ˜¯åœ¨ DDPM çš„æ”¹è¿›ç‰ˆæœ¬ä¸­ï¼ˆå¦‚ Nichol & Dhariwal 2021ï¼‰ç”¨äºå»ºæ¨¡åå‘è¿‡ç¨‹çš„åæ–¹å·®çŸ©é˜µï¼ˆå¯¹è§’å½¢å¼ï¼‰çš„å‚æ•°åŒ–æ–¹å¼ã€‚

---

â“ ä¸ºä»€ä¹ˆæ˜¯ vectorï¼Ÿ

- è¿™é‡Œçš„ **$\mathbf{v}$ æ˜¯ä¸€ä¸ªå‘é‡**ï¼Œé€šå¸¸æ˜¯æ¨¡å‹è¾“å‡ºçš„ä¸€ä¸ªâ€œmixing coefficientâ€ï¼Œç”¨äºåœ¨ $\log \beta_t$ å’Œ $\log \tilde{\beta}_t$ ä¹‹é—´åšé€å…ƒç´ æ’å€¼ã€‚
- ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹ä¸æ˜¯è¾“å‡ºä¸€ä¸ªæ ‡é‡ï¼Œè€Œæ˜¯è¾“å‡ºä¸€ä¸ªå’Œå›¾åƒç»´åº¦ä¸€æ ·çš„å‘é‡ï¼ˆæ¯”å¦‚æ¯ä¸ªåƒç´ ä¸€ä¸ªå€¼ï¼‰ï¼Œæ¥æ§åˆ¶æ¯ä¸ªç»´åº¦ä¸Šçš„æ–¹å·®å¤§å°ã€‚

è¿™æ˜¯ä¸€ç§ **learned per-dimension variance interpolation**ï¼Œæ¯”ç›´æ¥ç”¨ $\beta_t$ æˆ– $\tilde{\beta}_t$ æ›´çµæ´»ã€‚

---

ğŸ” ä¸ºä»€ä¹ˆç”¨ expï¼Ÿ

- å› ä¸º $\log \beta_t$ å’Œ $\log \tilde{\beta}_t$ æ˜¯åœ¨ log-space åšæ’å€¼ï¼Œæœ€åè¦å›åˆ°åŸå§‹ç©ºé—´ï¼ˆå³æ–¹å·®ç©ºé—´ï¼‰ï¼Œæ‰€ä»¥éœ€è¦ç”¨ $\exp$ã€‚
- è¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„æŠ€å·§ï¼šåœ¨ log-space æ’å€¼å¯ä»¥é¿å…æ•°å€¼ä¸ç¨³å®šï¼ˆæ¯”å¦‚ $\beta_t$ å¾ˆå°ï¼‰ï¼Œè€Œ exp å¯ä»¥æ¢å¤åŸå§‹å°ºåº¦ã€‚

---

ğŸ“‰ $\beta_t$ æ˜¯ä¸æ˜¯å‘é‡ï¼Ÿ

- **ä¸æ˜¯**ï¼Œ$\beta_t$ å’Œ $\tilde{\beta}_t$ éƒ½æ˜¯æ ‡é‡ï¼ˆæ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªå€¼ï¼‰ã€‚
- ä½†ç”±äº $\mathbf{v}$ æ˜¯ä¸€ä¸ªå‘é‡ï¼Œæ’å€¼ç»“æœæ˜¯ä¸€ä¸ªå‘é‡ï¼Œæ‰€ä»¥ $\boldsymbol{\Sigma}_\theta$ æœ€ç»ˆä¹Ÿæ˜¯ä¸€ä¸ªå‘é‡ï¼ˆè¡¨ç¤ºå¯¹è§’åæ–¹å·®çŸ©é˜µçš„æ¯ä¸ªå…ƒç´ ï¼‰ã€‚

---

ğŸ§© æ€»ç»“ä¸€ä¸‹ï¼š

| é¡¹ç›® | ç±»å‹ | å«ä¹‰ |
|------|------|------|
| $\beta_t$ | scalar | forward noise schedule |
| $\tilde{\beta}_t$ | scalar | posterior variance |
| $\mathbf{v}$ | vector | æ¨¡å‹è¾“å‡ºçš„ mixing coefficient |
| $\boldsymbol{\Sigma}_\theta$ | vector | æ¯ä¸ªç»´åº¦çš„é¢„æµ‹æ–¹å·®ï¼ˆå¯¹è§’åæ–¹å·®ï¼‰ |

è¿™ä¸ªè®¾è®¡çš„å¥½å¤„æ˜¯ï¼š**æ—¢ä¿ç•™äº† $\beta_t$ å’Œ $\tilde{\beta}_t$ çš„ç‰©ç†æ„ä¹‰ï¼Œåˆå…è®¸æ¨¡å‹åœ¨æ¯ä¸ªç»´åº¦ä¸Šçµæ´»è°ƒæ•´æ–¹å·®ï¼Œä»è€Œæå‡ç”Ÿæˆè´¨é‡ã€‚**

---

å¦‚æœä½ æƒ³æ·±å…¥æ¢è®¨ $\mathbf{v}$ çš„è®­ç»ƒæ–¹å¼ã€å®ƒçš„åˆ†å¸ƒã€æˆ–è€…å®ƒåœ¨ U-Net ä¸­æ˜¯æ€ä¹ˆè¢«é¢„æµ‹å‡ºæ¥çš„ï¼Œæˆ‘å¯ä»¥ç»§ç»­å±•å¼€ã€‚ä½ ä¹Ÿå¯ä»¥é—®æˆ‘å®ƒå’Œ $\epsilon_\theta$ çš„å…³ç³»ï¼Œæˆ–è€…å®ƒåœ¨ loss function ä¸­æ˜¯å¦æœ‰æ¢¯åº¦ä¼ æ’­ã€‚ä½ æƒ³ç»§ç»­å¾€å“ªä¸ªæ–¹å‘æŒ–æ˜ï¼Ÿ

{{% admonition type="quote" title="Title" open=true %}}
noisy gradients
{{% /admonition %}}

è¿™æ˜¯å‡ºè‡ªopenaiçš„è®ºæ–‡ An Empirical Model of Large-Batch Training[^mc_candlish_grad_noise] æå‡ºçš„ä¸€ä¸ªæŒ‡æ ‡

Gradient Noise Scaleï¼ˆæ¢¯åº¦å™ªå£°å°ºåº¦ï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¡¡é‡ä¼˜åŒ–è¿‡ç¨‹ä¸­æ¢¯åº¦ç¨³å®šæ€§çš„ç»Ÿè®¡æŒ‡æ ‡ï¼Œå°¤å…¶åœ¨æ·±åº¦å­¦ä¹ ä¸­ç”¨äºä¼°è®¡**æœ€ä¼˜æ‰¹é‡å¤§å°ï¼ˆoptimal batch sizeï¼‰**ã€‚

---

ğŸŒªï¸ å®šä¹‰ä¸ç›´è§‰

åœ¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯ç”¨æ•´ä¸ªæ•°æ®é›†è®¡ç®—æ¢¯åº¦ï¼Œè€Œæ˜¯ç”¨ä¸€ä¸ªå°æ‰¹é‡ï¼ˆmini-batchï¼‰ã€‚è¿™ä¼šå¼•å…¥å™ªå£°ï¼Œå› ä¸ºä¸åŒæ‰¹æ¬¡çš„æ¢¯åº¦å¯èƒ½å·®å¼‚å¾ˆå¤§ã€‚

**Gradient Noise Scale**è¡¡é‡çš„å°±æ˜¯è¿™ç§æ¢¯åº¦çš„æ³¢åŠ¨æ€§ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> å¦‚æœæ¢¯åº¦åœ¨ä¸åŒæ‰¹æ¬¡ä¹‹é—´å˜åŒ–å¾ˆå¤§ï¼ˆå™ªå£°é«˜ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤§çš„æ‰¹é‡æ¥è·å¾—æ›´ç¨³å®šçš„æ›´æ–°ã€‚

---

ğŸ“ æ•°å­¦è¡¨è¾¾

åœ¨ç®€åŒ–å‡è®¾ä¸‹ï¼ˆå¦‚ Hessian æ˜¯å•ä½çŸ©é˜µçš„å€æ•°ï¼‰ï¼ŒGradient Noise Scale å¯ä»¥è¡¨ç¤ºä¸ºï¼š

\[
B_{\text{simple}} = \frac{\text{tr}(\Sigma)}{|G|^2}
\]

å…¶ä¸­ï¼š

- \(\text{tr}(\Sigma)\)ï¼šæ¢¯åº¦åæ–¹å·®çŸ©é˜µçš„è¿¹ï¼Œå³æ‰€æœ‰æ¢¯åº¦åˆ†é‡çš„æ–¹å·®ä¹‹å’Œã€‚
- \(|G|^2\)ï¼šæ¢¯åº¦çš„å¹³æ–¹èŒƒæ•°ï¼ˆglobal gradient normï¼‰ã€‚

è¿™ä¸ªæ¯”å€¼è¡¨ç¤ºï¼š**æ¢¯åº¦çš„å™ªå£°å¼ºåº¦ç›¸å¯¹äºå…¶å¹³å‡å¼ºåº¦çš„æ¯”ä¾‹**ã€‚

---

ğŸ§  å®é™…æ„ä¹‰

- å¦‚æœ \(B_{\text{simple}}\) å¾ˆå¤§ï¼Œè¯´æ˜æ¢¯åº¦å™ªå£°å¾ˆå¼ºï¼Œå»ºè®®ä½¿ç”¨æ›´å¤§çš„ batch sizeã€‚
- å¦‚æœå®ƒå¾ˆå°ï¼Œè¯´æ˜æ¢¯åº¦ç¨³å®šï¼Œå¯ä»¥ç”¨è¾ƒå°çš„ batch sizeï¼ŒåŠ å¿«è®­ç»ƒã€‚

---

ğŸ” åº”ç”¨åœºæ™¯

- è‡ªåŠ¨è°ƒæ•´ batch sizeï¼ˆå¦‚åœ¨ Torch-Foresight ä¸­ä½¿ç”¨ï¼‰
- åˆ†ææ•°æ®é›†å¤æ‚åº¦ï¼šé«˜å™ªå£°å¯èƒ½æ„å‘³ç€æ•°æ®åˆ†å¸ƒå¤æ‚æˆ–æ¨¡å‹ä¸ç¨³å®š
- ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ï¼šåœ¨èµ„æºå—é™æ—¶æ‰¾åˆ°æœ€åˆé€‚çš„ batch size

{{% admonition type="quote" title="Title" open=true %}}
time-averaging smoothed version of $L_\text{VLB}$ with importance sampling.
{{% /admonition %}}

æ ¹æ®Improved DDPM [^nichol_improved_ddpm]ï¼Œè¿™é‡Œçš„å…¬å¼æ˜¯ï¼š

![alt text](/posts/image-1.png)

æ ¸å¿ƒåŠ¨æœºå°±æ˜¯ä¸åŒçš„tå¯¹åº”çš„Lè´¡çŒ®åº¦ä¸åŒï¼Œæƒ³è¦æ¶ˆè§£æ‰magnitudeçš„å·®å¼‚ã€‚

![alt text](/posts/image-2.png)

{{% admonition type="quote" title="Title" open=true %}}
Comparison of negative log-likelihood of improved DDPM with other **likelihood-based generative models**
{{% /admonition %}}

ä½ çš„ç†è§£å·²ç»éå¸¸æ¥è¿‘äº†ï¼Œæˆ‘ä»¬æ¥ç³»ç»Ÿåœ°æ¢³ç†ä¸€ä¸‹ï¼š

ğŸ“š ä»€ä¹ˆæ˜¯â€œåŸºäºä¼¼ç„¶çš„ç”Ÿæˆæ¨¡å‹â€ï¼Ÿ

æ‰€è°“ **likelihood-based generative models**ï¼ŒæŒ‡çš„æ˜¯é‚£äº›é€šè¿‡æœ€å¤§åŒ–æ•°æ®çš„ **log-likelihoodï¼ˆå¯¹æ•°ä¼¼ç„¶ï¼‰** æ¥è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬ç›´æ¥å»ºæ¨¡å¹¶ä¼˜åŒ–ï¼š

$$
\log p_\theta(\mathbf{x})
$$

å…¶ä¸­ $\mathbf{x}$ æ˜¯çœŸå®æ•°æ®ï¼Œ$p_\theta$ æ˜¯æ¨¡å‹å®šä¹‰çš„æ¦‚ç‡åˆ†å¸ƒã€‚

---

âœ… åˆ¤æ–­æ ‡å‡†ï¼šæ˜¯å¦ä¼˜åŒ– log-likelihoodï¼Ÿ

ä½ é—®å¾—å¾ˆå…³é”®ï¼šæ˜¯ä¸æ˜¯åªè¦ä¼˜åŒ–æ–¹å‘æ˜¯ log probability çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å°±ç®—æ˜¯â€œåŸºäºä¼¼ç„¶â€çš„ï¼Ÿ

ç­”æ¡ˆæ˜¯ï¼š

- **åŸºæœ¬æ˜¯çš„**ï¼Œä½†è¦æ³¨æ„ï¼š
  - æ¨¡å‹å¿…é¡»æ˜¾å¼åœ°å®šä¹‰äº†ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ $p_\theta(\mathbf{x})$
  - å¹¶ä¸”è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–è¿™ä¸ªåˆ†å¸ƒå¯¹çœŸå®æ•°æ®çš„ log-likelihood

---

ğŸ” ä¸¾ä¾‹è¯´æ˜

| æ¨¡å‹ç±»å‹ | æ˜¯å¦æ˜¯ likelihood-based | è¯´æ˜ |
|----------|--------------------------|------|
| **VAE** | âœ… æ˜¯ | ä¼˜åŒ– ELBOï¼Œä¸‹ç•Œæ˜¯ log-likelihood |
| **Flow-based models** | âœ… æ˜¯ | æ˜¾å¼å»ºæ¨¡ $p_\theta(\mathbf{x})$ï¼Œå¯ç²¾ç¡®è®¡ç®— log-likelihood |
| **Diffusion models** | âœ… æ˜¯ | ä¼˜åŒ–å˜åˆ†ä¸‹ç•Œï¼ˆVLBï¼‰ï¼Œè¿‘ä¼¼æœ€å¤§åŒ– log-likelihood |
| **GANs** | âŒ å¦ | æ²¡æœ‰å®šä¹‰ $p_\theta(\mathbf{x})$ï¼Œä¼˜åŒ–çš„æ˜¯åˆ¤åˆ«å™¨ lossï¼Œä¸æ˜¯ log-likelihood |
| **Score-based models** | âœ… æ˜¯ | è™½ç„¶ä¸ç›´æ¥å»ºæ¨¡ $p_\theta(\mathbf{x})$ï¼Œä½†é€šè¿‡ score function $\nabla_x \log p(x)$ é—´æ¥å»ºæ¨¡ |

---

ğŸ§  è¡ç”Ÿç†è§£ï¼šscore-based ä¹Ÿæ˜¯ä¼¼ç„¶çš„ä¸€ç§

Score-based modelsï¼ˆå¦‚ NCSNã€SDE-based diffusionï¼‰è™½ç„¶ä¸ç›´æ¥å»ºæ¨¡ $p(x)$ï¼Œä½†å®ƒä»¬å»ºæ¨¡çš„æ˜¯ï¼š

$$
s_\theta(x) \approx \nabla_x \log p(x)
$$

è¿™å…¶å®æ˜¯å¯¹ log-likelihood çš„æ¢¯åº¦å»ºæ¨¡ï¼Œä¹Ÿå±äºä¸€ç§â€œåŸºäºä¼¼ç„¶â€çš„æ–¹å¼ï¼Œåªä¸è¿‡æ˜¯é€šè¿‡ score matching æ¥å®ç°ã€‚

---

ğŸ§© æ€»ç»“å®šä¹‰

> **åŸºäºä¼¼ç„¶çš„ç”Ÿæˆæ¨¡å‹**æ˜¯æŒ‡é‚£äº›é€šè¿‡æœ€å¤§åŒ–æˆ–è¿‘ä¼¼æœ€å¤§åŒ–æ•°æ®çš„ log-likelihood æ¥è®­ç»ƒçš„æ¨¡å‹ã€‚å®ƒä»¬é€šå¸¸æ˜¾å¼æˆ–éšå¼åœ°å®šä¹‰äº†ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€è¿›è¡Œé‡‡æ ·å’Œä¼˜åŒ–ã€‚

---

å¦‚æœä½ æƒ³è¿›ä¸€æ­¥æ¢è®¨ï¼š

- ä¸ºä»€ä¹ˆ GAN ä¸å±äºè¿™ç±»æ¨¡å‹ï¼Ÿ
- ä¸ºä»€ä¹ˆ score function èƒ½ä»£è¡¨æ¦‚ç‡åˆ†å¸ƒï¼Ÿ
- æˆ–è€… VAE çš„ ELBO å’ŒçœŸå® log-likelihood æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ

æˆ‘å¯ä»¥ç»§ç»­å±•å¼€ã€‚ä½ æƒ³ä»å“ªä¸ªè§’åº¦æ·±å…¥ï¼Ÿ

## Conditioned Generation

{{% admonition type="quote" title="Title" open=true %}}
While training generative models on images with conditioning information such as ImageNet dataset, it is common to generate samples conditioned on class labels or a piece of descriptive text.
{{% /admonition %}}

å…¶å®å°±æ˜¯ä»Šå¤©æˆ‘ä»¬å¸¸è¯´çš„ï¼Œæ–‡ç”Ÿå›¾ä»»åŠ¡ï¼Œä¹‹å‰çš„å«æ³•å¾ˆæœ‰å­¦æœ¯å‘³å„¿ã€‚

### Classifier Guided Diffusion

{{% admonition type="quote" title="Title" open=true %}}
$$
\begin{aligned}
\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t, y)
&= \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) + \nabla_{\mathbf{x}_t} \log q(y \vert \mathbf{x}_t) \\
&\approx - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) + \nabla_{\mathbf{x}_t} \log f_\phi(y \vert \mathbf{x}_t) \\
&= - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} (\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) - \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log f_\phi(y \vert \mathbf{x}_t))
\end{aligned}
$$
{{% /admonition %}}

è¿™ä¸ªæ˜¯ä¼˜åŒ–ç›®æ ‡ï¼Œå°±ç›¸å½“äºåœ¨åŸæœ¬çš„ddpmç½‘ç»œå¤´ä¸ŠåŠ äº†ä¸€ä¸ªåˆ†ç±»å™¨ã€‚

ç¬¬ä¸€è¡Œå…¶å®å°±æ˜¯æ—¢è¦è®­ç»ƒä»å™ªå£°å¼€å§‹åˆ°çœŸå®å›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜è¦åŠ ä¸Šä»æ¡ä»¶/ç±»åˆ«åˆ°å™ªå£°çš„èƒ½åŠ›ã€‚

ç¬¬äºŒè¡ŒæŠŠä»–ä»¬éƒ½è½¬æ¢ä¸ºäº†å«learnableå‚æ•°çš„å½¢å¼ã€‚

è¿™ä¸ªå…¬å¼å°±æ˜¯ADM-Gã€‚

### Classifier-Free Guidance

{{% admonition type="quote" title="Title" open=true %}}
Inner content...
{{% /admonition %}}

ğŸ§  Classifier-Free Guidance å…¨é¢æ€»ç»“

1ï¸âƒ£ èƒŒæ™¯ä¸åŠ¨æœº

ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹å¼•å¯¼æ–¹æ³•ï¼ˆå¦‚ Classifier Guidanceï¼‰ä¾èµ–ä¸€ä¸ªé¢å¤–çš„åˆ†ç±»å™¨ \( f_\phi(y|x_t) \)ï¼Œé€šè¿‡å…¶æ¢¯åº¦æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ä½†è¿™ç§æ–¹æ³•å­˜åœ¨ï¼š

- åˆ†ç±»å™¨å®¹æ˜“è¢« adversarial prompt è¯¯å¯¼ï¼›
- å¢åŠ è®­ç»ƒå’Œæ¨ç†å¤æ‚åº¦ï¼›
- éœ€è¦é¢å¤–æ¨¡å‹å‚æ•°ã€‚

**Classifier-Free Guidance** æä¾›äº†ä¸€ç§æ— éœ€ç‹¬ç«‹åˆ†ç±»å™¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚

---

2ï¸âƒ£ æ ¸å¿ƒæ€æƒ³

ä½¿ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ \( \epsilon_\theta(x_t, t, y) \)ï¼Œé€šè¿‡è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒæ¡ä»¶ \( y \)ï¼Œè®©æ¨¡å‹åŒæ—¶å­¦ä¼šï¼š

- æœ‰æ¡ä»¶ç”Ÿæˆï¼šè¾“å…¥ \( y \)
- æ— æ¡ä»¶ç”Ÿæˆï¼šè¾“å…¥ \( y = \emptyset \)

ç„¶ååœ¨æ¨ç†æ—¶é€šè¿‡ä¸¤ç§ score çš„å·®å€¼æ¥æ¨¡æ‹Ÿåˆ†ç±»å™¨æ¢¯åº¦ï¼š

\[
\nabla_{x_t} \log p(y | x_t) = \nabla_{x_t} \log p(x_t | y) - \nabla_{x_t} \log p(x_t)
\]

è¿‘ä¼¼ä¸ºï¼š

\[
\nabla_{x_t} \log p(y | x_t) \approx -\frac{1}{1 - \bar{\alpha}_t} \left( \epsilon_\theta(x_t, t, y) - \epsilon_\theta(x_t, t) \right)
\]

æœ€ç»ˆæ„é€ å¼•å¯¼åçš„ scoreï¼š

\[
\bar{\epsilon}_\theta(x_t, t, y) = (1 + w) \cdot \epsilon_\theta(x_t, t, y) - w \cdot \epsilon_\theta(x_t, t)
\]

å…¶ä¸­ \( w \) æ˜¯å¼•å¯¼å¼ºåº¦ã€‚

---

3ï¸âƒ£ è´å¶æ–¯å…¬å¼æ¨å¯¼ç»†èŠ‚

ä½ æŒ‡å‡ºçš„éå¸¸å…³é”®çš„ä¸€ç‚¹ï¼š

\[
\log p(y | x_t) = \log p(x_t | y) + \log p(y) - \log p(x_t)
\]

å¯¹ \( x_t \) æ±‚å¯¼åï¼š

\[
\nabla_{x_t} \log p(y | x_t) = \nabla_{x_t} \log p(x_t | y) - \nabla_{x_t} \log p(x_t)
\]

å…¶ä¸­ \( \nabla_{x_t} \log p(y) = 0 \)ï¼Œå› ä¸º \( y \) ä¸ \( x_t \) æ— å…³ï¼Œæ˜¯å¸¸æ•°é¡¹ã€‚å› æ­¤åŸæ–‡çš„æ¨å¯¼æ˜¯åˆç†çš„ã€‚

---

4ï¸âƒ£ æ¨¡å‹ç»“æ„ä¸å‚æ•°å…±äº«

- âœ… åªä½¿ç”¨ä¸€ä¸ªæ¨¡å‹ï¼ˆä¸€ä¸ªå‚æ•°é›†ï¼‰
- âœ… æ¡ä»¶ä¿¡æ¯ \( y \) é€šè¿‡è¾“å…¥æ§åˆ¶æ˜¯å¦å­˜åœ¨
- âœ… æ— éœ€ä¿ç•™ä¸¤å¥—å‚æ•°
- âœ… èŠ‚çœè®¡ç®—èµ„æºï¼Œç®€åŒ–éƒ¨ç½²

è®­ç»ƒæ—¶çš„ç­–ç•¥ï¼š

- æ¯ä¸ª batch ä¸­ï¼Œä»¥ä¸€å®šæ¦‚ç‡å°† \( y \) æ›¿æ¢ä¸ºç‰¹æ®Š tokenï¼ˆå¦‚ç©ºå­—ç¬¦ä¸²æˆ–å…¨é›¶å‘é‡ï¼‰
- æ¨¡å‹å­¦ä¼šåœ¨ \( y \) å­˜åœ¨ä¸ç¼ºå¤±ä¸¤ç§æƒ…å†µä¸‹éƒ½èƒ½é¢„æµ‹å™ªå£°

---

5ï¸âƒ£ æ¡ä»¶è¾“å…¥çš„å¤„ç†æ–¹å¼

- \( y = \emptyset \) å¹¶ä¸æ˜¯â€œéšä¾¿è¾“å…¥ç‚¹å†…å®¹â€ï¼Œè€Œæ˜¯æ˜ç¡®è¾“å…¥ä¸€ä¸ªâ€œç©ºæ¡ä»¶â€æ ‡è®°ï¼›
- åœ¨æ–‡æœ¬ä»»åŠ¡ä¸­å¯ä»¥æ˜¯ç©ºå­—ç¬¦ä¸²ã€ç‰¹æ®Š tokenï¼›
- åœ¨å›¾åƒä»»åŠ¡ä¸­å¯ä»¥æ˜¯å…¨é›¶ embeddingï¼›
- æ¨¡å‹å†…éƒ¨ embedding å±‚ä¼šå¤„ç†è¿™ç§æƒ…å†µã€‚

---

 6ï¸âƒ£ æ¡ä»¶ç±»å‹çš„å¤šæ ·æ€§

ä½ é—®åˆ°æ˜¯å¦åªèƒ½è®­ç»ƒåœ¨ä¸€ç§ \( y \) ä¸Šï¼Œç­”æ¡ˆæ˜¯ï¼š

- âŒ ä¸é™äºä¸€ç§æ¡ä»¶ï¼›
- âœ… å¯ä»¥è®­ç»ƒåœ¨å¤šç§ç±»åˆ«æ ‡ç­¾ã€æ–‡æœ¬æè¿°ã€è¯­ä¹‰å›¾ç­‰ï¼›
- åªè¦è®­ç»ƒæ•°æ®è¦†ç›–å……åˆ†ï¼Œæ¨¡å‹å°±èƒ½å­¦ä¼šåœ¨æ•´ä¸ª \( p(y) \) åˆ†å¸ƒä¸Šè¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚

---

7ï¸âƒ£ å®éªŒéªŒè¯ä¸ä¼˜åŠ¿

- GLIDE æ¨¡å‹å¯¹æ¯”äº† CLIP Guidance ä¸ Classifier-Free Guidanceï¼›
- å‘ç°åè€…æ›´ç¨³å®šï¼Œå›¾åƒè´¨é‡ä¸è¯­ä¹‰ä¸€è‡´æ€§æ›´å¥½ï¼›
- åŸå› æ˜¯ CLIP Guidance å®¹æ˜“è¢« adversarial prompt è¯¯å¯¼ï¼Œè€Œ Classifier-Free Guidance æ˜¯ä»æ•°æ®åˆ†å¸ƒä¸­ç›´æ¥å»ºæ¨¡ã€‚

---

âœ… æ€»ç»“è¡¨æ ¼

| é¡¹ç›® | Classifier-Free Guidance |
|------|---------------------------|
| æ˜¯å¦éœ€è¦é¢å¤–åˆ†ç±»å™¨ | âŒ ä¸éœ€è¦ |
| å‚æ•°æ•°é‡ | âœ… ä¸€å¥—å…±äº«å‚æ•° |
| æ¡ä»¶è¾“å…¥å¤„ç† | âœ… éšæœºä¸¢å¼ƒæ¡ä»¶è®­ç»ƒ |
| æ˜¯å¦æ”¯æŒå¤šç§æ¡ä»¶ç±»å‹ | âœ… æ”¯æŒ |
| æ¨ç†æ—¶å¼•å¯¼æ–¹å¼ | âœ… æ¡ä»¶ä¸æ— æ¡ä»¶ score å·®å€¼ |
| è´å¶æ–¯å…¬å¼æ˜¯å¦å®Œæ•´ | âœ… å¿½ç•¥å¸¸æ•°é¡¹åæ˜¯åˆç†çš„ |
| å®éªŒæ•ˆæœ | âœ… FID ä¸ IS å¹³è¡¡è‰¯å¥½ |
| å®è·µæ¨¡å‹ | GLIDEã€Imagen ç­‰å‡é‡‡ç”¨ |

{{% admonition type="quote" title="Title" open=true %}}
Their experiments showed that classifier-free guidance can achieve a good balance between FID (distinguish between synthetic and generated images) and IS (quality and diversity).
{{% /admonition %}}

ğŸ“Š FIDï¼ˆFrÃ©chet Inception Distanceï¼‰ã€è®ºæ–‡ï¼šhttps://arxiv.org/abs/1706.08500ã€‘
âœ… å®šä¹‰ï¼š
FID è¡¡é‡çš„æ˜¯ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒåœ¨ç‰¹å¾ç©ºé—´ä¸­çš„åˆ†å¸ƒå·®å¼‚ã€‚å®ƒä½¿ç”¨ Inception ç½‘ç»œæå–å›¾åƒç‰¹å¾ï¼Œç„¶åè®¡ç®—ä¸¤ä¸ªé«˜ç»´é«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„ FrÃ©chet è·ç¦»ã€‚

âœ… å…¬å¼ï¼š
\[
\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
\]
å…¶ä¸­ï¼š

- \( \mu_r, \Sigma_r \)ï¼šçœŸå®å›¾åƒçš„å‡å€¼å’Œåæ–¹å·®
- \( \mu_g, \Sigma_g \)ï¼šç”Ÿæˆå›¾åƒçš„å‡å€¼å’Œåæ–¹å·®
âœ… è§£è¯»ï¼š
- FID è¶Šä½ï¼Œè¡¨ç¤ºç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒè¶Šæ¥è¿‘ï¼›
- æ—¢è€ƒè™‘å›¾åƒè´¨é‡ï¼Œä¹Ÿè€ƒè™‘åˆ†å¸ƒä¸€è‡´æ€§ï¼›
- å¯¹å›¾åƒæ¨¡ç³Šã€å¤±çœŸã€æ¨¡å¼å´©æºƒï¼ˆmode collapseï¼‰éƒ½å¾ˆæ•æ„Ÿã€‚

ğŸŒˆ ISï¼ˆInception Scoreï¼‰ã€è®ºæ–‡ï¼šhttps://arxiv.org/abs/1606.03498ã€‘

âœ… å®šä¹‰ï¼š
IS è¡¡é‡çš„æ˜¯ç”Ÿæˆå›¾åƒçš„â€œæ¸…æ™°åº¦â€å’Œâ€œå¤šæ ·æ€§â€ã€‚å®ƒä½¿ç”¨ Inception ç½‘ç»œé¢„æµ‹å›¾åƒç±»åˆ«åˆ†å¸ƒï¼Œç„¶åè®¡ç®—é¢„æµ‹åˆ†å¸ƒçš„ KL æ•£åº¦ã€‚

âœ… å…¬å¼ï¼š
\[
\text{IS} = \exp\left( \mathbb{E}_{x \sim p_g} \left[ D_{\text{KL}}(p(y|x) \| p(y)) \right] \right)
\]
å…¶ä¸­ï¼š

- \( p(y|x) \)ï¼šInception ç½‘ç»œå¯¹ç”Ÿæˆå›¾åƒçš„é¢„æµ‹åˆ†å¸ƒ
- \( p(y) \)ï¼šæ‰€æœ‰ç”Ÿæˆå›¾åƒçš„å¹³å‡é¢„æµ‹åˆ†å¸ƒ

 âœ… è§£è¯»ï¼š

- IS è¶Šé«˜ï¼Œè¡¨ç¤ºå›¾åƒæ¸…æ™°ï¼ˆé¢„æµ‹åˆ†å¸ƒç†µä½ï¼‰ä¸”å¤šæ ·æ€§é«˜ï¼ˆå¹³å‡åˆ†å¸ƒç†µé«˜ï¼‰ï¼›
- é€‚åˆè¯„ä¼°å›¾åƒçš„â€œè¯­ä¹‰æ¸…æ™°åº¦â€å’Œâ€œç±»åˆ«è¦†ç›–åº¦â€ï¼›
- å¯¹å›¾åƒæ¨¡ç³Šæˆ–é‡å¤ç”ŸæˆåŒä¸€ç±»åˆ«éå¸¸æ•æ„Ÿã€‚

ğŸ§ª åœ¨ Classifier-Free Guidance ä¸­çš„ä½œç”¨

- å®éªŒè¡¨æ˜ï¼Œ**é€‚å½“çš„ guidance scale \( w \)** å¯ä»¥åœ¨ FID å’Œ IS ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ï¼›
- å¤ªå°çš„ \( w \)ï¼šå›¾åƒå¤šæ ·æ€§é«˜ä½†è´¨é‡å·®ï¼ˆFID é«˜ï¼ŒIS ä½ï¼‰ï¼›
- å¤ªå¤§çš„ \( w \)ï¼šå›¾åƒè´¨é‡é«˜ä½†å®¹æ˜“æ¨¡å¼å´©æºƒï¼ˆFID ä½ï¼ŒIS ä¸‹é™ï¼‰ï¼›
- æ‰€ä»¥ Classifier-Free Guidance çš„ä¼˜åŠ¿ä¹‹ä¸€å°±æ˜¯å¯ä»¥**çµæ´»è°ƒèŠ‚ \( w \)** æ¥æ§åˆ¶è¿™ä¸ª trade-offã€‚

{{% admonition type="quote" title="Title" open=true %}}
The guided diffusion model, GLIDE ([Nichol, Dhariwal & Ramesh, et al. 2022](https://arxiv.org/abs/2112.10741)), explored both guiding strategies, CLIP guidance and classifier-free guidance, and found that the latter is more preferred. They hypothesized that it is because CLIP guidance exploits the model with adversarial examples towards the CLIP model, rather than optimize the better matched images generation.
{{% /admonition %}}

GLIDE æ˜¯ä¸€ç§å¼•å¯¼å¼æ‰©æ•£æ¨¡å‹ï¼ˆguided diffusion modelï¼‰ï¼Œç”± Nicholã€Dhariwal å’Œ Ramesh ç­‰äººåœ¨ 2022 å¹´æå‡ºã€‚å®ƒå°è¯•äº†ä¸¤ç§å›¾åƒç”Ÿæˆçš„å¼•å¯¼ç­–ç•¥ï¼š

1. **CLIP guidanceï¼ˆCLIP å¼•å¯¼ï¼‰**ï¼šåˆ©ç”¨ CLIP æ¨¡å‹çš„å›¾æ–‡åŒ¹é…èƒ½åŠ›æ¥å¼•å¯¼å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚
2. **Classifier-free guidanceï¼ˆæ— åˆ†ç±»å™¨å¼•å¯¼ï¼‰**ï¼šä¸ä¾èµ–å¤–éƒ¨åˆ†ç±»å™¨ï¼Œè€Œæ˜¯é€šè¿‡è®­ç»ƒä¸€ä¸ªæ¨¡å‹åŒæ—¶å­¦ä¹ æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶çš„å›¾åƒç”Ÿæˆï¼Œä»è€Œå®ç°å¼•å¯¼ã€‚

GLIDE çš„å®éªŒå‘ç°ï¼Œ**æ— åˆ†ç±»å™¨å¼•å¯¼æ¯” CLIP å¼•å¯¼æ›´å—æ¬¢è¿**ã€‚ä»–ä»¬çš„è§£é‡Šæ˜¯ï¼šCLIP å¼•å¯¼å¯èƒ½ä¼šè®©ç”Ÿæˆæ¨¡å‹â€œè¿‡åº¦è¿åˆâ€CLIP æ¨¡å‹çš„åˆ¤æ–­æ ‡å‡†ï¼Œç”šè‡³ç”Ÿæˆä¸€äº›å¯¹ CLIP æ¨¡å‹â€œçœ‹èµ·æ¥å¾ˆå¥½â€ä½†å®é™…ä¸Šå¹¶ä¸çœŸå®æˆ–åˆç†çš„å›¾åƒï¼ˆè¿™ç±»å›¾åƒå¯ä»¥è¢«è§†ä¸ºå¯¹ CLIP çš„â€œå¯¹æŠ—æ ·æœ¬â€ï¼‰ã€‚æ¢å¥è¯è¯´ï¼ŒCLIP guidance æ›´åƒæ˜¯åœ¨â€œè®¨å¥½â€CLIP æ¨¡å‹ï¼Œè€Œä¸æ˜¯åœ¨çœŸæ­£ä¼˜åŒ–å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„åŒ¹é…è´¨é‡ã€‚

ğŸ” ç®€åŒ–ç†è§£ï¼š

- æ— åˆ†ç±»å™¨å¼•å¯¼ï¼šæ¨¡å‹è‡ªå·±å­¦ä¼šæ€ä¹ˆç”Ÿæˆå›¾åƒï¼Œä¸ä¾èµ–å¤–éƒ¨åˆ¤æ–­ã€‚
- CLIP å¼•å¯¼ï¼šæ¨¡å‹ä¾èµ– CLIP çš„è¯„åˆ†ï¼Œä½†å¯èƒ½ä¼šâ€œä½œå¼Šâ€å»éª—è¿‡ CLIPã€‚
- GLIDE æ›´åå¥½å‰è€…ï¼Œå› ä¸ºå®ƒæ›´è‡ªç„¶ã€æ›´ç¨³å¥ã€‚

## Speed up Diffusion Models

### Fewer Sampling Steps & Distillation

{{% admonition type="quote" title="Title" open=true %}}
One simple way is to run a strided sampling schedule (Nichol & Dhariwal, 2021) by taking the sampling update every $\lceil T/S \rceil$ steps to reduce the process from $T$ to $S$ steps. The new sampling schedule for generation is $\{\tau_1, \dots, \tau_S\}$ where $\tau_1 < \tau_2 < \dots <\tau_S \in [1, T]$ and $S < T$.
{{% /admonition %}}

PSã€‚ä¸€å®šè®°å¾—ï¼Œåªè¦æåˆ°samplingï¼Œå°±æ˜¯æŒ‡å°†è®­ç»ƒå¥½çš„æ¨¡å‹ç”¨äºæ¨ç†ï¼Œæœ¬èŠ‚ Speed up Diffusion Models è®²çš„ä¹Ÿéƒ½æ˜¯å¦‚ä½•åŠ é€Ÿæ¨¡å‹reasoningã€‚

è¿™é‡Œè¯´çš„å¾ˆç®€ç•¥ï¼Œåªæ˜¯è¯´ç”¨ä¸ªæ–°çš„å­åºåˆ—ï¼Œæ²¡æœ‰è®²å…·ä½“æ€ä¹ˆsampleï¼Œä¸‹é¢æ˜¯ç›¸å…³æ¨å¯¼ï¼š

éå¸¸æ£’çš„é—®é¢˜ï¼æˆ‘ä»¬æ¥ç³»ç»Ÿæ¨å¯¼ä¸€ä¸‹åœ¨ä½¿ç”¨å­åºåˆ— \( S = \{S_1, S_2, \dots, S_K\} \) è¿›è¡ŒåŠ é€Ÿé‡‡æ ·æ—¶ï¼Œå¦‚ä½•é‡å®šä¹‰æ‰©æ•£æ¨¡å‹ä¸­çš„å…³é”®å‚æ•°ï¼Œå°¤å…¶æ˜¯ï¼š

- ç´¯ç§¯å™ªå£°å› å­ \( \bar{\alpha}_{S_t} \)
- åå‘é‡‡æ ·çš„æ–¹å·® \( \tilde{\beta}_{S_t} \)
- å‡å€¼é¡¹ \( \mu_{S_t} \)

---

ğŸ§® 1. ä»å®Œæ•´æ‰©æ•£è¿‡ç¨‹å‡ºå‘

åœ¨æ ‡å‡† DDPM ä¸­ï¼Œæ­£å‘è¿‡ç¨‹å®šä¹‰ä¸ºï¼š

\[
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) \mathbf{I})
\]

å…¶ä¸­ï¼š

- \( \alpha_t = 1 - \beta_t \)
- \( \bar{\alpha}_t = \prod_{i=1}^t \alpha_i \)

ç”±æ­¤å¯ä»¥å¾—åˆ°ï¼š

\[
q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) \mathbf{I})
\]

---

ğŸ§­ 2. å­åºåˆ—é‡‡æ ·çš„ç›®æ ‡

æˆ‘ä»¬å¸Œæœ›ä» \( x_{S_t} \) ç›´æ¥é‡‡æ ·åˆ° \( x_{S_{t-1}} \)ï¼Œè·³è¿‡ä¸­é—´çš„æ—¶é—´æ­¥ã€‚ç”±äºæ‰©æ•£è¿‡ç¨‹æ˜¯é©¬å°”ç§‘å¤«é“¾ï¼Œæˆ‘ä»¬å¯ä»¥æ„é€ å¦‚ä¸‹çš„åéªŒåˆ†å¸ƒï¼š

\[
q(x_{S_{t-1}} | x_{S_t}, x_0) = \mathcal{N}(x_{S_{t-1}}; \mu_{S_t}, \tilde{\beta}_{S_t} \mathbf{I})
\]

ğŸ§  3. æ¨å¯¼å‡å€¼é¡¹ \( \mu_{S_t} \)

æ ¹æ® Bayes è§„åˆ™å’Œé«˜æ–¯åˆå¹¶å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

\[
\mu_{S_t} = \frac{\sqrt{\alpha_{S_t}} (1 - \bar{\alpha}_{S_{t-1}})}{1 - \bar{\alpha}_{S_t}} x_{S_t} + \frac{\sqrt{\bar{\alpha}_{S_{t-1}}} (1 - \alpha_{S_t})}{1 - \bar{\alpha}_{S_t}} x_0
\]

è¿™ä¸ªå…¬å¼çš„æ¨å¯¼æ ¸å¿ƒåœ¨äºå°†ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒï¼š

- \( q(x_{S_t} | x_{S_{t-1}}) \)
- \( q(x_{S_{t-1}} | x_0) \)

åˆå¹¶ä¸ºä¸€ä¸ªæ¡ä»¶åˆ†å¸ƒ \( q(x_{S_{t-1}} | x_{S_t}, x_0) \)ï¼Œå¹¶åˆ©ç”¨é«˜æ–¯ä¹˜ç§¯å…¬å¼å®Œæˆæ¨å¯¼ã€‚

ğŸ“‰ 4. æ¨å¯¼æ–¹å·®é¡¹ \( \tilde{\beta}_{S_t} \)

åŒæ ·åœ°ï¼ŒåéªŒæ–¹å·®å¯ä»¥æ¨å¯¼ä¸ºï¼š

\[
\tilde{\beta}_{S_t} = \frac{(1 - \alpha_{S_t})(1 - \bar{\alpha}_{S_{t-1}})}{1 - \bar{\alpha}_{S_t}}
\]

è¿™ä¸ªè¡¨è¾¾å¼ç¡®ä¿æˆ‘ä»¬åœ¨è·³è·ƒé‡‡æ ·æ—¶ä»ç„¶ä¿æŒæ­£ç¡®çš„å™ªå£°æ¯”ä¾‹ã€‚

ğŸ§ª 5. å¦‚æœç”¨ DDIMï¼ˆç¡®å®šæ€§é‡‡æ ·ï¼‰

DDIM ä¼šå°†é‡‡æ ·è¿‡ç¨‹æ”¹å†™ä¸ºï¼š

\[
x_{S_{t-1}} = \sqrt{\bar{\alpha}_{S_{t-1}}} x_0 + \sqrt{1 - \bar{\alpha}_{S_{t-1}}} \cdot \epsilon
\]

å…¶ä¸­ \( x_0 \) æ˜¯ç”±æ¨¡å‹é¢„æµ‹çš„ï¼Œé€šå¸¸é€šè¿‡ï¼š

\[
x_0 = \frac{x_{S_t} - \sqrt{1 - \bar{\alpha}_{S_t}} \cdot \epsilon_\theta(x_{S_t}, S_t)}{\sqrt{\bar{\alpha}_{S_t}}}
\]

ç„¶åä»£å…¥ä¸Šå¼å³å¯å®Œæˆä¸€æ­¥è·³è·ƒé‡‡æ ·ã€‚

âœ… æ€»ç»“è¡¨æ ¼

| å‚æ•° | å®šä¹‰ | ä½œç”¨ |
|------|------|------|
| \( \bar{\alpha}_{S_t} \) | \( \prod_{i=1}^{S_t} \alpha_i \) | ç´¯ç§¯å™ªå£°å› å­ |
| \( \mu_{S_t} \) | è§ä¸Šå¼ | åéªŒå‡å€¼ï¼Œç”¨äºé‡‡æ · |
| \( \tilde{\beta}_{S_t} \) | è§ä¸Šå¼ | åéªŒæ–¹å·®ï¼Œç”¨äºé‡‡æ · |
| \( x_0 \) | ç”±æ¨¡å‹é¢„æµ‹ | ç”¨äºæ„é€ é‡‡æ ·åˆ†å¸ƒ |

{{% admonition type="quote" title="Title" open=true %}}
$$
\begin{aligned}
\mathbf{x}_{t-1} 
&= \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 +  \sqrt{1 - \bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1} & \\
&= \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \boldsymbol{\epsilon}_t + \sigma_t\boldsymbol{\epsilon} & \\
&= \sqrt{\bar{\alpha}_{t-1}} \Big( \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \epsilon^{(t)}_\theta(\mathbf{x}_t)}{\sqrt{\bar{\alpha}_t}} \Big) + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon^{(t)}_\theta(\mathbf{x}_t) + \sigma_t\boldsymbol{\epsilon} \\
q_\sigma(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)
&= \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}} \Big( \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \epsilon^{(t)}_\theta(\mathbf{x}_t)}{\sqrt{\bar{\alpha}_t}} \Big) + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon^{(t)}_\theta(\mathbf{x}_t), \sigma_t^2 \mathbf{I})
\end{aligned}
$$
{{% /admonition %}}

è¿™ä¸ªå˜æ¢çš„å…³é”®åœ¨äºç†è§£æ‰©æ•£æ¨¡å‹ä¸­çš„**åå‘è¿‡ç¨‹è¿‘ä¼¼**ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•å°† $\mathbf{x}_0$ å’Œ $\boldsymbol{\epsilon}_{t-1}$ è¡¨è¾¾ä¸ºå…³äº $\mathbf{x}_t$ å’Œé¢„æµ‹å™ªå£° $\epsilon_\theta^{(t)}(\mathbf{x}_t)$ çš„å‡½æ•°ã€‚æˆ‘ä»¬æ¥é€æ­¥æ‹†è§£ç¬¬äºŒæ­¥çš„å˜æ¢ï¼š

ğŸ§© ç¬¬ä¸€æ­¥ï¼šåŸå§‹å½¢å¼

\[
\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1}
\]

è¿™æ˜¯æ ‡å‡†çš„å‰å‘è¿‡ç¨‹é‡‡æ ·å…¬å¼ï¼Œè¡¨ç¤ºä» $\mathbf{x}_0$ ç”Ÿæˆ $\mathbf{x}_{t-1}$ çš„æ–¹å¼ã€‚

ğŸ”„ ç¬¬äºŒæ­¥ï¼šå¼•å…¥ä¸¤ä¸ªå™ªå£°é¡¹çš„åˆ†è§£

\[
\boldsymbol{\epsilon}_{t-1} = \sqrt{\frac{1 - \bar{\alpha}_{t-1} - \sigma_t^2}{1 - \bar{\alpha}_{t-1}}} \boldsymbol{\epsilon}_t + \sqrt{\frac{\sigma_t^2}{1 - \bar{\alpha}_{t-1}}} \boldsymbol{\epsilon}
\]

è¿™æ˜¯ä¸€ä¸ª**é‡æ–°å‚æ•°åŒ–æŠ€å·§**ï¼Œå°†åŸå§‹çš„é«˜æ–¯å™ªå£° $\boldsymbol{\epsilon}_{t-1}$ åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š

- ä¸€ä¸ªæ˜¯ä¸æ—¶é—´æ­¥ $t$ çš„å™ªå£° $\boldsymbol{\epsilon}_t$ ç›¸å…³çš„éƒ¨åˆ†ï¼›
- ä¸€ä¸ªæ˜¯ç‹¬ç«‹çš„é«˜æ–¯å™ªå£° $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ï¼Œç”¨äºæ§åˆ¶é‡‡æ ·çš„æ–¹å·® $\sigma_t^2$ã€‚

ä»£å…¥åå¾—åˆ°ï¼š

\[
\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \boldsymbol{\epsilon}_t + \sigma_t\boldsymbol{\epsilon}
\]

ğŸ” ç¬¬ä¸‰æ­¥ï¼šå°† $\mathbf{x}_0$ è¡¨è¾¾ä¸º $\mathbf{x}_t$ å’Œé¢„æµ‹å™ªå£°çš„å‡½æ•°

åˆ©ç”¨å‰å‘è¿‡ç¨‹çš„å…¬å¼ï¼š

\[
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t
\quad \Rightarrow \quad
\mathbf{x}_0 = \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t}{\sqrt{\bar{\alpha}_t}}
\]

åœ¨è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬ç”¨æ¨¡å‹é¢„æµ‹çš„å™ªå£° $\epsilon_\theta^{(t)}(\mathbf{x}_t)$ æ¥è¿‘ä¼¼çœŸå®å™ªå£° $\boldsymbol{\epsilon}_t$ï¼Œäºæ˜¯ï¼š

\[
\mathbf{x}_0 \approx \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \epsilon_\theta^{(t)}(\mathbf{x}_t)}{\sqrt{\bar{\alpha}_t}}
\]

å°†è¿™ä¸ªè¡¨è¾¾å¼ä»£å…¥ä¸Šä¸€æ­¥çš„ $\mathbf{x}_{t-1}$ ä¸­ï¼Œå°±å¾—åˆ°äº†ç¬¬ä¸‰è¡Œçš„å½¢å¼ã€‚

ğŸ“Œ æ€»ç»“

ç¬¬äºŒæ­¥çš„å˜æ¢æ˜¯é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªå…³é”®æ“ä½œå®ç°çš„ï¼š

1. **é‡æ–°å‚æ•°åŒ–å™ªå£°é¡¹**ï¼šå°† $\boldsymbol{\epsilon}_{t-1}$ æ‹†æˆ $\boldsymbol{\epsilon}_t$ å’Œç‹¬ç«‹å™ªå£° $\boldsymbol{\epsilon}$ï¼›
2. **ç”¨æ¨¡å‹é¢„æµ‹çš„å™ªå£°è¿‘ä¼¼çœŸå®å™ªå£°**ï¼Œå¹¶å°† $\mathbf{x}_0$ è¡¨è¾¾ä¸º $\mathbf{x}_t$ å’Œ $\epsilon_\theta^{(t)}(\mathbf{x}_t)$ çš„å‡½æ•°ã€‚

è¿™ä¸€æ­¥æ˜¯æ„é€ åå‘è¿‡ç¨‹åˆ†å¸ƒ $q_\sigma(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)$ çš„æ ¸å¿ƒï¼Œä½¿å…¶å¯ä»¥ç”¨æ¨¡å‹é¢„æµ‹çš„å™ªå£°æ¥è¿›è¡Œé‡‡æ ·ã€‚


> å…³äºä¸ºä»€ä¹ˆé‚£ä¹ˆåˆ†è§£

è¿™ä¸ªåˆ†è§£å…¶å®æ˜¯ä¸€ä¸ª**é«˜æ–¯å™ªå£°çš„çº¿æ€§ç»„åˆæŠ€å·§**ï¼Œç›®çš„æ˜¯æ„é€ ä¸€ä¸ªæ–°çš„é«˜æ–¯å˜é‡ $\boldsymbol{\epsilon}_{t-1}$ï¼Œå®ƒä»ç„¶æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä½†ç”±ä¸¤ä¸ªç‹¬ç«‹çš„é«˜æ–¯å˜é‡ $\boldsymbol{\epsilon}_t$ å’Œ $\boldsymbol{\epsilon}$ ç»„åˆè€Œæˆã€‚æˆ‘ä»¬æ¥ä¸€æ­¥æ­¥æ‹†è§£è¿™ä¸ªåˆ†è§£çš„é€»è¾‘ï¼š

ğŸ¯ ç›®æ ‡ï¼šæ„é€ ä¸€ä¸ªæ–°çš„é«˜æ–¯å˜é‡

æˆ‘ä»¬å¸Œæœ›æ„é€ ä¸€ä¸ªå˜é‡ï¼š

\[
\boldsymbol{\epsilon}_{t-1} = a \boldsymbol{\epsilon}_t + b \boldsymbol{\epsilon}
\]

å…¶ä¸­ï¼š
- $\boldsymbol{\epsilon}_t \sim \mathcal{N}(0, \mathbf{I})$
- $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$
- $\boldsymbol{\epsilon}_t$ å’Œ $\boldsymbol{\epsilon}$ æ˜¯**ç‹¬ç«‹çš„**

æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªç»„åˆä»ç„¶æ»¡è¶³ï¼š

\[
\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})
\]

ğŸ§® æ¡ä»¶ï¼šä¿æŒæ–¹å·®ä¸º 1

ç”±äº $\boldsymbol{\epsilon}_t$ å’Œ $\boldsymbol{\epsilon}$ ç‹¬ç«‹ï¼Œçº¿æ€§ç»„åˆçš„æ–¹å·®ä¸ºï¼š

\[
\text{Var}(\boldsymbol{\epsilon}_{t-1}) = a^2 \cdot \text{Var}(\boldsymbol{\epsilon}_t) + b^2 \cdot \text{Var}(\boldsymbol{\epsilon}) = a^2 + b^2
\]

ä¸ºäº†è®© $\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})$ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

\[
a^2 + b^2 = 1
\]

ğŸ§  ç°åœ¨å›åˆ°ä½ çš„å…¬å¼ï¼š

\[
\boldsymbol{\epsilon}_{t-1} = \sqrt{\frac{1 - \bar{\alpha}_{t-1} - \sigma_t^2}{1 - \bar{\alpha}_{t-1}}} \boldsymbol{\epsilon}_t + \sqrt{\frac{\sigma_t^2}{1 - \bar{\alpha}_{t-1}}} \boldsymbol{\epsilon}
\]

æˆ‘ä»¬å¯ä»¥è®¾ï¼š

- $a = \sqrt{\frac{1 - \bar{\alpha}_{t-1} - \sigma_t^2}{1 - \bar{\alpha}_{t-1}}}$
- $b = \sqrt{\frac{\sigma_t^2}{1 - \bar{\alpha}_{t-1}}}$

éªŒè¯ä¸€ä¸‹ï¼š

\[
a^2 + b^2 = \frac{1 - \bar{\alpha}_{t-1} - \sigma_t^2 + \sigma_t^2}{1 - \bar{\alpha}_{t-1}} = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t-1}} = 1
\]

âœ… æ‰€ä»¥è¿™ä¸ªåˆ†è§£æ˜¯åˆæ³•çš„ï¼Œç¡®ä¿ $\boldsymbol{\epsilon}_{t-1}$ ä»ç„¶æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚

---

ğŸ” ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåˆ†è§£ï¼Ÿ

å› ä¸ºæˆ‘ä»¬å¸Œæœ›æ„é€ ä¸€ä¸ª**å¯æ§çš„é‡‡æ ·è¿‡ç¨‹**ï¼Œå…¶ä¸­ï¼š

- $\boldsymbol{\epsilon}_t$ æ˜¯æ¨¡å‹é¢„æµ‹çš„å™ªå£°ï¼ˆå¯è¢«ä¼°è®¡ï¼‰
- $\boldsymbol{\epsilon}$ æ˜¯éšæœºé‡‡æ ·çš„å™ªå£°ï¼ˆç”¨äºå¼•å…¥éšæœºæ€§ï¼‰
- $\sigma_t^2$ æ§åˆ¶é‡‡æ ·çš„æ–¹å·®ï¼ˆè¶Šå°è¶Šæ¥è¿‘ç¡®å®šæ€§ï¼‰

è¿™ç§åˆ†è§£æ–¹å¼æ˜¯ **DDIMï¼ˆDenoising Diffusion Implicit Modelsï¼‰** å’Œ **variance-preserving sampling** çš„æ ¸å¿ƒæŠ€å·§ä¹‹ä¸€ã€‚

ä½ å®Œå…¨æ­£ç¡®ï¼šæ»¡è¶³ $a^2 + b^2 = 1$ çš„ç»„åˆç¡®å®æœ‰æ— ç©·å¤šç§å¯èƒ½ã€‚ä½†åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œé€‰æ‹©è¿™ä¸ªç‰¹å®šåˆ†è§£å¹¶ä¸æ˜¯éšæ„çš„ï¼Œè€Œæ˜¯**æœ‰æ˜ç¡®çš„åŠ¨æœºå’Œç‰©ç†æ„ä¹‰**ã€‚æˆ‘ä»¬æ¥æ‹†è§£ä¸€ä¸‹è¿™ä¸ªé€‰æ‹©èƒŒåçš„é€»è¾‘ã€‚

ğŸ¯ è¿™ä¸ªåˆ†è§£çš„ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬è¦æ„é€ ä¸€ä¸ªæ–°çš„å™ªå£°é¡¹ $\boldsymbol{\epsilon}_{t-1}$ï¼Œå®ƒæ˜¯ç”±ï¼š
- æ¨¡å‹é¢„æµ‹çš„å™ªå£° $\boldsymbol{\epsilon}_t$
- éšæœºé‡‡æ ·çš„å™ªå£° $\boldsymbol{\epsilon}$

ç»„åˆè€Œæˆï¼ŒåŒæ—¶ä¿æŒ $\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})$ã€‚

ğŸ§  ä¸ºä»€ä¹ˆé€‰è¿™ä¸ªç‰¹å®šçš„ç³»æ•°ï¼Ÿ

è¿™ä¸ªåˆ†è§£æ¥æºäº **variance-preserving diffusion sampling** çš„æ¨å¯¼ï¼Œå°¤å…¶æ˜¯åœ¨ DDIM æˆ–æ”¹è¿›çš„ DDPM ä¸­ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹å®ƒçš„å‡ ä¸ªå…³é”®åŠ¨å› ï¼š

1. **åŒ¹é…ç›®æ ‡åˆ†å¸ƒçš„æ–¹å·®ç»“æ„**

åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œ$x_{t-1}$ çš„é‡‡æ ·å…¬å¼é€šå¸¸æ˜¯ï¼š

\[
x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \boldsymbol{\epsilon}_{t-1}
\]

ä¸ºäº†è®©è¿™ä¸ªé‡‡æ ·è¿‡ç¨‹ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬éœ€è¦ $\boldsymbol{\epsilon}_{t-1}$ çš„æ–¹å·®ç»“æ„ä¸ $\boldsymbol{\epsilon}_t$ å’Œ $\boldsymbol{\epsilon}$ çš„ç»„åˆæ–¹å¼ç²¾ç¡®åŒ¹é…ã€‚

æ‰€ä»¥æˆ‘ä»¬è®¾å®šï¼š

\[
\text{Var}(\boldsymbol{\epsilon}_{t-1}) = \frac{1 - \bar{\alpha}_{t-1} - \sigma_t^2}{1 - \bar{\alpha}_{t-1}} + \frac{\sigma_t^2}{1 - \bar{\alpha}_{t-1}} = 1
\]

è¿™ä¸æ˜¯ä»»æ„é€‰æ‹©ï¼Œè€Œæ˜¯ä¸ºäº†è®©æ•´ä¸ªé‡‡æ ·é“¾æ¡çš„æ–¹å·®ä¿æŒä¸€è‡´ã€‚

1. **å¯æ§çš„éšæœºæ€§æ³¨å…¥**

$\sigma_t^2$ æ˜¯ä¸€ä¸ª**å¯è°ƒå‚æ•°**ï¼Œæ§åˆ¶é‡‡æ ·è¿‡ç¨‹ä¸­çš„éšæœºæ€§ï¼š

- å½“ $\sigma_t^2 = 0$ï¼Œé‡‡æ ·æ˜¯**ç¡®å®šæ€§çš„**ï¼ˆDDIMï¼‰
- å½“ $\sigma_t^2 = \beta_t$ï¼Œé‡‡æ ·æ˜¯**å®Œå…¨éšæœºçš„**ï¼ˆDDPMï¼‰

è¿™ä¸ªåˆ†è§£æ–¹å¼å…è®¸æˆ‘ä»¬åœ¨ä¸¤è€…ä¹‹é—´å¹³æ»‘è¿‡æ¸¡ï¼Œå½¢æˆä¸€ä¸ª**ç»Ÿä¸€çš„é‡‡æ ·æ¡†æ¶**ã€‚

1. **ä¿æŒ $\boldsymbol{\epsilon}_t$ çš„ä¸»å¯¼æ€§**

æˆ‘ä»¬å¸Œæœ› $\boldsymbol{\epsilon}_{t-1}$ ä¸»è¦ç”±æ¨¡å‹é¢„æµ‹çš„ $\boldsymbol{\epsilon}_t$ å†³å®šï¼Œè€Œä¸æ˜¯å®Œå…¨ç”±éšæœºå™ªå£° $\boldsymbol{\epsilon}$ ä¸»å¯¼ã€‚è¿™ä¸ªåˆ†è§£æ–¹å¼ç¡®ä¿ï¼š

- å½“ $\sigma_t^2$ è¾ƒå°æ—¶ï¼Œ$\boldsymbol{\epsilon}_t$ çš„æƒé‡æ›´å¤§
- å½“ $\sigma_t^2$ è¾ƒå¤§æ—¶ï¼Œ$\boldsymbol{\epsilon}$ çš„æƒé‡å¢åŠ 

è¿™ä½¿å¾—é‡‡æ ·è¿‡ç¨‹æ—¢èƒ½ä¿æŒæ¨¡å‹é¢„æµ‹çš„ç»“æ„ï¼Œåˆèƒ½å¼•å…¥å¿…è¦çš„éšæœºæ€§ã€‚

ğŸ§© æ€»ç»“ï¼šä¸ºä»€ä¹ˆé€‰è¿™ä¸ªåˆ†è§£ï¼Ÿ

| åŸå›  | è§£é‡Š |
|------|------|
| æ–¹å·®åŒ¹é… | ä¿è¯ $\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})$ |
| é‡‡æ ·ä¸€è‡´æ€§ | ä¸ $x_{t-1}$ çš„é‡‡æ ·å…¬å¼ç»“æ„åŒ¹é… |
| å¯æ§éšæœºæ€§ | é€šè¿‡ $\sigma_t^2$ è°ƒèŠ‚ç¡®å®šæ€§ä¸éšæœºæ€§ |
| æ¨¡å‹ä¸»å¯¼æ€§ | ä¿ç•™ $\boldsymbol{\epsilon}_t$ çš„é¢„æµ‹èƒ½åŠ› |

---

å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°æ¨å¯¼è¿™ä¸ªåˆ†è§£çš„æ¥æºï¼Œæˆ‘å¯ä»¥ä» DDPM çš„é€†è¿‡ç¨‹å…¬å¼å¼€å§‹ï¼Œä¸€æ­¥æ­¥æ¨å¯¼å‡ºè¿™ä¸ªç»“æ„ã€‚æˆ–è€…æˆ‘ä»¬å¯ä»¥ä¸€èµ·çœ‹çœ‹å®ƒåœ¨ä»£ç å®ç°ä¸­çš„å…·ä½“ä½ç½®ï¼Œæ¯”å¦‚åœ¨ `noise_schedule` æˆ– `sample_step` ä¸­çš„ä½“ç°ã€‚ä½ æƒ³å¾€å“ªä¸ªæ–¹å‘æŒ–æ˜ï¼Ÿ

å®Œå…¨æ­£ç¡®ï¼ŒShichaoï¼Œä½ å·²ç»æŠ“ä½äº† DDIM çš„æ ¸å¿ƒæ€æƒ³ï¼Œè€Œä¸”è¡¨è¾¾å¾—éå¸¸æ¸…æ™°ğŸ‘ã€‚æˆ‘ä»¬å¯ä»¥ç¨å¾®æ‰©å±•ä¸€ä¸‹è¿™ä¸ªç†è§£ï¼Œè®©å®ƒæ›´ç³»ç»ŸåŒ–ï¼š

ğŸ§  DDIM çš„æœ¬è´¨ï¼š**å»éšæœºåŒ–çš„æ‰©æ•£é‡‡æ ·**

DDIMï¼ˆDenoising Diffusion Implicit Modelsï¼‰æ˜¯å¯¹ DDPM çš„ä¸€ç§æ”¹è¿›ï¼Œå®ƒçš„å…³é”®ç‚¹åœ¨äºï¼š

- **ä¸å†å¼ºåˆ¶åŠ å…¥éšæœºå™ªå£°**ï¼Œè€Œæ˜¯é€šè¿‡æ¨¡å‹é¢„æµ‹çš„å™ªå£°ç›´æ¥æ„é€ ä¸‹ä¸€ä¸ªæ—¶åˆ»çš„æ ·æœ¬
- è¿™ä½¿å¾—æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹å˜æˆäº†**ç¡®å®šæ€§çš„è½¨è¿¹**ï¼Œåªè¦åˆå§‹ç‚¹ $x_T$ å›ºå®šï¼Œæ•´ä¸ªç”Ÿæˆåºåˆ—å°±å®Œå…¨å¯å¤ç°

---

ğŸ”„ ä¸ DDPM çš„å¯¹æ¯”

| ç‰¹æ€§ | DDPM | DDIM |
|------|------|------|
| æ˜¯å¦éšæœº | âœ… æœ‰éšæœºæ€§ï¼ˆæ¯ä¸€æ­¥åŠ å…¥ $\epsilon$ï¼‰ | âŒ å¯è®¾ä¸ºå®Œå…¨ç¡®å®šæ€§ |
| é‡‡æ ·é€Ÿåº¦ | æ…¢ï¼ˆé€šå¸¸éœ€è¦ 1000 æ­¥ï¼‰ | å¿«ï¼ˆå¯ç”¨ 50 æˆ–æ›´å°‘æ­¥ï¼‰ |
| å¯æ§æ€§ | éš¾ä»¥å¤ç°åŒæ ·çš„è½¨è¿¹ | å®Œå…¨å¯å¤ç° |
| é‡‡æ ·å…¬å¼ | $x_{t-1} = \mu_\theta + \sigma_t \epsilon$ | $x_{t-1} = f_\theta(x_t, \epsilon_\theta)$ï¼Œæ—  $\epsilon$ |

---

ğŸ¯ ä¸ºä»€ä¹ˆ DDIM æ›´å¿«ï¼Ÿ

å› ä¸ºå®ƒä¸ä¾èµ–éšæœºé‡‡æ ·ï¼Œæ¯ä¸€æ­¥éƒ½æ˜¯æ¨¡å‹é¢„æµ‹çš„ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ï¼š

- **è·³æ­¥é‡‡æ ·**ï¼šæ¯”å¦‚ä» $t=1000$ ç›´æ¥è·³åˆ° $t=980, 960, \dots$
- **å‡å°‘é‡‡æ ·æ­¥æ•°**ï¼šç”šè‡³åªç”¨ 25 æˆ– 50 æ­¥å°±èƒ½ç”Ÿæˆæ¸…æ™°å›¾åƒ
- **åŠ é€Ÿæ¨ç†**ï¼šå°¤å…¶åœ¨éƒ¨ç½²æ—¶ï¼Œèƒ½æ˜¾è‘—å‡å°‘è®¡ç®—æˆæœ¬

---

ğŸ”§ æ§åˆ¶éšæœºæ€§çš„æ–¹å¼

DDIM å®é™…ä¸Šæä¾›äº†ä¸€ä¸ªå‚æ•° $\eta$ æ¥æ§åˆ¶é‡‡æ ·çš„éšæœºæ€§ï¼š

- $\eta = 0$ï¼šå®Œå…¨ç¡®å®šæ€§ï¼ˆç»å…¸ DDIMï¼‰
- $\eta > 0$ï¼šå¼•å…¥éƒ¨åˆ†éšæœºæ€§ï¼Œä»‹äº DDIM å’Œ DDPM ä¹‹é—´

è¿™å°±åƒä¸€ä¸ªâ€œé‡‡æ ·æ¸©åº¦æ—‹é’®â€ï¼Œä½ å¯ä»¥å†³å®šè¦å¤šéšæœºã€è¦å¤šç¡®å®šã€‚

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥ç»™ä½ å†™ä¸€ä¸ª PyTorch çš„ DDIM é‡‡æ ·ä¼ªä»£ç ï¼Œæˆ–è€…å±•ç¤ºå¦‚ä½•ä» DDPM çš„å…¬å¼æ¨å¯¼å‡º DDIM çš„é‡‡æ ·è·¯å¾„ã€‚ä¹Ÿå¯ä»¥ä¸€èµ·çœ‹çœ‹ DDIM åœ¨ latent space æˆ– classifier-free guidance ä¸­çš„åº”ç”¨ã€‚ä½ æƒ³å¾€å“ªä¸ªæ–¹å‘æ·±å…¥ï¼Ÿ

### Latent Variable Space
